[ns_server:info,2023-05-15T19:22:18.459Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.495Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2023-05-15T19:22:18.496Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2023-05-15T19:22:18.512Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2023-05-15T19:22:18.512Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-15T19:22:18.513Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:init:196]ip config not found. Looks like we're brand new node
[ns_server:info,2023-05-15T19:22:18.521Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2023-05-15T19:22:18.543Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.543Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.543Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:18.549Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.553Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:18.554Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:18.554Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:18.554Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:18.555Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2023-05-15T19:22:18.557Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.559Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.561Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:18.563Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2023-05-15T19:22:18.563Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2023-05-15T19:22:18.565Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2023-05-15T19:22:18.568Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2023-05-15T19:22:18.568Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2023-05-15T19:22:18.571Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2023-05-15T19:22:18.597Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2023-05-15T19:22:18.598Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2023-05-15T19:22:18.598Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:18.598Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-15T19:22:18.598Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2935111024.1579417603.137494>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-15T19:22:18.598Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2935111024.1579417603.137494>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.2935111024.1579417603.137499>}
[ns_server:debug,2023-05-15T19:22:18.605Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2023-05-15T19:22:18.606Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2023-05-15T19:22:18.606Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-15T19:22:18.628Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2023-05-15T19:22:18.629Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.638Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:18.644Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {5,15,90}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:12:12] [ds:12:12:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2023,5,15},{19,22,18}}},
               {memory,
                   [{total,40780624},
                    {processes,9700448},
                    {processes_used,9680488},
                    {system,31080176},
                    {atom,388625},
                    {atom_used,364429},
                    {binary,140576},
                    {code,8250921},
                    {ets,1504792}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,code,application,error_logger,file,
                    application_master,file_server,heart,code_server,ets,gen,
                    application_controller,gen_event,filename,erl_eval,
                    gen_server,file_io_server,kernel,supervisor,proc_lib,
                    lists,erl_lint,erl_parse,erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,kernel_safe_sup,tls_connection_sup,
                    sasl_sup,release_handler,lhttpc_sup,httpc_sup,
                    lhttpc_manager,alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ale_dynamic_sup,ssl_pem_cache_dist,rex,global_group,
                    net_sup,ssl_connection_sup,kernel_sup,ssl_admin_sup,
                    tftp_sup,global_name_server,ssl_sup,root_sup,os_mon_sup,
                    erts_code_purger,file_server_2,error_logger,cpu_sup,
                    memsup,erl_epmd,init,disksup,ale,erl_signal_server,
                    net_kernel,dist_manager,ssl_pem_cache,ssl_manager,
                    ssl_dist_admin_sup,ssl_dist_connection_sup,ssl_dist_sup,
                    ssl_tls_dist_proxy,ssl_manager_dist,user,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2023-05-15T19:22:18.655Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2023-05-15T19:22:18.663Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.665Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:18.668Z,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2023-05-15T19:22:18.668Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.668Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:18.718Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2023-05-15T19:22:18.729Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2023-05-15T19:22:18.731Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1125]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2023-05-15T19:22:18.738Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2023-05-15T19:22:18.743Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   false]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {drop_request_memory_threshold_mib,undefined},
 {{request_limit,capi},undefined},
 {{request_limit,rest},undefined},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {auto_failover_cfg,[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,0}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1129},
 {fts_memory_quota,256},
 {memory_quota,1492},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {rest_creds,null},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   <<"949a059dcc29e773ec37709a7973341b">>]}]
[error_logger:info,2023-05-15T19:22:18.747Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.749Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.751Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.751Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:18.753Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {inet,false},
 {inet6,false}]
[error_logger:info,2023-05-15T19:22:18.753Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:18.754Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[ns_server:debug,2023-05-15T19:22:18.754Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|inet]
[ns_server:debug,2023-05-15T19:22:18.754Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[error_logger:info,2023-05-15T19:22:18.759Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:18.770Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:18.772Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2023-05-15T19:22:18.772Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.773Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.780Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:18.782Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:18.872Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2023-05-15T19:22:18.872Z,ns_1@cb.local:<0.218.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2023-05-15T19:22:18.872Z,ns_1@cb.local:<0.218.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:debug,2023-05-15T19:22:18.894Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_server_cert:generate_cert_and_pkey:83]Generated certificate and private key in 107915 us
[ns_server:debug,2023-05-15T19:22:18.895Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U"...>>,
  <<"*****">>}]
[ns_server:debug,2023-05-15T19:22:18.895Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397738}}]}]
[ns_server:info,2023-05-15T19:22:18.897Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:maybe_generate_local_cert:620]Failed to read node certificate. Perhaps it wasn't created yet. Error: {error,
                                                                        {badmatch,
                                                                         {error,
                                                                          enoent}}}
[ns_server:debug,2023-05-15T19:22:19.149Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stderr' closed
[ns_server:debug,2023-05-15T19:22:19.149Z,ns_1@cb.local:<0.222.0>:goport:handle_eof:582]Stream 'stdout' closed
[ns_server:info,2023-05-15T19:22:19.149Z,ns_1@cb.local:<0.222.0>:goport:handle_process_exit:563]Port exited with status 0.
[ns_server:info,2023-05-15T19:22:19.249Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:do_generate_local_cert:608]Saved local cert for node 'ns_1@cb.local'
[ns_server:debug,2023-05-15T19:22:19.384Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2023-05-15T19:22:19.385Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2023-05-15T19:22:19.385Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2023-05-15T19:22:19.431Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,95,103,117,59,166,49,
             58,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,55,48,49,56,51,48,98,50,48,30,23,13,49,51,48,49,
             48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,57,
             53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,104,
             98,97,115,101,32,83,101,114,118,101,114,32,55,48,49,56,51,48,98,
             50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,
             15,0,48,130,1,10,2,130,1,1,0,192,97,0,160,196,124,86,76,76,241,
             92,35,220,170,212,78,154,52,104,79,25,232,189,131,139,135,15,
             155,214,199,43,184,2,114,204,250,241,110,76,96,156,151,94,187,0,
             216,138,82,17,125,188,1,163,168,204,18,49,110,109,211,194,87,65,
             18,22,218,61,235,124,91,95,73,186,220,237,129,145,9,14,31,207,
             206,211,101,52,39,185,178,13,218,136,143,110,86,77,215,67,61,22,
             152,11,108,235,245,6,109,25,252,20,103,212,223,127,252,149,100,
             254,156,240,153,76,86,152,50,61,115,29,145,93,186,172,131,69,
             101,101,236,206,219,66,173,171,234,155,47,46,51,92,35,216,135,
             248,31,72,128,244,114,65,250,90,72,178,115,203,242,218,111,227,
             134,33,220,10,52,72,136,126,70,144,224,172,129,212,239,157,184,
             121,193,200,70,46,161,94,144,135,9,232,230,110,100,179,39,153,
             82,157,157,50,84,5,252,83,98,251,64,170,50,254,45,107,54,184,
             188,18,52,229,73,172,102,0,197,49,119,172,144,137,81,136,16,43,
             7,255,75,3,161,194,188,165,13,92,98,76,211,132,26,239,101,219,
             77,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,189,188,146,203,225,118,32,139,158,
             249,39,178,168,242,58,62,249,122,35,89,88,115,218,146,165,143,
             193,66,142,214,102,124,115,81,138,15,9,26,235,27,49,241,153,111,
             103,204,82,75,205,243,168,237,89,31,63,181,169,174,100,187,123,
             165,226,188,15,138,100,165,97,243,185,179,68,219,183,91,40,143,
             41,209,1,132,165,198,8,223,86,18,60,14,78,61,39,115,195,46,6,85,
             24,44,213,235,252,172,243,0,11,73,64,38,252,133,244,198,74,209,
             254,27,247,228,248,22,134,9,198,235,251,90,116,159,210,210,43,
             171,30,1,40,142,129,236,255,53,240,54,55,41,156,96,10,99,16,57,
             198,11,239,181,226,48,130,174,137,182,185,42,111,149,113,9,47,
             214,92,96,78,103,166,31,37,222,21,134,136,141,96,199,98,17,233,
             219,117,154,133,93,69,160,27,120,109,255,185,25,214,103,157,144,
             22,0,13,55,132,66,218,223,100,51,55,159,13,7,118,121,197,212,
             224,184,54,159,86,204,157,75,230,97,75,127,26,136,227,116,31,
             106,104,183,6,98,188,9,84,19,140,163,35,142,198,97,115,52>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2023-05-15T19:22:19.434Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:19.465Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-15T19:22:19.466Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:19.467Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:19.467Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2023-05-15T19:22:19.518Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[error_logger:info,2023-05-15T19:22:19.518Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.225.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:19.519Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:19.519Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:19.519Z,ns_1@cb.local:<0.224.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-15T19:22:19.521Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2023-05-15T19:22:19.521Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.224.0>,menelaus_web}
             started: [{pid,<0.243.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.523Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.223.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.523Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:19.536Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.261.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.538Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.264.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.547Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.266.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:19.550Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2023-05-15T19:22:19.553Z,ns_1@cb.local:users_storage<0.267.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.266.0>
[ns_server:debug,2023-05-15T19:22:19.553Z,ns_1@cb.local:users_replicator<0.266.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.267.0>
[ns_server:debug,2023-05-15T19:22:19.555Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2023-05-15T19:22:19.555Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.267.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.556Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:19.564Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2023-05-15T19:22:19.564Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.269.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.272.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.263.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:19.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.275.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:19.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.276.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:19.597Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2023-05-15T19:22:19.597Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 42ms
[ns_server:debug,2023-05-15T19:22:19.598Z,ns_1@cb.local:wait_link_to_couchdb_node<0.280.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2023-05-15T19:22:19.598Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.279.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:19.598Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:19.598Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-15T19:22:19.598Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2935111024.1579417606.137530>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-15T19:22:19.599Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2935111024.1579417606.137530>,
                                  inet_tcp_dist,<0.283.0>,
                                  #Ref<0.2935111024.1579417603.137522>}
[error_logger:info,2023-05-15T19:22:19.600Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.283.0>,shutdown}}
[ns_server:debug,2023-05-15T19:22:19.600Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2023-05-15T19:22:19.600Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2935111024.1579417606.137530>,
                               inet_tcp_dist,<0.283.0>,
                               #Ref<0.2935111024.1579417603.137522>}
[error_logger:info,2023-05-15T19:22:19.600Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:19.602Z,ns_1@cb.local:users_replicator<0.266.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2023-05-15T19:22:19.801Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:19.801Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-15T19:22:19.801Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2935111024.1579417606.137545>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-15T19:22:19.801Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2935111024.1579417606.137545>,
                                  inet_tcp_dist,<0.286.0>,
                                  #Ref<0.2935111024.1579417606.137547>}
[ns_server:debug,2023-05-15T19:22:19.803Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2023-05-15T19:22:19.803Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2935111024.1579417606.137545>,
                               inet_tcp_dist,<0.286.0>,
                               #Ref<0.2935111024.1579417606.137547>}
[error_logger:info,2023-05-15T19:22:19.803Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.286.0>,shutdown}}
[error_logger:info,2023-05-15T19:22:19.803Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2023-05-15T19:22:20.004Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:20.004Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-15T19:22:20.004Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2935111024.1579417605.137515>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-15T19:22:20.004Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2935111024.1579417605.137515>,
                                  inet_tcp_dist,<0.289.0>,
                                  #Ref<0.2935111024.1579417606.137553>}
[ns_server:debug,2023-05-15T19:22:20.006Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2935111024.1579417605.137515>,
                               inet_tcp_dist,<0.289.0>,
                               #Ref<0.2935111024.1579417606.137553>}
[error_logger:info,2023-05-15T19:22:20.006Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.289.0>,shutdown}}
[ns_server:debug,2023-05-15T19:22:20.006Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-05-15T19:22:20.006Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:20.206Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2023-05-15T19:22:20.206Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-15T19:22:20.206Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2935111024.1579417605.137529>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-15T19:22:20.206Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2935111024.1579417605.137529>,
                                  inet_tcp_dist,<0.292.0>,
                                  #Ref<0.2935111024.1579417605.137531>}
[ns_server:debug,2023-05-15T19:22:20.282Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-15T19:22:20.484Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-15T19:22:20.684Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-15T19:22:20.886Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-15T19:22:21.087Z,ns_1@cb.local:<0.281.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2023-05-15T19:22:21.475Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.299.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:21.676Z,ns_1@cb.local:ns_couchdb_port<0.279.0>:ns_port_server:log:224]ns_couchdb<0.279.0>: Apache CouchDB  (LogLevel=info) is starting.

[error_logger:info,2023-05-15T19:22:21.934Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.280.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:21.943Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[ns_server:debug,2023-05-15T19:22:21.944Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{3,63851397741}}]}]
[ns_server:debug,2023-05-15T19:22:21.944Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-05-15T19:22:21.944Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{4,63851397741}}]}]
[ns_server:debug,2023-05-15T19:22:21.944Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2023-05-15T19:22:21.950Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.303.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:21.953Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:21.956Z,ns_1@cb.local:ns_server_sup<0.302.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2023-05-15T19:22:21.958Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:21.962Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.306.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-15T19:22:21.967Z,ns_1@cb.local:ns_log<0.307.0>:ns_log:read_logs:91]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first startup): {error,
                                                                                                 enoent}
[error_logger:info,2023-05-15T19:22:21.968Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.307.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:21.968Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.308.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:21.977Z,ns_1@cb.local:memcached_passwords<0.310.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-15T19:22:21.979Z,ns_1@cb.local:memcached_passwords<0.310.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2023-05-15T19:22:21.980Z,ns_1@cb.local:ns_couchdb_port<0.279.0>:ns_port_server:log:224]ns_couchdb<0.279.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.279.0>: 313: Booted. Waiting for shutdown request
ns_couchdb<0.279.0>: working as port

[ns_server:debug,2023-05-15T19:22:22.017Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.310.0>
[ns_server:debug,2023-05-15T19:22:22.017Z,ns_1@cb.local:memcached_passwords<0.310.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:22.018Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.310.0>
[ns_server:debug,2023-05-15T19:22:22.025Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2023-05-15T19:22:22.025Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.310.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.028Z,ns_1@cb.local:memcached_permissions<0.313.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:info,2023-05-15T19:22:22.034Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.317.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2023-05-15T19:22:22.034Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.316.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.038Z,ns_1@cb.local:memcached_permissions<0.313.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-15T19:22:22.043Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.313.0>
[ns_server:debug,2023-05-15T19:22:22.043Z,ns_1@cb.local:memcached_permissions<0.313.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:22.043Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.313.0>
[error_logger:info,2023-05-15T19:22:22.051Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.313.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.054Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.318.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.057Z,ns_1@cb.local:ns_node_disco<0.321.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[error_logger:info,2023-05-15T19:22:22.057Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.320.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.057Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[user:info,2023-05-15T19:22:22.057Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_init:84]Initial otp cookie generated: {sanitized,
                                  <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.058Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]}]
[ns_server:debug,2023-05-15T19:22:22.058Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]
[ns_server:debug,2023-05-15T19:22:22.058Z,ns_1@cb.local:<0.322.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.061Z,ns_1@cb.local:<0.322.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:warn,2023-05-15T19:22:22.062Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:info,2023-05-15T19:22:22.062Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.321.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.062Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-05-15T19:22:22.062Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2023-05-15T19:22:22.064Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-15T19:22:22.064Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-15T19:22:22.066Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.324.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.069Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.325.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.072Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2023-05-15T19:22:22.072Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.326.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.072Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2023-05-15T19:22:22.073Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2023-05-15T19:22:22.073Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2023-05-15T19:22:22.073Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-15T19:22:22.073Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              1442687077},
                                                                             {0,
                                                                              1442687077},
                                                                             false,
                                                                             []}
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:<0.333.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:<0.334.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:<0.333.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:<0.334.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-15T19:22:22.074Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]
[ns_server:debug,2023-05-15T19:22:22.075Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-05-15T19:22:22.075Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-05-15T19:22:22.075Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U"...>>,
  <<"*****">>}]
[ns_server:debug,2023-05-15T19:22:22.076Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2023-05-15T19:22:22.076Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[ns_server:debug,2023-05-15T19:22:22.076Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|inet]
[ns_server:debug,2023-05-15T19:22:22.076Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2023-05-15T19:22:22.076Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{enabled,true},{timeout,120},{max_nodes,1},{count,0}]
[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[error_logger:info,2023-05-15T19:22:22.077Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.327.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[],{configs,[]}]
[error_logger:info,2023-05-15T19:22:22.077Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.319.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1129
[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2023-05-15T19:22:22.077Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,auto_failover_cfg,
                               auto_reprovision_cfg,autocompaction,buckets,
                               cbas_memory_quota,cert_and_pkey,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,secure_headers,
                               server_groups,set_view_update_daemon,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',audit},
                               {node,'ns_1@cb.local',capi_port},
                               {node,'ns_1@cb.local',cbas_admin_port},
                               {node,'ns_1@cb.local',cbas_cc_client_port},
                               {node,'ns_1@cb.local',cbas_cc_cluster_port},
                               {node,'ns_1@cb.local',cbas_cc_http_port},
                               {node,'ns_1@cb.local',cbas_cluster_port},
                               {node,'ns_1@cb.local',cbas_console_port},
                               {node,'ns_1@cb.local',cbas_data_port},
                               {node,'ns_1@cb.local',cbas_debug_port},
                               {node,'ns_1@cb.local',cbas_dirs},
                               {node,'ns_1@cb.local',cbas_http_port},
                               {node,'ns_1@cb.local',cbas_messaging_port},
                               {node,'ns_1@cb.local',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@cb.local',cbas_metadata_port},
                               {node,'ns_1@cb.local',cbas_parent_port},
                               {node,'ns_1@cb.local',cbas_replication_port},
                               {node,'ns_1@cb.local',cbas_result_port},
                               {node,'ns_1@cb.local',cbas_ssl_port},
                               {node,'ns_1@cb.local',compaction_daemon},
                               {node,'ns_1@cb.local',config_version},
                               {node,'ns_1@cb.local',erl_external_listeners},
                               {node,'ns_1@cb.local',eventing_debug_port},
                               {node,'ns_1@cb.local',eventing_dir},
                               {node,'ns_1@cb.local',eventing_http_port},
                               {node,'ns_1@cb.local',eventing_https_port},
                               {node,'ns_1@cb.local',fts_grpc_port},
                               {node,'ns_1@cb.local',fts_grpc_ssl_port},
                               {node,'ns_1@cb.local',fts_http_port},
                               {node,'ns_1@cb.local',fts_ssl_port}]..)
[ns_server:debug,2023-05-15T19:22:22.078Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2023-05-15T19:22:22.078Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2023-05-15T19:22:22.078Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2023-05-15T19:22:22.078Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2023-05-15T19:22:22.079Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
30
[ns_server:debug,2023-05-15T19:22:22.079Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2023-05-15T19:22:22.079Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
1492
[ns_server:debug,2023-05-15T19:22:22.079Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@cb.local']
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
['ns_1@cb.local']
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
null
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2023-05-15T19:22:22.080Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2023-05-15T19:22:22.081Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2023-05-15T19:22:22.081Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2023-05-15T19:22:22.081Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2023-05-15T19:22:22.081Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',audit} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8092]
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9110]
[ns_server:debug,2023-05-15T19:22:22.082Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_client_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9113]
[ns_server:debug,2023-05-15T19:22:22.083Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9112]
[ns_server:debug,2023-05-15T19:22:22.083Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9111]
[ns_server:debug,2023-05-15T19:22:22.083Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9115]
[ns_server:debug,2023-05-15T19:22:22.083Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_console_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9114]
[ns_server:debug,2023-05-15T19:22:22.083Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_data_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9116]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8095]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_messaging_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9118]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9119]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9121]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_parent_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9122]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_replication_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9120]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_result_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9117]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18095]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',compaction_daemon} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',config_version} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|{6,5}]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9140]
[ns_server:debug,2023-05-15T19:22:22.084Z,ns_1@cb.local:memcached_passwords<0.310.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8096]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18096]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9130]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19130]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8094]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18094]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9100]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9102]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19102]
[ns_server:debug,2023-05-15T19:22:22.085Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_scan_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9101]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stcatchup_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9104]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stinit_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9103]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stmaint_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9105]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',is_enterprise} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',isasl} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2023-05-15T19:22:22.086Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 active]
[ns_server:debug,2023-05-15T19:22:22.088Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2023-05-15T19:22:22.089Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_config} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2023-05-15T19:22:22.089Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|11206]
[ns_server:debug,2023-05-15T19:22:22.090Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_defaults} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2023-05-15T19:22:22.090Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',moxi} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,0}]
[ns_server:debug,2023-05-15T19:22:22.090Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ns_log} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2023-05-15T19:22:22.090Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',port_servers} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8093]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',rest} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',saslauthd_enabled} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18092]
[ns_server:debug,2023-05-15T19:22:22.091Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18093]
[ns_server:debug,2023-05-15T19:22:22.092Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18091]
[ns_server:debug,2023-05-15T19:22:22.092Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',uuid} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 <<"949a059dcc29e773ec37709a7973341b">>]
[ns_server:debug,2023-05-15T19:22:22.092Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',xdcr_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9998]
[ns_server:debug,2023-05-15T19:22:22.092Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[ns_server:debug,2023-05-15T19:22:22.092Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]}]
[error_logger:info,2023-05-15T19:22:22.096Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.339.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.101Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.341.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.102Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.344.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.102Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.345.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.103Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.346.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.107Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.347.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.111Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2023-05-15T19:22:22.112Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[error_logger:info,2023-05-15T19:22:22.118Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.353.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.118Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.355.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.119Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.352.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.123Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.360.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.129Z,ns_1@cb.local:ns_heart<0.353.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2023-05-15T19:22:22.129Z,ns_1@cb.local:ns_heart<0.353.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:debug,2023-05-15T19:22:22.131Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.310.0>
[ns_server:debug,2023-05-15T19:22:22.131Z,ns_1@cb.local:memcached_passwords<0.310.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:22.131Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.310.0>
[ns_server:info,2023-05-15T19:22:22.134Z,ns_1@cb.local:<0.351.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:debug,2023-05-15T19:22:22.134Z,ns_1@cb.local:<0.356.0>:restartable:start_child:98]Started child process <0.359.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2023-05-15T19:22:22.134Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.362.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.135Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.356.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2023-05-15T19:22:22.134Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2023-05-15T19:22:22.135Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.365.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.140Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2023-05-15T19:22:22.141Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-15T19:22:22.142Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-15T19:22:22.147Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.366.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.147Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.367.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.148Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.368.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.148Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.369.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.152Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.371.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.158Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.373.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.158Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.375.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.178Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.378.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.178Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.380.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.183Z,ns_1@cb.local:ns_heart<0.353.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-15T19:22:22.185Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.381.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.189Z,ns_1@cb.local:ns_heart<0.353.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-15T19:22:22.193Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.382.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.197Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.383.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.197Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.384.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.199Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-15T19:22:22.200Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:22.201Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:22.202Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2023-05-15T19:22:22.203Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.386.0>,menelaus_web}
             started: [{pid,<0.387.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.211Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-15T19:22:22.212Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:22.212Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:22.212Z,ns_1@cb.local:<0.386.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-15T19:22:22.213Z,ns_1@cb.local:<0.385.0>:restartable:start_child:98]Started child process <0.386.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2023-05-15T19:22:22.213Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.386.0>,menelaus_web}
             started: [{pid,<0.407.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.213Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.385.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.216Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.426.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.227Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.429.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.231Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.432.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.232Z,ns_1@cb.local:ns_heart_slow_status_updater<0.355.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2023-05-15T19:22:22.232Z,ns_1@cb.local:ns_heart_slow_status_updater<0.355.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2023-05-15T19:22:22.234Z,ns_1@cb.local:ns_heart_slow_status_updater<0.355.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-05-15T19:22:22.234Z,ns_1@cb.local:ns_heart_slow_status_updater<0.355.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-15T19:22:22.236Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.441.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2023-05-15T19:22:22.236Z,ns_1@cb.local:ns_server_sup<0.302.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2023-05-15T19:22:22.236Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.376.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.237Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.447.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.240Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.451.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.240Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.452.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.240Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.450.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.245Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.454.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.257Z,ns_1@cb.local:ns_ports_setup<0.447.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2023-05-15T19:22:22.270Z,ns_1@cb.local:memcached_auth_server<0.463.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2023-05-15T19:22:22.270Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.463.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.270Z,ns_1@cb.local:ns_audit_cfg<0.466.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      1},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {disabled,
                                                                                      []},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2023-05-15T19:22:22.303Z,ns_1@cb.local:ns_audit_cfg<0.466.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2023-05-15T19:22:22.304Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.466.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-15T19:22:22.305Z,ns_1@cb.local:<0.469.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2023-05-15T19:22:22.308Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.470.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.308Z,ns_1@cb.local:memcached_config_mgr<0.471.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-05-15T19:22:22.308Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.471.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.311Z,ns_1@cb.local:<0.472.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2023-05-15T19:22:22.312Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.472.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.317Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.473.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.323Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.475.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.333Z,ns_1@cb.local:ns_ports_setup<0.447.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2023-05-15T19:22:22.333Z,ns_1@cb.local:memcached_config_mgr<0.471.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2023-05-15T19:22:22.335Z,ns_1@cb.local:terse_cluster_info_uploader<0.475.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":5,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}]}">>
[ns_server:warn,2023-05-15T19:22:22.336Z,ns_1@cb.local:<0.480.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2023-05-15T19:22:22.337Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.481.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:22.341Z,ns_1@cb.local:memcached_config_mgr<0.471.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[error_logger:info,2023-05-15T19:22:22.344Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.483.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.344Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.477.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.351Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.486.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.356Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.490.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.364Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.492.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.366Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.493.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.367Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.495.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.371Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.496.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.371Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.499.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.380Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.500.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.391Z,ns_1@cb.local:memcached_config_mgr<0.471.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2023-05-15T19:22:22.392Z,ns_1@cb.local:memcached_config_mgr<0.471.0>:memcached_config_mgr:init:86]activated memcached port server
[error_logger:info,2023-05-15T19:22:22.393Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.502.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.393Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.504.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.396Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.505.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.401Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.507.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.402Z,ns_1@cb.local:goxdcr_status_keeper<0.507.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-05-15T19:22:22.408Z,ns_1@cb.local:goxdcr_status_keeper<0.507.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-15T19:22:22.408Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.511.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.413Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.513.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.421Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.514.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.425Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.517.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.428Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.520.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.429Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.512.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.429Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.523.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.429Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.510.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:22.439Z,ns_1@cb.local:<0.527.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.527.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2023-05-15T19:22:22.442Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:22:22.442Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:22:22.442Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:22:22.442Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:22:22.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:22:22.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2023-05-15T19:22:22.443Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.525.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.445Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.529.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.445Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.528.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.446Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.530.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.456Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.534.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.460Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.535.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.460Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.533.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.465Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.537.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.469Z,ns_1@cb.local:leader_registry_sup<0.536.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2023-05-15T19:22:22.469Z,ns_1@cb.local:leader_registry_sup<0.536.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2023-05-15T19:22:22.469Z,ns_1@cb.local:leader_registry_sup<0.536.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2023-05-15T19:22:22.473Z,ns_1@cb.local:mb_master<0.540.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2023-05-15T19:22:22.474Z,ns_1@cb.local:leader_registry<0.537.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2023-05-15T19:22:22.490Z,ns_1@cb.local:mb_master<0.540.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[ns_server:debug,2023-05-15T19:22:22.493Z,ns_1@cb.local:leader_lease_acquirer<0.543.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2023-05-15T19:22:22.493Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.543.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.497Z,ns_1@cb.local:leader_quorum_nodes_manager<0.545.0>:leader_utils:wait_cluster_is_55:54]Delaying start since cluster is not fully upgraded to 5.5 yet.
[error_logger:info,2023-05-15T19:22:22.497Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.545.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.502Z,ns_1@cb.local:mb_master_sup<0.542.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.547.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-15T19:22:22.502Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.547.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.505Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.549.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.511Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,0]}]

[ns_server:info,2023-05-15T19:22:22.511Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,1]
[ns_server:debug,2023-05-15T19:22:22.512Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,1]},
 {set,client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {set,buckets,[{configs,[]}]}]

[ns_server:info,2023-05-15T19:22:22.515Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [5,5]
[ns_server:debug,2023-05-15T19:22:22.515Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[5,5]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120}">>},
 {set,{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {set,buckets,[{configs,[]}]},
 {delete,{rbac_upgrade,[5,5]}},
 {set,audit,
      [{enabled,[]},
       {disabled_users,[]},
       {auditd_enabled,false},
       {rotate_interval,86400},
       {rotate_size,20971520},
       {disabled,[]},
       {sync,[]},
       {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {set,quorum_nodes,['ns_1@cb.local']},
 {set,scramsha_fallback_salt,<<211,88,119,65,90,45,162,36,255,132,241,44>>}]

[ns_server:info,2023-05-15T19:22:22.516Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,0]
[ns_server:debug,2023-05-15T19:22:22.516Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,0]}]

[ns_server:info,2023-05-15T19:22:22.524Z,ns_1@cb.local:ns_config<0.193.0>:ns_online_config_upgrader:do_upgrade_config:46]Performing online config upgrade to [6,5]
[ns_server:debug,2023-05-15T19:22:22.535Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:do_upgrade_config:757]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Eventing function definition was created or updated">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Eventing function definition was deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Eventing function definition was read">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Eventing deployed functions list was read">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Eventing function draft definitions were read">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Eventing function draft definitions were deleted">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Save a draft definition to the store">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Save settings for a given app">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Get config for eventing">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Save config for eventing">>},
         {enabled,true},
         {module,eventing}]},
       {32782,
        [{name,<<"Cleanup Eventing">>},
         {description,<<"Clears up app definitions and settings from metakv">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Get settings for a given app">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Import a list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Export the list of functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Eventing running function list was read">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]}]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true}]},
 {set,max_bucket_count,30},
 {set,retry_rebalance,
      [{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>}]

[ns_server:debug,2023-05-15T19:22:22.538Z,ns_1@cb.local:leader_lease_acquirer<0.543.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2023-05-15T19:22:22.538Z,ns_1@cb.local:leader_quorum_nodes_manager<0.545.0>:leader_utils:wait_cluster_is_55_loop:78]Cluster upgraded to 5.5. Starting.
[ns_server:debug,2023-05-15T19:22:22.538Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2023-05-15T19:22:22.538Z,ns_1@cb.local:leader_quorum_nodes_manager<0.545.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2023-05-15T19:22:22.538Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,1442687077},
                                                                {0,1442687077},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               1442687077},
                                                                              {0,
                                                                               1442687077},
                                                                              false,
                                                                              []}
[ns_server:debug,2023-05-15T19:22:22.539Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([audit,audit_decriptors,auto_failover_cfg,
                               buckets,client_cert_auth,
                               cluster_compat_version,max_bucket_count,
                               quorum_nodes,retry_rebalance,
                               scramsha_fallback_salt,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2023-05-15T19:22:22.540Z,ns_1@cb.local:memcached_permissions<0.313.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-15T19:22:22.542Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2023-05-15T19:22:22.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<211,88,119,65,90,45,162,36,255,132,241,44>>]
[ns_server:debug,2023-05-15T19:22:22.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},6,5]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:handle_call:122]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:leader_quorum_nodes_manager<0.545.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:handle_call:128]Fully synchronized config in 22 us
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[user:warn,2023-05-15T19:22:22.545Z,ns_1@cb.local:compat_mode_manager<0.550.0>:compat_mode_manager:handle_consider_switching_compat_mode:49]Changed cluster compat mode from undefined to [6,5]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}],{configs,[]}]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|30]
[ns_server:debug,2023-05-15T19:22:22.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 'ns_1@cb.local']
[error_logger:info,2023-05-15T19:22:22.545Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.550.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{6,63851397742}}]}]
[ns_server:warn,2023-05-15T19:22:22.548Z,ns_1@cb.local:<0.555.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[error_logger:error,2023-05-15T19:22:22.548Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]Error in process <0.555.0> on node 'ns_1@cb.local' with exit value:
{{badmatch,{error,couldnt_connect_to_memcached}},
 [{ns_memcached,'-config_validate/1-fun-0-',1,
                [{file,"src/ns_memcached.erl"},{line,1382}]},
  {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,197}]}]}

[error_logger:error,2023-05-15T19:22:22.549Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]** Generic server <0.471.0> terminating 
** Last message in was do_check
** When Server state == {state,<12939.116.0>,
                               <<"{\n  \"active_external_users_push_interval\": 600,\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    \"minidump_dir\": \"/opt/couchbase/var/lib/couchbase/crash\"\n  },\n  \"client_cert_auth\": {\n    \"state\": \"disable\"\n  },\n  \"collections_enabled\": false,\n  \"connection_idle_time\": 0,\n  \"datatype_snappy\": false,\n  \"dedupe_nmvb_maps\": false,\n  \"external_auth_service\": false,\n  \"interfaces\": [\n    {\n      \"host\": \"*\",\n      \"port\": 11210,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11209,\n      \"system\": true,\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11207,\n      \"ssl\": {\n        \"key\": \"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem\",\n        \"cert\": \"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem\"\n      },\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    },\n    {\n      \"host\": \"*\",\n      \"port\": 11206,\n      \"system\": true,\n      \"ssl\": {\n        \"key\": \"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem\",\n        \"cert\": \"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem\"\n      },\n      \"ipv4\": \"required\",\n      \"ipv6\": \"optional\"\n    }\n  ],\n  \"logger\": {\n    \"filename\": \"/opt/couchbase/var/lib/couchbase/logs/memcached.log\",\n    \"cyclesize\": 10485760,\n    \"sleeptime\": 19\n  },\n  \"max_connections\": 65000,\n  \"num_reader_threads\": \"default\",\n  \"num_writer_threads\": \"default\",\n  \"opentracing\": {\n    \"enabled\": false,\n    \"module\": \"\",\n    \"config\": \"\"\n  },\n  \"privilege_debug\": false,\n  \"rbac_file\": \"/opt/couchbase/var/lib/couchbase/config/memcached.rbac\",\n  \"root\": \"/opt/couchbase\",\n  \"scramsha_fallback_salt\": \"c2FsdA==\",\n  \"ssl_cipher_list\": {\n    \"tls 1.2\": \"HIGH\",\n    \"tls 1.3\": \"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256\"\n  },\n  \"ssl_cipher_order\": true,\n  \"ssl_minimum_protocol\": \"tlsv1\",\n  \"system_connections\": 5000,\n  \"tracing_enabled\": true,\n  \"verbosity\": 0,\n  \"xattr_enabled\": true\n}\n">>}
** Reason for termination == 
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/1-fun-0-',1,
                   [{file,"src/ns_memcached.erl"},{line,1382}]},
     {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,197}]}]}

[ns_server:debug,2023-05-15T19:22:22.554Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.313.0>
[ns_server:debug,2023-05-15T19:22:22.554Z,ns_1@cb.local:memcached_permissions<0.313.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:22.555Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.313.0>
[ns_server:debug,2023-05-15T19:22:22.563Z,ns_1@cb.local:leader_lease_agent<0.535.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"69e29cc338e2047067b01d5fd46cb73a">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2023-05-15T19:22:22.572Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-05-15T19:22:22.576Z,ns_1@cb.local:<0.482.0>:remote_monitors:handle_down:158]Caller of remote monitor <0.471.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/1-fun-0-',1,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1382}]},
                                               {async,'-async_init/4-fun-1-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,197}]}]}. Exiting
[ns_server:debug,2023-05-15T19:22:22.576Z,ns_1@cb.local:<0.485.0>:ns_pubsub:do_subscribe_link_continue:152]Parent process of subscription {ns_config_events,<0.471.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/1-fun-0-',
                                                                                  1,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1382}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-1-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    197}]}]}
[ns_server:debug,2023-05-15T19:22:22.591Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [ssl_service,capi_ssl_service] about client_cert_auth change
[ns_server:debug,2023-05-15T19:22:22.591Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service,ssl_service]
[ns_server:debug,2023-05-15T19:22:22.591Z,ns_1@cb.local:<0.223.0>:restartable:loop:71]Restarting child <0.224.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.578.0>,#Ref<0.2935111024.1579417608.137829>}
[error_logger:error,2023-05-15T19:22:22.591Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.471.0>
    registered_name: memcached_config_mgr
    exception error: no match of right hand side value {error,
                                                        couldnt_connect_to_memcached}
      in function  ns_memcached:'-config_validate/1-fun-0-'/1 (src/ns_memcached.erl, line 1382)
      in call from async:'-async_init/4-fun-1-'/3 (src/async.erl, line 197)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.206.0>,
                  ns_server_cluster_sup,root_sup,<0.118.0>]
    message_queue_len: 2
    messages: [do_check,do_check]
    links: [<0.302.0>,<0.485.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 27
    reductions: 87533
  neighbours:

[error_logger:info,2023-05-15T19:22:22.592Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.573.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.592Z,ns_1@cb.local:<0.223.0>:restartable:shutdown_child:120]Successfully terminated process <0.224.0>
[ns_server:info,2023-05-15T19:22:22.596Z,ns_1@cb.local:ns_orchestrator_child_sup<0.572.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.580.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-15T19:22:22.596Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.580.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.605Z,ns_1@cb.local:<0.579.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2023-05-15T19:22:22.605Z,ns_1@cb.local:ns_orchestrator_child_sup<0.572.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.582.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-15T19:22:22.606Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.582.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.606Z,ns_1@cb.local:ns_orchestrator_child_sup<0.572.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.583.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-15T19:22:22.606Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.583.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.606Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.572.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:info,2023-05-15T19:22:22.609Z,ns_1@cb.local:<0.567.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"69e29cc338e2047067b01d5fd46cb73a">>)
[ns_server:info,2023-05-15T19:22:22.611Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-15T19:22:22.611Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:22.612Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:22.612Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2023-05-15T19:22:22.613Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.581.0>,menelaus_web}
             started: [{pid,<0.585.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.614Z,ns_1@cb.local:<0.603.0>:auto_failover:init:185]init auto_failover.
[user:info,2023-05-15T19:22:22.614Z,ns_1@cb.local:<0.603.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2023-05-15T19:22:22.617Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-15T19:22:22.618Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-15T19:22:22.618Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-15T19:22:22.619Z,ns_1@cb.local:<0.581.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-15T19:22:22.619Z,ns_1@cb.local:<0.223.0>:restartable:start_child:98]Started child process <0.581.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:info,2023-05-15T19:22:22.619Z,ns_1@cb.local:<0.578.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service ssl_service
[ns_server:info,2023-05-15T19:22:22.620Z,ns_1@cb.local:ns_orchestrator_sup<0.548.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.603.0> on 'ns_1@cb.local'

[ns_server:info,2023-05-15T19:22:22.620Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [ssl_service,capi_ssl_service]
[ns_server:debug,2023-05-15T19:22:22.620Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{7,63851397742}}]}]
[ns_server:info,2023-05-15T19:22:22.620Z,ns_1@cb.local:mb_master_sup<0.542.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.624.0> on 'ns_1@cb.local'

[ns_server:debug,2023-05-15T19:22:22.620Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2023-05-15T19:22:22.620Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[error_logger:info,2023-05-15T19:22:22.619Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.581.0>,menelaus_web}
             started: [{pid,<0.605.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.624Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.603.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.548.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-15T19:22:22.625Z,ns_1@cb.local:<0.629.0>:license_reporting:init:66]Starting license_reporting server
[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.624.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:mb_master_sup<0.542.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.629.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.629.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.625Z,ns_1@cb.local:<0.531.0>:restartable:start_child:98]Started child process <0.532.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.540.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.536.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.531.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.630Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.631.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.630Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.633.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.630Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.634.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.633Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.635.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.636Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.636.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.649Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[error_logger:info,2023-05-15T19:22:22.649Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.639.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.649Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.641.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.649Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.642.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.654Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.648.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.657Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.654.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.657Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.638.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.660Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.656.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.666Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[ns_server:debug,2023-05-15T19:22:22.666Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2023-05-15T19:22:22.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.657.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.666Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.207.0>
[error_logger:error,2023-05-15T19:22:22.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================SUPERVISOR REPORT=========================
     Supervisor: {local,ns_server_sup}
     Context:    child_terminated
     Reason:     {{badmatch,{error,couldnt_connect_to_memcached}},
                  [{ns_memcached,'-config_validate/1-fun-0-',1,
                                 [{file,"src/ns_memcached.erl"},{line,1382}]},
                   {async,'-async_init/4-fun-1-',3,
                          [{file,"src/async.erl"},{line,197}]}]}
     Offender:   [{pid,<0.471.0>},
                  {name,memcached_config_mgr},
                  {mfargs,{memcached_config_mgr,start_link,[]}},
                  {restart_type,{permanent,4}},
                  {shutdown,1000},
                  {child_type,worker}]


[ns_server:debug,2023-05-15T19:22:22.666Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2023-05-15T19:22:22.666Z,ns_1@cb.local:<0.206.0>:restartable:start_child:98]Started child process <0.207.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2023-05-15T19:22:22.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.302.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.659.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:22.666Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.206.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.669Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.660.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:22.669Z,ns_1@cb.local:<0.5.0>:child_erlang:child_loop:130]219: Entered child_loop
[error_logger:info,2023-05-15T19:22:22.669Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.185.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-15T19:22:22.670Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@cb.local'

[ns_server:debug,2023-05-15T19:22:22.670Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2023-05-15T19:22:22.671Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[ns_server:debug,2023-05-15T19:22:22.672Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2023-05-15T19:22:22.673Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2023-05-15T19:22:22.676Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:init:89]found memcached port to be already active
[ns_server:debug,2023-05-15T19:22:22.692Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:apply_changed_memcached_config:179]New memcached config is hot-reloadable.
[ns_server:debug,2023-05-15T19:22:22.692Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.665.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.665.0>
[ns_server:debug,2023-05-15T19:22:22.692Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.666.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.666.0>
[ns_server:debug,2023-05-15T19:22:22.692Z,ns_1@cb.local:menelaus_cbauth<0.441.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.665.0>} started
[ns_server:debug,2023-05-15T19:22:22.693Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:do_read_current_memcached_config:287]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2023-05-15T19:22:22.695Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[user:info,2023-05-15T19:22:22.762Z,ns_1@cb.local:memcached_config_mgr<0.659.0>:memcached_config_mgr:hot_reload_config:248]Hot-reloaded memcached.json for config change of the following keys: [<<"client_cert_auth">>,
                                                                      <<"datatype_snappy">>,
                                                                      <<"scramsha_fallback_salt">>]
[ns_server:debug,2023-05-15T19:22:23.320Z,ns_1@cb.local:ns_audit_cfg<0.466.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2023-05-15T19:22:23.342Z,ns_1@cb.local:terse_cluster_info_uploader<0.475.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2023-05-15T19:22:23.346Z,ns_1@cb.local:ns_audit_cfg<0.466.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[ns_server:debug,2023-05-15T19:22:23.616Z,ns_1@cb.local:<0.603.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@cb.local',<<"949a059dcc29e773ec37709a7973341b">>} state new -> up
[ns_server:debug,2023-05-15T19:22:29.127Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],anonymous}
[ns_server:debug,2023-05-15T19:22:29.159Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[ns_server:debug,2023-05-15T19:22:29.159Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{8,63851397749}}]}]
[ns_server:debug,2023-05-15T19:22:29.159Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2023-05-15T19:22:29.159Z,ns_1@cb.local:terse_cluster_info_uploader<0.475.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":8,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:debug,2023-05-15T19:22:29.184Z,ns_1@cb.local:menelaus_ui_auth<0.378.0>:token_server:handle_cast:211]Purge tokens []
[ns_server:debug,2023-05-15T19:22:29.184Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from {[6,5],
                                                                {0,1442687077},
                                                                {0,1442687077},
                                                                false,[]} to {[6,
                                                                               5],
                                                                              {0,
                                                                               1442687077},
                                                                              {0,
                                                                               1442687077},
                                                                              true,
                                                                              []}
[ns_server:debug,2023-05-15T19:22:29.184Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{9,63851397749}}]}]
[ns_server:debug,2023-05-15T19:22:29.184Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2023-05-15T19:22:29.185Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([rest_creds,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[ns_server:debug,2023-05-15T19:22:29.188Z,ns_1@cb.local:memcached_passwords<0.310.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-15T19:22:29.189Z,ns_1@cb.local:memcached_permissions<0.313.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-15T19:22:29.197Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.313.0>
[ns_server:debug,2023-05-15T19:22:29.197Z,ns_1@cb.local:memcached_permissions<0.313.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:29.197Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.313.0>
[ns_server:debug,2023-05-15T19:22:29.205Z,ns_1@cb.local:ns_config_rep<0.327.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([uuid,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[ns_server:debug,2023-05-15T19:22:29.206Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{10,63851397749}}]}]
[ns_server:debug,2023-05-15T19:22:29.205Z,ns_1@cb.local:ns_audit<0.470.0>:ns_audit:handle_call:125]Audit password_change: [{identity,{[{domain,builtin},
                                    {user,<<"<ud>admin</ud>">>}]}},
                        {real_userid,{[{domain,anonymous},
                                       {user,<<"<ud></ud>">>}]}},
                        {remote,{[{ip,<<"127.0.0.1">>},{port,37682}]}},
                        {timestamp,<<"2023-05-15T19:22:29.205Z">>}]
[ns_server:debug,2023-05-15T19:22:29.206Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
uuid ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 <<"b1cf035dd538cdacd1c975b6e16f1302">>]
[error_logger:info,2023-05-15T19:22:29.208Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.828.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:29.210Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-05-15T19:22:29.211Z,ns_1@cb.local:ns_ports_setup<0.447.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,projector,goxdcr]
[error_logger:info,2023-05-15T19:22:29.216Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.830.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-15T19:22:29.223Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.832.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-15T19:22:29.226Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:302]Suspended by process <0.310.0>
[ns_server:debug,2023-05-15T19:22:29.226Z,ns_1@cb.local:memcached_passwords<0.310.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-15T19:22:29.226Z,ns_1@cb.local:users_storage<0.267.0>:replicated_dets:handle_call:309]Released by process <0.310.0>
[ns_server:debug,2023-05-15T19:22:29.238Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac] succeeded
[ns_server:debug,2023-05-15T19:22:29.244Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2023-05-15T19:22:29.248Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],anonymous}
[ns_server:debug,2023-05-15T19:22:29.267Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [isasl] succeeded
[menelaus:info,2023-05-15T19:22:29.301Z,ns_1@cb.local:<0.398.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6058>,'POST',"/settigs/stats",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"57"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[menelaus:info,2023-05-15T19:22:29.312Z,ns_1@cb.local:<0.394.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6060>,'POST',"/settings/compaction",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"54"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[ns_server:debug,2023-05-15T19:22:29.412Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2023-05-15T19:22:29.413Z,ns_1@cb.local:json_rpc_connection-projector-cbauth<0.841.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.841.0>
[ns_server:debug,2023-05-15T19:22:29.413Z,ns_1@cb.local:menelaus_cbauth<0.441.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"projector-cbauth",<0.841.0>} started
[ns_server:debug,2023-05-15T19:22:29.436Z,ns_1@cb.local:compiled_roles_cache<0.269.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2023-05-15T19:22:52.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:22:52.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:22:52.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:22:52.443Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:23:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:23:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:23:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:23:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:23:34.537Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:23:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:23:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:23:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:23:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:24:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:24:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:24:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:24:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:24:49.537Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:24:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:24:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:24:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:24:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:25:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:25:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:25:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:25:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:25:52.449Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:25:52.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:25:52.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:25:52.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:26:04.538Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:26:22.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:26:22.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:26:22.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:26:22.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:26:52.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:26:52.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:26:52.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:26:52.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:27:19.539Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:27:22.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:27:22.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:27:22.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:27:22.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:27:52.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:27:52.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:27:52.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:27:52.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:28:22.454Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:28:22.454Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:28:22.454Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:28:22.454Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:28:34.541Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:28:52.455Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:28:52.455Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:28:52.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:28:52.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:29:22.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:29:22.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:29:22.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:29:22.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:29:49.542Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:29:49.569Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T19:29:49.569Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T19:29:52.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:29:52.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:29:52.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:29:52.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:30:22.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:30:22.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:30:22.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:30:22.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:30:52.459Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:30:52.459Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:30:52.459Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:30:52.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:31:04.517Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:31:22.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:31:22.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:31:22.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:31:22.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:31:52.461Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:31:52.461Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:31:52.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:31:52.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:32:19.544Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:32:19.569Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:32:22.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:32:22.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:32:22.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:32:22.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:32:52.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:32:52.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:32:52.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:32:52.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:33:22.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:33:22.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:33:22.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:33:22.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:33:34.545Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:33:52.465Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:33:52.465Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:33:52.465Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:33:52.465Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:34:22.466Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:34:22.466Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:34:22.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:34:22.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:34:49.546Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:34:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:34:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:34:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:34:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:35:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:35:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:35:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:35:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:35:52.469Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:35:52.469Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:35:52.469Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:35:52.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:36:04.547Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:36:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:36:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:36:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:36:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:36:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:36:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:36:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:36:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:37:19.548Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:37:19.570Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T19:37:19.570Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T19:37:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:37:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:37:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:37:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:37:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:37:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:37:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:37:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:38:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:38:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:38:22.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:38:22.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:38:34.549Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:38:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:38:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:38:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:38:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:39:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:39:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:39:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:39:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:39:49.550Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:39:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:39:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:39:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:39:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:40:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:40:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:40:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:40:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:40:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:40:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:40:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:40:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:41:04.551Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:41:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:41:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:41:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:41:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:41:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:41:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:41:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:41:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:42:19.552Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:42:19.570Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:42:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:42:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:42:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:42:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:42:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:42:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:42:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:42:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:43:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:43:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:43:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:43:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:43:34.553Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:43:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:43:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:43:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:43:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:44:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:44:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:44:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:44:22.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:44:49.554Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:44:49.571Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T19:44:49.571Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T19:44:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:44:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:44:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:44:52.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:45:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:45:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:45:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:45:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:45:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:45:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:45:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:45:52.475Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:46:04.555Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:46:22.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:46:22.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:46:22.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:46:22.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:46:52.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:46:52.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:46:52.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:46:52.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:47:19.556Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:47:22.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:47:22.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:47:22.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:47:22.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:47:52.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:47:52.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:47:52.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:47:52.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:48:22.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:48:22.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:48:22.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:48:22.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:48:34.539Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:48:52.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:48:52.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:48:52.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:48:52.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:49:22.482Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:49:22.482Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:49:22.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:49:22.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:49:49.558Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:49:52.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:49:52.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:49:52.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:49:52.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:50:22.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:50:22.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:50:22.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:50:22.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:50:52.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:50:52.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:50:52.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:50:52.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:51:04.559Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:51:22.486Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:51:22.486Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:51:22.487Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:51:22.487Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:51:52.487Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:51:52.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:51:52.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:51:52.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:52:19.560Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:52:19.571Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:52:19.572Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T19:52:19.572Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T19:52:22.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:52:22.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:52:22.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:52:22.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:52:52.489Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:52:52.489Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:52:52.489Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:52:52.489Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:53:22.490Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:53:22.490Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:53:22.490Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:53:22.490Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:53:34.561Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:53:52.491Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:53:52.491Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:53:52.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:53:52.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:54:22.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:54:22.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:54:22.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:54:22.492Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:54:49.562Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:54:52.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:54:52.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:54:52.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:54:52.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:55:22.494Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:55:22.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:55:22.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:55:22.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:55:52.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:55:52.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:55:52.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:55:52.495Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:56:04.564Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:56:22.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:56:22.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:56:22.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:56:22.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:56:52.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:56:52.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:56:52.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:56:52.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:57:19.566Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:57:22.498Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:57:22.498Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:57:22.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:57:22.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:57:52.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:57:52.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:57:52.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:57:52.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:58:22.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:58:22.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:58:22.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:58:22.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:58:34.567Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:58:52.501Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:58:52.501Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:58:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:58:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:59:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:59:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:59:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:59:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:59:49.568Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T19:59:49.573Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T19:59:49.573Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T19:59:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:59:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T19:59:52.503Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T19:59:52.503Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:00:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:00:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:00:22.549Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:00:22.550Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:00:52.549Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:00:52.550Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:00:52.594Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:00:52.594Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:01:04.569Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:01:22.595Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:01:22.595Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:01:22.596Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:01:22.596Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:01:52.596Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:01:52.596Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:01:52.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:01:52.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:02:19.570Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:02:19.572Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:02:22.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:02:22.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:02:22.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:02:22.597Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:02:52.598Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:02:52.598Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:02:52.598Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:02:52.598Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:03:22.599Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:03:22.599Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:03:22.599Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:03:22.599Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:03:34.570Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:03:52.600Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:03:52.600Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:03:52.600Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:03:52.600Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:04:22.601Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:04:22.601Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:04:22.601Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:04:22.601Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:04:49.572Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:04:52.602Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:04:52.602Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:04:52.602Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:04:52.602Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:05:22.603Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:05:22.603Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:05:22.603Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:05:22.603Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:05:52.604Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:05:52.604Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:05:52.604Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:05:52.604Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:06:04.572Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:06:22.605Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:06:22.605Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:06:22.605Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:06:22.605Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:06:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:06:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:06:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:06:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:07:19.574Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:07:19.602Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:07:19.602Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:07:22.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:07:22.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:07:22.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:07:22.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:07:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:07:52.606Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:07:52.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:07:52.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:08:22.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:08:22.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:08:22.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:08:22.608Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:08:34.574Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:08:52.609Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:08:52.609Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:08:52.609Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:08:52.609Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:09:22.610Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:09:22.610Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:09:22.610Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:09:22.610Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:09:49.576Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:09:52.612Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:09:52.612Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:09:52.612Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:09:52.612Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:10:22.613Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:10:22.613Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:10:22.613Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:10:22.613Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:10:52.614Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:10:52.614Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:10:52.614Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:10:52.614Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:11:04.578Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:11:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:11:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:11:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:11:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:11:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:11:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:11:52.616Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:11:52.616Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:12:19.575Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:12:19.579Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:12:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:12:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:12:22.617Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:12:22.617Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:12:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:12:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:12:52.618Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:12:52.618Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:13:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:13:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:13:22.619Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:13:22.619Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:13:34.580Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:13:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:13:52.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:13:52.620Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:13:52.620Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:14:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:14:22.615Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:14:22.621Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:14:22.621Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:14:49.581Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:14:49.603Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:14:49.603Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:14:52.618Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:14:52.618Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:14:52.622Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:14:52.622Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:15:22.620Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:15:22.620Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:15:22.623Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:15:22.623Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:15:52.621Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:15:52.621Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:15:52.624Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:15:52.624Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:16:04.582Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:16:22.622Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:16:22.622Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:16:22.625Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:16:22.625Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:16:52.623Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:16:52.623Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:16:52.626Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:16:52.626Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:17:19.584Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:17:22.624Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:17:22.624Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:17:22.627Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:17:22.627Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:17:52.625Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:17:52.625Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:17:52.628Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:17:52.628Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:18:22.626Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:18:22.626Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:18:22.629Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:18:22.629Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:18:34.585Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:18:52.627Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:18:52.627Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:18:52.630Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:18:52.630Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:19:22.628Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:19:22.628Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:19:22.631Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:19:22.631Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:19:49.586Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:19:52.649Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:19:52.649Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:19:52.649Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:19:52.649Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:20:22.650Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:20:22.650Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:20:22.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:20:22.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:20:52.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:20:52.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:20:52.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:20:52.651Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:21:04.587Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:21:22.652Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:21:22.652Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:21:22.652Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:21:22.652Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:21:52.653Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:21:52.653Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:21:52.653Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:21:52.653Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:19.577Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:22:19.588Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:22:19.604Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:22:19.604Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:22:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:22.444Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:22.654Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:22.654Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:22.654Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:22.654Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:52.445Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:52.655Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:52.655Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:22:52.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:22:52.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:22.446Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:22.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:22.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:22.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:22.656Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:34.589Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:23:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:52.447Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:52.657Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:52.657Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:23:52.657Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:23:52.657Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:22.448Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:22.658Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:22.658Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:22.658Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:22.658Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:49.591Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:24:52.450Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:52.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:52.659Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:52.659Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:24:52.659Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:24:52.659Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:22.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:22.451Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:22.660Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:22.660Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:22.660Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:22.660Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:52.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:52.452Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:52.661Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:52.661Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:25:52.661Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:25:52.661Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:04.592Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:26:22.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:22.453Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:22.662Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:22.662Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:22.662Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:22.662Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:52.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:52.456Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:52.663Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:52.663Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:26:52.663Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:26:52.663Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:19.593Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:27:22.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:22.457Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:22.664Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:22.664Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:22.664Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:22.664Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:52.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:52.458Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:52.665Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:52.665Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:27:52.666Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:27:52.666Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:22.459Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:22.459Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:22.666Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:22.666Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:22.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:22.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:34.594Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:28:52.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:52.460Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:52.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:52.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:28:52.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:28:52.667Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:22.461Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:22.461Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:22.668Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:22.668Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:22.668Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:22.668Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:49.595Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:29:49.605Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:29:49.605Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:29:52.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:52.462Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:52.669Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:52.669Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:29:52.669Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:29:52.670Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:22.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:22.463Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:22.670Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:22.670Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:22.670Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:22.670Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:52.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:52.464Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:52.671Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:52.671Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:30:52.671Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:30:52.671Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:04.583Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:31:22.466Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:22.466Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:22.672Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:22.672Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:22.672Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:22.672Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:52.467Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:52.673Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:52.673Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:31:52.673Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:31:52.673Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:19.578Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:32:19.597Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:32:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:22.468Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:22.674Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:22.674Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:22.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:22.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:52.469Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:52.469Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:52.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:52.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:32:52.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:32:52.675Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:22.470Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:22.676Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:22.676Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:22.676Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:22.676Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:34.598Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:33:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:52.471Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:52.677Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:52.677Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:33:52.677Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:33:52.677Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:22.472Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:22.678Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:22.678Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:22.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:22.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:49.599Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:34:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:52.473Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:52.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:52.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:34:52.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:34:52.679Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:22.474Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:22.680Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:22.680Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:22.680Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:22.680Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:52.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:52.476Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:52.681Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:52.681Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:35:52.681Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:35:52.681Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:04.600Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:36:22.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:22.477Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:22.682Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:22.682Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:22.682Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:22.682Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:52.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:52.478Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:52.683Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:52.683Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:36:52.683Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:36:52.683Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:19.601Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:37:19.606Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:37:19.606Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:37:22.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:22.479Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:22.684Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:22.684Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:22.684Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:22.685Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:52.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:52.480Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:52.685Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:52.685Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:37:52.685Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:37:52.685Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:22.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:22.481Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:22.686Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:22.686Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:22.686Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:22.686Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:34.602Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:38:52.482Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:52.482Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:52.687Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:52.687Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:38:52.687Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:38:52.687Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:22.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:22.483Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:22.688Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:22.688Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:22.688Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:22.688Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:49.603Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:39:52.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:52.484Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:52.689Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:52.689Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:39:52.689Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:39:52.689Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:22.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:22.485Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:22.690Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:22.690Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:22.690Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:22.690Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:52.486Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:52.486Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:52.691Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:52.691Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:40:52.691Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:40:52.691Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:04.604Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:41:22.487Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:22.487Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:22.692Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:22.692Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:22.692Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:22.692Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:52.488Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:52.489Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:52.693Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:52.693Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:41:52.694Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:41:52.694Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:19.579Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:42:19.605Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:42:22.490Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:22.491Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:22.694Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:22.694Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:22.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:22.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:52.491Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:52.491Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:52.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:52.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:42:52.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:42:52.695Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:22.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:22.493Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:22.696Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:22.696Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:22.697Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:22.697Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:34.606Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:43:52.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:52.496Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:52.697Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:52.697Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:43:52.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:43:52.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:22.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:22.497Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:22.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:22.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:22.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:22.698Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:49.607Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:44:49.607Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:44:49.608Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:44:52.498Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:52.498Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:52.699Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:52.699Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:44:52.699Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:44:52.700Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:22.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:22.499Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:22.700Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:22.700Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:22.700Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:22.701Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:52.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:52.500Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:52.701Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:52.701Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:45:52.701Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:45:52.701Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:04.608Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:46:22.501Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:22.702Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:22.702Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:22.703Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:22.703Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:52.503Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:52.703Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:52.703Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:46:52.703Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:46:52.704Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:19.609Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:47:22.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:22.503Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:22.705Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:22.705Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:22.705Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:22.705Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:52.502Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:52.503Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:52.706Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:52.706Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:47:52.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:47:52.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:22.504Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:22.504Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:22.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:22.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:22.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:22.707Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:34.610Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:48:52.505Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:52.505Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:52.708Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:52.708Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:48:52.708Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:48:52.708Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:22.506Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:22.506Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:22.709Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:22.709Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:22.709Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:22.709Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:49.611Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:49:52.507Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:52.507Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:52.710Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:52.710Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:49:52.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:49:52.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:22.508Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:22.508Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:22.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:22.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:22.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:22.711Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:52.510Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:52.512Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:52.712Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:52.712Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:50:52.712Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:50:52.712Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:04.613Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:51:22.513Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:22.513Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:22.713Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:22.713Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:22.713Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:22.713Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:52.514Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:52.514Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:52.714Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:52.714Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:51:52.714Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:51:52.714Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:19.581Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:52:19.609Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:52:19.609Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:52:19.614Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:52:22.515Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:22.515Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:22.715Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:22.715Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:22.715Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:22.715Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:52.516Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:52.516Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:52.716Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:52.716Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:52:52.716Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:52:52.716Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:22.517Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:22.517Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:22.717Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:22.717Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:22.717Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:22.717Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:34.615Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:53:52.519Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:52.519Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:52.718Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:52.718Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:53:52.718Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:53:52.718Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:22.520Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:22.520Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:22.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:22.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:22.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:22.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:49.616Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:54:52.521Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:52.521Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:52.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:52.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:54:52.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:54:52.719Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:22.522Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:22.522Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:22.720Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:22.720Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:22.721Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:22.721Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:52.523Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:52.523Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:52.721Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:52.721Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:55:52.721Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:55:52.722Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:04.617Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:56:22.524Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:22.524Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:22.722Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:22.722Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:22.722Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:22.722Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:52.525Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:52.525Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:52.723Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:52.723Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:56:52.723Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:56:52.723Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:19.618Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:57:22.526Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:22.526Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:22.724Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:22.724Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:22.724Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:22.724Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:52.527Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:52.527Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:52.725Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:52.725Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:57:52.725Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:57:52.725Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:22.528Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:22.528Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:22.726Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:22.726Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:22.726Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:22.726Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:34.619Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:58:52.529Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:52.529Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:52.727Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:52.727Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:58:52.727Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:58:52.727Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:22.530Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:22.530Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:22.728Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:22.728Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:22.728Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:22.729Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:49.610Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T20:59:49.610Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T20:59:49.620Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T20:59:52.531Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:52.531Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:52.729Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:52.729Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T20:59:52.729Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T20:59:52.729Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:22.532Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:22.533Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:22.730Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:22.730Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:22.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:22.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:52.533Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:52.533Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:52.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:52.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:00:52.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:00:52.731Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:04.622Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:01:22.534Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:22.534Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:22.732Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:22.732Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:22.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:22.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:52.535Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:52.535Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:52.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:52.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:01:52.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:01:52.733Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:19.582Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:cleanup:231]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:02:19.623Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:02:22.536Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:22.536Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:22.734Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:22.734Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:22.734Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:22.734Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:52.574Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:52.574Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:52.735Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:52.735Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:02:52.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:02:52.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:22.575Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:22.575Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:22.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:22.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:22.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:22.736Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:34.624Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:03:52.576Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:52.576Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:52.737Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:52.737Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:03:52.737Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:03:52.737Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:22.577Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:22.577Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:22.738Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:22.738Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:22.738Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:22.738Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:49.625Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:04:52.578Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:52.578Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:52.739Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:52.739Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:04:52.739Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:04:52.740Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:22.579Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:22.579Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:22.740Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:22.740Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:22.740Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:22.740Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:52.580Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:52.580Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:52.741Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:52.741Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:05:52.741Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:05:52.742Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:04.626Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:06:22.581Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:22.582Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:22.743Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:22.743Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:22.743Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:22.743Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:52.582Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:52.582Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:52.744Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:52.744Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:06:52.744Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:06:52.745Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:07:19.611Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:211]Starting roles_cache cache renewal
[ns_server:debug,2023-05-15T21:07:19.611Z,ns_1@cb.local:roles_cache<0.272.0>:active_cache:renew:217]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-05-15T21:07:19.627Z,ns_1@cb.local:ldap_auth_cache<0.261.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-15T21:07:22.617Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:07:22.617Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:07:22.745Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:07:22.745Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-15T21:07:22.745Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-15T21:07:22.745Z,ns_1@cb.local:compaction_daemon<0.525.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2023-05-24T12:22:18.811Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.829Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.830Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.830Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.830Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2023-05-24T12:22:18.830Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2023-05-24T12:22:18.840Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2023-05-24T12:22:18.840Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-24T12:22:18.846Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2023-05-24T12:22:18.858Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.858Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.858Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.861Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.863Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.863Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.863Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.863Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:18.863Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2023-05-24T12:22:18.864Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.865Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.866Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:18.867Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2023-05-24T12:22:18.867Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2023-05-24T12:22:18.867Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2023-05-24T12:22:18.869Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2023-05-24T12:22:18.869Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.869Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2023-05-24T12:22:18.870Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2023-05-24T12:22:18.878Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2023-05-24T12:22:18.879Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2023-05-24T12:22:18.879Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2023-05-24T12:22:18.879Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-24T12:22:18.879Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2818819729.3791388680.8672>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-24T12:22:18.879Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2818819729.3791388680.8672>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.2818819729.3791388679.8612>}
[ns_server:debug,2023-05-24T12:22:18.883Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2023-05-24T12:22:18.883Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2023-05-24T12:22:18.884Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-24T12:22:18.888Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2023-05-24T12:22:18.889Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.893Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:18.895Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {5,10,102}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:12:12] [ds:12:12:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2023,5,24},{12,22,18}}},
               {memory,
                   [{total,40753224},
                    {processes,9694576},
                    {processes_used,9668736},
                    {system,31058648},
                    {atom,388625},
                    {atom_used,364429},
                    {binary,120392},
                    {code,8250921},
                    {ets,1504792}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,application,error_logger,heart,code,
                    application_master,file_server,kernel,gen_event,
                    file_io_server,code_server,gen,application_controller,
                    gen_server,erl_eval,ets,file,proc_lib,lists,filename,
                    erl_parse,supervisor,erl_lint,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ale_dynamic_sup,ssl_pem_cache_dist,rex,global_group,
                    net_sup,ssl_connection_sup,kernel_sup,ssl_admin_sup,
                    tftp_sup,global_name_server,ssl_sup,root_sup,os_mon_sup,
                    erts_code_purger,file_server_2,kernel_safe_sup,
                    error_logger,cpu_sup,memsup,erl_epmd,init,disksup,ale,
                    erl_signal_server,net_kernel,dist_manager,ssl_pem_cache,
                    ssl_manager,ssl_dist_admin_sup,ssl_dist_connection_sup,
                    ssl_dist_sup,ssl_tls_dist_proxy,ssl_manager_dist,user,
                    sasl_safe_sup,ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2023-05-24T12:22:18.898Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2023-05-24T12:22:18.901Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.902Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.902Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:18.903Z,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2023-05-24T12:22:18.903Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.903Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:18.925Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2023-05-24T12:22:18.928Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2023-05-24T12:22:18.945Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{uuid,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
    <<"b1cf035dd538cdacd1c975b6e16f1302">>]},
  {rest_creds,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
    {"<ud>admin</ud>",
     {auth,
      [{<<"plain">>,"*****"},
       {<<"sha512">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,
           <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
          {<<"i">>,4000}]}},
       {<<"sha256">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
          {<<"i">>,4000}]}},
       {<<"sha1">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
          {<<"i">>,4000}]}}]}}]},
  {rest,[{port,8091}]},
  {auto_failover_cfg,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]},
    {can_abort_rebalance,true}]},
  {retry_rebalance,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {enabled,false},
    {after_time_period,300},
    {max_attempts,1}]},
  {audit_decriptors,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {32768,
     [{name,<<"Create Function">>},
      {description,<<"Eventing function definition was created or updated">>},
      {enabled,true},
      {module,eventing}]},
    {32769,
     [{name,<<"Delete Function">>},
      {description,<<"Eventing function definition was deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32770,
     [{name,<<"Fetch Functions">>},
      {description,<<"Eventing function definition was read">>},
      {enabled,false},
      {module,eventing}]},
    {32771,
     [{name,<<"List Deployed">>},
      {description,<<"Eventing deployed functions list was read">>},
      {enabled,false},
      {module,eventing}]},
    {32772,
     [{name,<<"Fetch Drafts">>},
      {description,<<"Eventing function draft definitions were read">>},
      {enabled,false},
      {module,eventing}]},
    {32773,
     [{name,<<"Delete Drafts">>},
      {description,<<"Eventing function draft definitions were deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32774,
     [{name,<<"Save Draft">>},
      {description,<<"Save a draft definition to the store">>},
      {enabled,true},
      {module,eventing}]},
    {32775,
     [{name,<<"Start Debug">>},
      {description,<<"Start eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32776,
     [{name,<<"Stop Debug">>},
      {description,<<"Stop eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32777,
     [{name,<<"Start Tracing">>},
      {description,<<"Start tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32778,
     [{name,<<"Stop Tracing">>},
      {description,<<"Stop tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32779,
     [{name,<<"Set Settings">>},
      {description,<<"Save settings for a given app">>},
      {enabled,true},
      {module,eventing}]},
    {32780,
     [{name,<<"Fetch Config">>},
      {description,<<"Get config for eventing">>},
      {enabled,false},
      {module,eventing}]},
    {32781,
     [{name,<<"Save Config">>},
      {description,<<"Save config for eventing">>},
      {enabled,true},
      {module,eventing}]},
    {32782,
     [{name,<<"Cleanup Eventing">>},
      {description,<<"Clears up app definitions and settings from metakv">>},
      {enabled,true},
      {module,eventing}]},
    {32783,
     [{name,<<"Get Settings">>},
      {description,<<"Get settings for a given app">>},
      {enabled,false},
      {module,eventing}]},
    {32784,
     [{name,<<"Import Functions">>},
      {description,<<"Import a list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32785,
     [{name,<<"Export Functions">>},
      {description,<<"Export the list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32786,
     [{name,<<"List Running">>},
      {description,<<"Eventing running function list was read">>},
      {enabled,false},
      {module,eventing}]},
    {36865,
     [{name,<<"Service configuration change">>},
      {description,<<"A successful service configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36866,
     [{name,<<"Node configuration change">>},
      {description,<<"A successful node configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    <<211,88,119,65,90,45,162,36,255,132,241,44>>]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    <<"{\"ram_quota\":256}">>]},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
    <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
  {client_cert_auth,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},
    6,5]},
  {otp,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]},
  {{node,'ns_1@cb.local',eventing_dir},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@cb.local',cbas_dirs},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {cert_and_pkey,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U0J7myDdqIj25WTddDPRaYC2zr9QZtGfwU\nZ9Tff/yVZP6c8JlMVpgyPXMdkV26rINFZWXszttCravqmy8uM1wj2If4H0iA9HJB\n+lpIsnPL8tpv44Yh3Ao0SIh+RpDgrIHU7524ecHIRi6hXpCHCejmbmSzJ5lSnZ0y\nVAX8U2L7QKoy/i1rNri8EjTlSaxmAMUxd6yQiVGIECsH/0sDocK8pQ1cYkzThBrv\nZdtNAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQC9vJLL4XYgi575\nJ7Ko8jo++XojWVhz2pKlj8FCjtZmfHNRig8JGusbMfGZb2fMUkvN86jtWR8/tamu\nZLt7peK8D4pkpWHzubNE27dbKI8p0QGEpcYI31YSPA5OPSdzwy4GVRgs1ev8rPMA\nC0lAJvyF9MZK0f4b9+T4FoYJxuv7WnSf0tIrqx4BKI6B7P818DY3KZxgCmMQOcYL\n77XiMIKuiba5Km+VcQkv1lxgTmemHyXeFYaIjWDHYhHp23WahV1FoBt4bf+5GdZn\nnZAWAA03hELa32QzN58NB3Z5xdTguDafVsydS+ZhS38aiON0H2potwZivAlUE4yj\nI47GYXM0\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    false]},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    inet]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
    {configs,[]}]},
  {cbas_memory_quota,1129},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,256},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    30]},
  {memcached,[]},
  {memory_quota,1492},
  {nodes_wanted,['ns_1@cb.local']},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    'ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    <<"949a059dcc29e773ec37709a7973341b">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    false]},
  {{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>},
   [{'_vclock',
     [{<<"949a059dcc29e773ec37709a7973341b">>,{10,63851397749}}]}]}]]
[ns_server:info,2023-05-24T12:22:18.949Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{10,63851397749}}]}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   <<"949a059dcc29e773ec37709a7973341b">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9140]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   'ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,1492},
 {memcached,[]},
 {max_bucket_count,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|30]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,256},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cbas_memory_quota,1129},
 {buckets,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
   {configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   inet]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   false]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {inet,false},
   {inet6,false}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U0J7myDdqIj25WTddDPRaYC2zr9QZtGfwU\nZ9Tff/yVZP6c8JlMVpgyPXMdkV26rINFZWXszttCravqmy8uM1wj2If4H0iA9HJB\n+lpIsnPL8tpv44Yh3Ao0SIh+RpDgrIHU7524ecHIRi6hXpCHCejmbmSzJ5lSnZ0y\nVAX8U2L7QKoy/i1rNri8EjTlSaxmAMUxd6yQiVGIECsH/0sDocK8pQ1cYkzThBrv\nZdtNAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQC9vJLL4XYgi575\nJ7Ko8jo++XojWVhz2pKlj8FCjtZmfHNRig8JGusbMfGZb2fMUkvN86jtWR8/tamu\nZLt7peK8D4pkpWHzubNE27dbKI8p0QGEpcYI31YSPA5OPSdzwy4GVRgs1ev8rPMA\nC0lAJvyF9MZK0f4b9+T4FoYJxuv7WnSf0tIrqx4BKI6B7P818DY3KZxgCmMQOcYL\n77XiMIKuiba5Km+VcQkv1lxgTmemHyXeFYaIjWDHYhHp23WahV1FoBt4bf+5GdZn\nnZAWAA03hELa32QzN58NB3Z5xdTguDafVsydS+ZhS38aiON0H2potwZivAlUE4yj\nI47GYXM0\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {{node,'ns_1@cb.local',cbas_dirs},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@cb.local',eventing_dir},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {otp,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]},
 {cluster_compat_version,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},
   6,5]},
 {client_cert_auth,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
   <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
   <<"{\"ram_quota\":256}">>]},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
   <<211,88,119,65,90,45,162,36,255,132,241,44>>]},
 {audit_decriptors,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {32768,
    [{name,<<"Create Function">>},
     {description,<<"Eventing function definition was created or updated">>},
     {enabled,true},
     {module,eventing}]},
   {32769,
    [{name,<<"Delete Function">>},
     {description,<<"Eventing function definition was deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32770,
    [{name,<<"Fetch Functions">>},
     {description,<<"Eventing function definition was read">>},
     {enabled,false},
     {module,eventing}]},
   {32771,
    [{name,<<"List Deployed">>},
     {description,<<"Eventing deployed functions list was read">>},
     {enabled,false},
     {module,eventing}]},
   {32772,
    [{name,<<"Fetch Drafts">>},
     {description,<<"Eventing function draft definitions were read">>},
     {enabled,false},
     {module,eventing}]},
   {32773,
    [{name,<<"Delete Drafts">>},
     {description,<<"Eventing function draft definitions were deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32774,
    [{name,<<"Save Draft">>},
     {description,<<"Save a draft definition to the store">>},
     {enabled,true},
     {module,eventing}]},
   {32775,
    [{name,<<"Start Debug">>},
     {description,<<"Start eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32776,
    [{name,<<"Stop Debug">>},
     {description,<<"Stop eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32777,
    [{name,<<"Start Tracing">>},
     {description,<<"Start tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32778,
    [{name,<<"Stop Tracing">>},
     {description,<<"Stop tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32779,
    [{name,<<"Set Settings">>},
     {description,<<"Save settings for a given app">>},
     {enabled,true},
     {module,eventing}]},
   {32780,
    [{name,<<"Fetch Config">>},
     {description,<<"Get config for eventing">>},
     {enabled,false},
     {module,eventing}]},
   {32781,
    [{name,<<"Save Config">>},
     {description,<<"Save config for eventing">>},
     {enabled,true},
     {module,eventing}]},
   {32782,
    [{name,<<"Cleanup Eventing">>},
     {description,<<"Clears up app definitions and settings from metakv">>},
     {enabled,true},
     {module,eventing}]},
   {32783,
    [{name,<<"Get Settings">>},
     {description,<<"Get settings for a given app">>},
     {enabled,false},
     {module,eventing}]},
   {32784,
    [{name,<<"Import Functions">>},
     {description,<<"Import a list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32785,
    [{name,<<"Export Functions">>},
     {description,<<"Export the list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32786,
    [{name,<<"List Running">>},
     {description,<<"Eventing running function list was read">>},
     {enabled,false},
     {module,eventing}]},
   {36865,
    [{name,<<"Service configuration change">>},
     {description,<<"A successful service configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36866,
    [{name,<<"Node configuration change">>},
     {description,<<"A successful node configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]}]},
 {retry_rebalance,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {enabled,false},
   {after_time_period,300},
   {max_attempts,1}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {rest,[{port,8091}]},
 {rest_creds,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
   {"<ud>admin</ud>",
    {auth,
     [{<<"plain">>,"*****"},
      {<<"sha512">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,
          <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
         {<<"i">>,4000}]}},
      {<<"sha256">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
         {<<"i">>,4000}]}},
      {<<"sha1">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
         {<<"i">>,4000}]}}]}}]},
 {uuid,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
   <<"b1cf035dd538cdacd1c975b6e16f1302">>]}]
[error_logger:info,2023-05-24T12:22:18.953Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.954Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.954Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.955Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:18.956Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{11,63852150138}}]}]
[error_logger:info,2023-05-24T12:22:18.956Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.957Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:18.963Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:18.964Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2023-05-24T12:22:18.964Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.964Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.967Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:18.968Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:19.020Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2023-05-24T12:22:19.020Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2023-05-24T12:22:19.020Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2023-05-24T12:22:19.039Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,95,103,117,59,166,49,
             58,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,55,48,49,56,51,48,98,50,48,30,23,13,49,51,48,49,
             48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,57,
             53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,104,
             98,97,115,101,32,83,101,114,118,101,114,32,55,48,49,56,51,48,98,
             50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,
             15,0,48,130,1,10,2,130,1,1,0,192,97,0,160,196,124,86,76,76,241,
             92,35,220,170,212,78,154,52,104,79,25,232,189,131,139,135,15,
             155,214,199,43,184,2,114,204,250,241,110,76,96,156,151,94,187,0,
             216,138,82,17,125,188,1,163,168,204,18,49,110,109,211,194,87,65,
             18,22,218,61,235,124,91,95,73,186,220,237,129,145,9,14,31,207,
             206,211,101,52,39,185,178,13,218,136,143,110,86,77,215,67,61,22,
             152,11,108,235,245,6,109,25,252,20,103,212,223,127,252,149,100,
             254,156,240,153,76,86,152,50,61,115,29,145,93,186,172,131,69,
             101,101,236,206,219,66,173,171,234,155,47,46,51,92,35,216,135,
             248,31,72,128,244,114,65,250,90,72,178,115,203,242,218,111,227,
             134,33,220,10,52,72,136,126,70,144,224,172,129,212,239,157,184,
             121,193,200,70,46,161,94,144,135,9,232,230,110,100,179,39,153,
             82,157,157,50,84,5,252,83,98,251,64,170,50,254,45,107,54,184,
             188,18,52,229,73,172,102,0,197,49,119,172,144,137,81,136,16,43,
             7,255,75,3,161,194,188,165,13,92,98,76,211,132,26,239,101,219,
             77,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,189,188,146,203,225,118,32,139,158,
             249,39,178,168,242,58,62,249,122,35,89,88,115,218,146,165,143,
             193,66,142,214,102,124,115,81,138,15,9,26,235,27,49,241,153,111,
             103,204,82,75,205,243,168,237,89,31,63,181,169,174,100,187,123,
             165,226,188,15,138,100,165,97,243,185,179,68,219,183,91,40,143,
             41,209,1,132,165,198,8,223,86,18,60,14,78,61,39,115,195,46,6,85,
             24,44,213,235,252,172,243,0,11,73,64,38,252,133,244,198,74,209,
             254,27,247,228,248,22,134,9,198,235,251,90,116,159,210,210,43,
             171,30,1,40,142,129,236,255,53,240,54,55,41,156,96,10,99,16,57,
             198,11,239,181,226,48,130,174,137,182,185,42,111,149,113,9,47,
             214,92,96,78,103,166,31,37,222,21,134,136,141,96,199,98,17,233,
             219,117,154,133,93,69,160,27,120,109,255,185,25,214,103,157,144,
             22,0,13,55,132,66,218,223,100,51,55,159,13,7,118,121,197,212,
             224,184,54,159,86,204,157,75,230,97,75,127,26,136,227,116,31,
             106,104,183,6,98,188,9,84,19,140,163,35,142,198,97,115,52>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2023-05-24T12:22:19.041Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:19.051Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-24T12:22:19.051Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-24T12:22:19.052Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-24T12:22:19.052Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2023-05-24T12:22:19.067Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-24T12:22:19.067Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[error_logger:info,2023-05-24T12:22:19.067Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:19.068Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-24T12:22:19.068Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-24T12:22:19.068Z,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2023-05-24T12:22:19.068Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.069Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.069Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:19.074Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.074Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.077Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:19.078Z,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2023-05-24T12:22:19.079Z,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2023-05-24T12:22:19.080Z,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2023-05-24T12:22:19.081Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2023-05-24T12:22:19.081Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.081Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:19.081Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2023-05-24T12:22:19.081Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.085Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.085Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:19.088Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:19.088Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:19.093Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.268.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:19.093Z,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[ns_server:debug,2023-05-24T12:22:19.093Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2023-05-24T12:22:19.093Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-24T12:22:19.093Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2818819729.3791388680.8803>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-24T12:22:19.093Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2818819729.3791388680.8803>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.2818819729.3791388680.8805>}
[ns_server:debug,2023-05-24T12:22:19.094Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2818819729.3791388680.8803>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.2818819729.3791388680.8805>}
[error_logger:info,2023-05-24T12:22:19.094Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[ns_server:debug,2023-05-24T12:22:19.094Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-05-24T12:22:19.094Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2023-05-24T12:22:19.102Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2023-05-24T12:22:19.103Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 22ms
[ns_server:debug,2023-05-24T12:22:19.104Z,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[ns_server:debug,2023-05-24T12:22:19.295Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2023-05-24T12:22:19.294Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-24T12:22:19.295Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2818819729.3791388680.8826>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-24T12:22:19.295Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2818819729.3791388680.8826>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.2818819729.3791388679.8700>}
[ns_server:debug,2023-05-24T12:22:19.295Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.2818819729.3791388680.8826>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.2818819729.3791388679.8700>}
[ns_server:debug,2023-05-24T12:22:19.295Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-05-24T12:22:19.295Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,shutdown}}
[error_logger:info,2023-05-24T12:22:19.295Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2023-05-24T12:22:19.495Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-24T12:22:19.495Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-24T12:22:19.496Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.2818819729.3791388680.8840>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-24T12:22:19.496Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.2818819729.3791388680.8840>,
                                  inet_tcp_dist,<0.282.0>,
                                  #Ref<0.2818819729.3791388679.8701>}
[ns_server:debug,2023-05-24T12:22:19.534Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-24T12:22:19.736Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-24T12:22:19.937Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2023-05-24T12:22:20.265Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.287.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.465Z,ns_1@cb.local:ns_couchdb_port<0.268.0>:ns_port_server:log:224]ns_couchdb<0.268.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.268.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2023-05-24T12:22:20.476Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.273.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.480Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2023-05-24T12:22:20.483Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.484Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.485Z,ns_1@cb.local:ns_server_sup<0.289.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2023-05-24T12:22:20.485Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.487Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.494Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.294.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.494Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.295.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.497Z,ns_1@cb.local:memcached_passwords<0.296.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-24T12:22:20.497Z,ns_1@cb.local:memcached_passwords<0.296.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-24T12:22:20.519Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.296.0>
[ns_server:debug,2023-05-24T12:22:20.519Z,ns_1@cb.local:memcached_passwords<0.296.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-24T12:22:20.519Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.296.0>
[ns_server:debug,2023-05-24T12:22:20.521Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2023-05-24T12:22:20.521Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.296.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.524Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.300.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2023-05-24T12:22:20.524Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.299.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.525Z,ns_1@cb.local:memcached_permissions<0.301.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-24T12:22:20.528Z,ns_1@cb.local:memcached_permissions<0.301.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-24T12:22:20.529Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.301.0>
[ns_server:debug,2023-05-24T12:22:20.529Z,ns_1@cb.local:memcached_permissions<0.301.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-24T12:22:20.529Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.301.0>
[ns_server:warn,2023-05-24T12:22:20.531Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-24T12:22:20.531Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-05-24T12:22:20.532Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2023-05-24T12:22:20.532Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.301.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-24T12:22:20.532Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-24T12:22:20.532Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-24T12:22:20.533Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.534Z,ns_1@cb.local:ns_node_disco<0.307.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[ns_server:debug,2023-05-24T12:22:20.534Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[error_logger:info,2023-05-24T12:22:20.534Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.306.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2023-05-24T12:22:20.534Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:128]Node 'ns_1@cb.local' synchronized otp cookie {sanitized,
                                              <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>} from cluster
[ns_server:debug,2023-05-24T12:22:20.534Z,ns_1@cb.local:<0.308.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-24T12:22:20.535Z,ns_1@cb.local:<0.308.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[error_logger:info,2023-05-24T12:22:20.535Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.307.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.536Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.309.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.537Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.310.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.538Z,ns_1@cb.local:ns_config_rep<0.312.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2023-05-24T12:22:20.538Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.311.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.538Z,ns_1@cb.local:ns_config_rep<0.312.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_config_rep<0.312.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              5],
                                                                             {0,
                                                                              477034929},
                                                                             {0,
                                                                              477034929},
                                                                             true,
                                                                             []}
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:<0.318.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:<0.319.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:<0.318.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:<0.319.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:memcached_permissions<0.301.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2023-05-24T12:22:20.541Z,ns_1@cb.local:memcached_passwords<0.296.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-24T12:22:20.542Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.301.0>
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:memcached_permissions<0.301.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.301.0>
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2023-05-24T12:22:20.544Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}],{configs,[]}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1129
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U"...>>,
  <<"*****">>}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},6,5]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|30]
[ns_server:debug,2023-05-24T12:22:20.545Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[error_logger:info,2023-05-24T12:22:20.545Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.312.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.546Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.305.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
1492
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@cb.local']
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 'ns_1@cb.local']
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_rep<0.312.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               cert_and_pkey,client_cert_auth,
                               cluster_compat_version,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,retry_rebalance,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,uuid,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',audit},
                               {node,'ns_1@cb.local',capi_port},
                               {node,'ns_1@cb.local',cbas_admin_port},
                               {node,'ns_1@cb.local',cbas_cc_client_port},
                               {node,'ns_1@cb.local',cbas_cc_cluster_port},
                               {node,'ns_1@cb.local',cbas_cc_http_port},
                               {node,'ns_1@cb.local',cbas_cluster_port},
                               {node,'ns_1@cb.local',cbas_console_port},
                               {node,'ns_1@cb.local',cbas_data_port},
                               {node,'ns_1@cb.local',cbas_debug_port},
                               {node,'ns_1@cb.local',cbas_dirs},
                               {node,'ns_1@cb.local',cbas_http_port},
                               {node,'ns_1@cb.local',cbas_messaging_port},
                               {node,'ns_1@cb.local',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@cb.local',cbas_metadata_port},
                               {node,'ns_1@cb.local',cbas_parent_port},
                               {node,'ns_1@cb.local',cbas_replication_port},
                               {node,'ns_1@cb.local',cbas_result_port},
                               {node,'ns_1@cb.local',cbas_ssl_port},
                               {node,'ns_1@cb.local',compaction_daemon},
                               {node,'ns_1@cb.local',config_version},
                               {node,'ns_1@cb.local',erl_external_listeners}]..)
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<211,88,119,65,90,45,162,36,255,132,241,44>>]
[ns_server:debug,2023-05-24T12:22:20.546Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
uuid ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 <<"b1cf035dd538cdacd1c975b6e16f1302">>]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{11,63852150138}}]}]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|inet]
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',audit} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:warn,2023-05-24T12:22:20.547Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-05-24T12:22:20.547Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8092]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9110]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_client_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9113]
[error_logger:info,2023-05-24T12:22:20.548Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.324.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9112]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9111]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9115]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_console_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9114]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_data_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9116]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8095]
[ns_server:debug,2023-05-24T12:22:20.548Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_messaging_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9118]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9119]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9121]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_parent_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9122]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_replication_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9120]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_result_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9117]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18095]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',compaction_daemon} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',config_version} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|{6,5}]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9140]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8096]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18096]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9130]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19130]
[ns_server:debug,2023-05-24T12:22:20.549Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8094]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18094]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9100]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9102]
[error_logger:info,2023-05-24T12:22:20.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.326.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19102]
[error_logger:info,2023-05-24T12:22:20.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.329.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_scan_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9101]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stcatchup_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9104]
[error_logger:info,2023-05-24T12:22:20.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.330.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stinit_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9103]
[error_logger:info,2023-05-24T12:22:20.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.331.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stmaint_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9105]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',is_enterprise} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',isasl} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2023-05-24T12:22:20.550Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 active]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_config} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|11206]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_defaults} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',moxi} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,0}]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ns_log} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2023-05-24T12:22:20.551Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',port_servers} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8093]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',rest} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',saslauthd_enabled} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18092]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18093]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18091]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',uuid} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 <<"949a059dcc29e773ec37709a7973341b">>]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',xdcr_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9998]
[ns_server:debug,2023-05-24T12:22:20.552Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[error_logger:info,2023-05-24T12:22:20.553Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.332.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.553Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2023-05-24T12:22:20.553Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[error_logger:info,2023-05-24T12:22:20.557Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.338.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.557Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.341.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.557Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.337.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.559Z,ns_1@cb.local:ns_heart<0.338.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2023-05-24T12:22:20.559Z,ns_1@cb.local:ns_heart<0.338.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[ns_server:info,2023-05-24T12:22:20.559Z,ns_1@cb.local:<0.336.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2023-05-24T12:22:20.560Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2023-05-24T12:22:20.561Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.346.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.565Z,ns_1@cb.local:<0.343.0>:restartable:start_child:98]Started child process <0.345.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2023-05-24T12:22:20.565Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.347.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.343.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.350.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.566Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.296.0>
[ns_server:debug,2023-05-24T12:22:20.566Z,ns_1@cb.local:memcached_passwords<0.296.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-24T12:22:20.566Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.296.0>
[ns_server:debug,2023-05-24T12:22:20.569Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2023-05-24T12:22:20.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.351.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.352.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.353.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.569Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.354.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-24T12:22:20.570Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-24T12:22:20.570Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-24T12:22:20.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.356.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.574Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.359.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.574Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.361.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.578Z,ns_1@cb.local:ns_heart<0.338.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-24T12:22:20.579Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.363.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.579Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.365.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.580Z,ns_1@cb.local:ns_heart<0.338.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-24T12:22:20.581Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.366.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.583Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.367.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.585Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.368.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.585Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.369.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.586Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-24T12:22:20.587Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-24T12:22:20.588Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-24T12:22:20.588Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2023-05-24T12:22:20.588Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.371.0>,menelaus_web}
             started: [{pid,<0.374.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.589Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-24T12:22:20.590Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-24T12:22:20.590Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-24T12:22:20.590Z,ns_1@cb.local:<0.371.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-24T12:22:20.590Z,ns_1@cb.local:<0.370.0>:restartable:start_child:98]Started child process <0.371.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2023-05-24T12:22:20.590Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.371.0>,menelaus_web}
             started: [{pid,<0.394.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.591Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.370.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.593Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.413.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.595Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.415.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.595Z,ns_1@cb.local:ns_heart_slow_status_updater<0.341.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2023-05-24T12:22:20.596Z,ns_1@cb.local:ns_heart_slow_status_updater<0.341.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2023-05-24T12:22:20.597Z,ns_1@cb.local:ns_heart_slow_status_updater<0.341.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-05-24T12:22:20.597Z,ns_1@cb.local:ns_heart_slow_status_updater<0.341.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-24T12:22:20.597Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.425.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.599Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.426.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2023-05-24T12:22:20.599Z,ns_1@cb.local:ns_server_sup<0.289.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2023-05-24T12:22:20.599Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.362.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.600Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.432.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.601Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.436.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.601Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.437.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.601Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.435.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.605Z,ns_1@cb.local:ns_ports_setup<0.432.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,projector,goxdcr]
[error_logger:info,2023-05-24T12:22:20.607Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.448.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.616Z,ns_1@cb.local:memcached_auth_server<0.449.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2023-05-24T12:22:20.616Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.449.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.616Z,ns_1@cb.local:ns_audit_cfg<0.451.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2023-05-24T12:22:20.629Z,ns_1@cb.local:ns_audit_cfg<0.451.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2023-05-24T12:22:20.630Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.451.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-24T12:22:20.630Z,ns_1@cb.local:<0.455.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2023-05-24T12:22:20.632Z,ns_1@cb.local:memcached_config_mgr<0.457.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-05-24T12:22:20.632Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.456.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.632Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.457.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.633Z,ns_1@cb.local:<0.458.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2023-05-24T12:22:20.633Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.458.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.635Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.459.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.636Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.460.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.639Z,ns_1@cb.local:terse_cluster_info_uploader<0.460.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":11,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2023-05-24T12:22:20.641Z,ns_1@cb.local:<0.464.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2023-05-24T12:22:20.641Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.465.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.642Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.466.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.642Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.462.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.644Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.468.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.645Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.472.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.647Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.474.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.648Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.475.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.648Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.477.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.649Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.478.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.649Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.480.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.650Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.481.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.483.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.485.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.653Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.486.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.654Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.488.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.655Z,ns_1@cb.local:goxdcr_status_keeper<0.488.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-05-24T12:22:20.656Z,ns_1@cb.local:goxdcr_status_keeper<0.488.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-24T12:22:20.659Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.492.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.661Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.494.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.673Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.495.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.677Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.498.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.677Z,ns_1@cb.local:ns_couchdb_port<0.268.0>:ns_port_server:log:224]ns_couchdb<0.268.0>: 283: Booted. Waiting for shutdown request
ns_couchdb<0.268.0>: working as port

[error_logger:info,2023-05-24T12:22:20.681Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.502.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.681Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.493.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.681Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.505.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.681Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.491.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.697Z,ns_1@cb.local:<0.509.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.509.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2023-05-24T12:22:20.706Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.507.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:22:20.706Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2023-05-24T12:22:20.708Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.511.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.708Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.510.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.708Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.512.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.727Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.516.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.729Z,ns_1@cb.local:ns_ports_setup<0.432.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2023-05-24T12:22:20.729Z,ns_1@cb.local:memcached_config_mgr<0.457.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2023-05-24T12:22:20.735Z,ns_1@cb.local:memcached_config_mgr<0.457.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[ns_server:warn,2023-05-24T12:22:20.741Z,ns_1@cb.local:leader_lease_agent<0.517.0>:leader_lease_agent:maybe_recover_persisted_lease:399]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"69e29cc338e2047067b01d5fd46cb73a">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2023-05-24T12:22:20.741Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.517.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.741Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.515.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.745Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.523.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.747Z,ns_1@cb.local:leader_registry_sup<0.522.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2023-05-24T12:22:20.747Z,ns_1@cb.local:leader_registry_sup<0.522.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2023-05-24T12:22:20.748Z,ns_1@cb.local:leader_registry_sup<0.522.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2023-05-24T12:22:20.750Z,ns_1@cb.local:mb_master<0.526.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2023-05-24T12:22:20.751Z,ns_1@cb.local:leader_registry<0.523.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2023-05-24T12:22:20.756Z,ns_1@cb.local:memcached_config_mgr<0.457.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2023-05-24T12:22:20.757Z,ns_1@cb.local:memcached_config_mgr<0.457.0>:memcached_config_mgr:init:86]activated memcached port server
[ns_server:debug,2023-05-24T12:22:20.758Z,ns_1@cb.local:mb_master<0.526.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[error_logger:info,2023-05-24T12:22:20.760Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.529.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.761Z,ns_1@cb.local:leader_quorum_nodes_manager<0.531.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[ns_server:debug,2023-05-24T12:22:20.761Z,ns_1@cb.local:leader_quorum_nodes_manager<0.531.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[error_logger:info,2023-05-24T12:22:20.761Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.531.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-24T12:22:20.766Z,ns_1@cb.local:<0.535.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@cb.local' because its already taken by {'ns_1@cb.local',
                                                                           <<"69e29cc338e2047067b01d5fd46cb73a">>} (valid for 14975ms)
[ns_server:info,2023-05-24T12:22:20.770Z,ns_1@cb.local:mb_master_sup<0.528.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.538.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-24T12:22:20.771Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.538.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.774Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.540.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.778Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.541.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.793Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.543.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.797Z,ns_1@cb.local:ns_orchestrator_child_sup<0.542.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.544.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-24T12:22:20.797Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.544.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.800Z,ns_1@cb.local:ns_orchestrator_child_sup<0.542.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.545.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-24T12:22:20.800Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.545.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-24T12:22:20.800Z,ns_1@cb.local:ns_orchestrator_child_sup<0.542.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.546.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-24T12:22:20.801Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.546.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.801Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.542.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.806Z,ns_1@cb.local:<0.548.0>:auto_failover:init:185]init auto_failover.
[user:info,2023-05-24T12:22:20.806Z,ns_1@cb.local:<0.548.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2023-05-24T12:22:20.810Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{12,63852150140}}]}]
[ns_server:info,2023-05-24T12:22:20.810Z,ns_1@cb.local:ns_orchestrator_sup<0.539.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.548.0> on 'ns_1@cb.local'

[ns_server:debug,2023-05-24T12:22:20.810Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2023-05-24T12:22:20.810Z,ns_1@cb.local:ns_config_rep<0.312.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[ns_server:info,2023-05-24T12:22:20.810Z,ns_1@cb.local:mb_master_sup<0.528.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.553.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-24T12:22:20.810Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.548.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.811Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.539.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.811Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.553.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.813Z,ns_1@cb.local:<0.558.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2023-05-24T12:22:20.813Z,ns_1@cb.local:mb_master_sup<0.528.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.558.0> on 'ns_1@cb.local'

[ns_server:debug,2023-05-24T12:22:20.813Z,ns_1@cb.local:<0.513.0>:restartable:start_child:98]Started child process <0.514.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2023-05-24T12:22:20.813Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.558.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.814Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.526.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.814Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.522.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.814Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.513.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.816Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.560.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.816Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.562.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.816Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.563.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.817Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.564.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.818Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.565.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.823Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.568.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.823Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.570.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.825Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.577.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.830Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.579.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.831Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.581.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.832Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.571.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.833Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.583.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.834Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.589.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-24T12:22:20.834Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.567.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.836Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.591.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.843Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2023-05-24T12:22:20.843Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.592.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.844Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.207.0>
[ns_server:debug,2023-05-24T12:22:20.844Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[error_logger:info,2023-05-24T12:22:20.844Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.289.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-24T12:22:20.844Z,ns_1@cb.local:<0.206.0>:restartable:start_child:98]Started child process <0.207.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2023-05-24T12:22:20.844Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.206.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.846Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.594.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-24T12:22:20.846Z,ns_1@cb.local:<0.5.0>:child_erlang:child_loop:130]219: Entered child_loop
[error_logger:info,2023-05-24T12:22:20.846Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.185.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-24T12:22:20.846Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@cb.local'

[ns_server:debug,2023-05-24T12:22:20.848Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2023-05-24T12:22:20.856Z,ns_1@cb.local:json_rpc_connection-projector-cbauth<0.595.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.595.0>
[ns_server:debug,2023-05-24T12:22:20.856Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.597.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.597.0>
[ns_server:debug,2023-05-24T12:22:20.857Z,ns_1@cb.local:menelaus_cbauth<0.426.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"projector-cbauth",<0.595.0>} started
[ns_server:debug,2023-05-24T12:22:20.857Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.596.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.596.0>
[ns_server:debug,2023-05-24T12:22:20.866Z,ns_1@cb.local:menelaus_cbauth<0.426.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.596.0>} started
[ns_server:debug,2023-05-24T12:22:20.867Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2023-05-24T12:22:20.868Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2023-05-24T12:22:21.542Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2023-05-24T12:22:21.807Z,ns_1@cb.local:<0.548.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@cb.local',<<"949a059dcc29e773ec37709a7973341b">>} state new -> up
[ns_server:info,2023-05-24T12:22:25.800Z,ns_1@cb.local:<0.546.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:debug,2023-05-24T12:22:30.628Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],anonymous}
[menelaus:info,2023-05-24T12:22:30.671Z,ns_1@cb.local:<0.603.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6021>,'POST',"/settigs/stats",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"57"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[menelaus:info,2023-05-24T12:22:30.682Z,ns_1@cb.local:<0.604.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6023>,'POST',"/settings/compaction",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"54"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[ns_server:info,2023-05-24T12:22:30.800Z,ns_1@cb.local:<0.546.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:debug,2023-05-24T12:22:35.741Z,ns_1@cb.local:leader_lease_agent<0.517.0>:leader_lease_agent:handle_lease_expired:286]Lease held by {lease_holder,<<"69e29cc338e2047067b01d5fd46cb73a">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:warn,2023-05-24T12:22:35.742Z,ns_1@cb.local:<0.535.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@cb.local' because its already taken by {'ns_1@cb.local',
                                                                           <<"69e29cc338e2047067b01d5fd46cb73a">>} (valid for 0ms)
[ns_server:debug,2023-05-24T12:22:35.743Z,ns_1@cb.local:leader_lease_agent<0.517.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"02fa71c1eed17d8523c799a2efd87a61">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:info,2023-05-24T12:22:35.756Z,ns_1@cb.local:<0.535.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"02fa71c1eed17d8523c799a2efd87a61">>)
[ns_server:debug,2023-05-24T12:22:53.553Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:22:53.553Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:22:53.553Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:22:53.553Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:23:23.264Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:23:23.264Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:23:23.264Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:23:23.264Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:23:36.504Z,ns_1@cb.local:ldap_auth_cache<0.254.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-24T12:23:52.975Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:23:52.975Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:23:52.975Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:23:52.975Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:24:22.686Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:24:22.686Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:24:22.686Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:24:22.686Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:24:50.779Z,ns_1@cb.local:ldap_auth_cache<0.254.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-24T12:24:52.397Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:24:52.397Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:24:52.397Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:24:52.397Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:25:22.109Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:25:22.109Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:25:22.109Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:25:22.109Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:25:51.820Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:25:51.820Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:25:51.820Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:25:51.820Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:26:05.068Z,ns_1@cb.local:ldap_auth_cache<0.254.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-24T12:26:21.530Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:26:21.531Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:26:21.531Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:26:21.531Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:26:51.242Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:26:51.242Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-24T12:26:51.242Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-24T12:26:51.242Z,ns_1@cb.local:compaction_daemon<0.507.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:info,2023-05-27T08:32:47.108Z,nonode@nohost:<0.118.0>:ns_server:init_logging:150]Started & configured logging
[ns_server:info,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.122Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.123Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.123Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.123Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.123Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2023-05-27T08:32:47.123Z,nonode@nohost:<0.118.0>:ns_server:log_pending:32]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2023-05-27T08:32:47.137Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2023-05-27T08:32:47.138Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:read_address_config_from_path:99]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-27T08:32:47.147Z,nonode@nohost:dist_manager<0.166.0>:dist_manager:bringup:249]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2023-05-27T08:32:47.157Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.170.0>},
                       {id,ssl_pem_cache_dist},
                       {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.157Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_admin_sup}
             started: [{pid,<0.171.0>},
                       {id,ssl_dist_manager},
                       {mfargs,{ssl_manager,start_link_dist,[[]]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.157Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.169.0>},
                       {id,ssl_dist_admin_sup},
                       {mfargs,{ssl_dist_admin_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.160Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.172.0>},
                       {id,ssl_tls_dist_proxy},
                       {mfargs,{ssl_tls_dist_proxy,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.162Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.174.0>},
                       {id,dist_tls_connection},
                       {mfargs,{tls_connection_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.162Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_connection_sup}
             started: [{pid,<0.175.0>},
                       {id,dist_tls_socket},
                       {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.162Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ssl_dist_sup}
             started: [{pid,<0.173.0>},
                       {id,ssl_dist_connection_sup},
                       {mfargs,{ssl_dist_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,4000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.162Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.168.0>},
                       {id,ssl_dist_sup},
                       {mfargs,{ssl_dist_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:47.162Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting cb_dist with config []
[error_logger:info,2023-05-27T08:32:47.163Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.176.0>},
                       {id,cb_dist},
                       {mfargs,{cb_dist,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.164Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.177.0>},
                       {id,cb_epmd},
                       {mfargs,{cb_epmd,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.165Z,nonode@nohost:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.178.0>},
                       {id,auth},
                       {mfargs,{auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:47.166Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Initial protos: [inet_tcp_dist,inet6_tcp_dist], required protos: [inet_tcp_dist]
[ns_server:debug,2023-05-27T08:32:47.166Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet_tcp_dist listener on 21100...
[ns_server:debug,2023-05-27T08:32:47.166Z,nonode@nohost:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Starting inet6_tcp_dist listener on 21100...
[ns_server:debug,2023-05-27T08:32:47.168Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:configure_net_kernel:293]Set net_kernel vebosity to 10 -> 0
[error_logger:info,2023-05-27T08:32:47.168Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,net_sup}
             started: [{pid,<0.179.0>},
                       {id,net_kernel},
                       {mfargs,
                           {net_kernel,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.168Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_sup}
             started: [{pid,<0.167.0>},
                       {id,net_sup_dynamic},
                       {mfargs,
                           {erl_distribution,start_link,
                               [['ns_1@cb.local',longnames],false]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[ns_server:info,2023-05-27T08:32:47.169Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_node:175]saving node to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2023-05-27T08:32:47.178Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:bringup:263]Attempted to save node name to disk: ok
[ns_server:debug,2023-05-27T08:32:47.178Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:270]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2023-05-27T08:32:47.178Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2023-05-27T08:32:47.178Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-27T08:32:47.178Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.151144609.1471676417.258108>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-27T08:32:47.178Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.151144609.1471676417.258108>,
                                  inet_tcp_dist,<0.183.0>,
                                  #Ref<0.151144609.1471676425.256894>}
[ns_server:debug,2023-05-27T08:32:47.182Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:wait_for_node:282]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2023-05-27T08:32:47.183Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:162]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2023-05-27T08:32:47.183Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:163]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-05-27T08:32:47.188Z,ns_1@cb.local:dist_manager<0.166.0>:dist_manager:save_address_config:166]Persisted the address successfully
[error_logger:info,2023-05-27T08:32:47.189Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.166.0>},
                       {id,dist_manager},
                       {mfargs,{dist_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.193Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.186.0>},
                       {id,local_tasks},
                       {mfargs,{local_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:47.196Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:25]OS type: {unix,linux} Version: {5,10,102}
Runtime info: [{otp_release,"20"},
               {erl_version,"9.3.3.9"},
               {erl_version_long,
                   "Erlang/OTP 20 [erts-9.3.3.9] [source-d27a01ddb8] [64-bit] [smp:12:12] [ds:12:12:10] [async-threads:16] [kernel-poll:true]\n"},
               {system_arch_raw,"x86_64-unknown-linux-gnu"},
               {system_arch,"x86_64-unknown-linux-gnu"},
               {localtime,{{2023,5,27},{8,32,47}}},
               {memory,
                   [{total,40885872},
                    {processes,9819128},
                    {processes_used,9812424},
                    {system,31066744},
                    {atom,388625},
                    {atom_used,364429},
                    {binary,134608},
                    {code,8250921},
                    {ets,1504848}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    inet6_tcp,inet6_tcp_dist,re,auth,rand,
                    ssl_dist_connection_sup,ssl_tls_dist_proxy,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,cb_epmd,gen_udp,
                    inet_hosts,dist_manager,root_sup,path_config,cb_dist,
                    unicode_util,calendar,ale_default_formatter,
                    'ale_logger-metakv','ale_logger-rebalance',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,io_lib_fread,
                    filelib,cpu_sup,memsup,disksup,os_mon,string,io,
                    release_handler,alarm_handler,sasl,timer,tftp_sup,
                    httpd_sup,httpc_handler_sup,httpc_cookie,inets_trace,
                    httpc_manager,httpc,httpc_profile_sup,httpc_sup,ftp_sup,
                    inets_sup,inets_app,ssl,lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_udp_sup,dtls_connection_sup,ssl_listen_tracker_sup,
                    tls_connection_sup,ssl_connection_sup,ssl_session_cache,
                    ssl_manager,ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,
                    ssl_sup,ssl_app,ale_error_logger_handler,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,maps,beam_dict,beam_asm,beam_validator,
                    beam_z,beam_flatten,beam_trim,beam_record,beam_receive,
                    beam_bsm,beam_peep,beam_dead,beam_split,beam_type,
                    beam_clean,beam_bs,beam_except,beam_block,beam_utils,
                    beam_reorder,beam_jump,beam_a,v3_codegen,v3_life,
                    v3_kernel,sys_core_dsetel,sys_core_bsm,erl_bifs,
                    cerl_clauses,cerl_sets,sys_core_fold,cerl_trees,
                    sys_core_inline,core_lib,cerl,v3_core,erl_expand_records,
                    sofs,erl_internal,sets,ordsets,compile,dynamic_compile,
                    ale_utils,io_lib_pretty,io_lib_format,io_lib,ale_codegen,
                    dict,ale,ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,
                    child_erlang,orddict,c,erl_signal_handler,kernel_config,
                    user_io,user_sup,supervisor_bridge,standard_error,
                    net_kernel,global_group,erl_distribution,epp,
                    inet_gethost_native,inet_parse,inet,inet_udp,inet_config,
                    inet_db,global,rpc,unicode,os,hipe_unified_loader,
                    gb_trees,gb_sets,binary,erl_anno,proplists,erl_scan,
                    error_handler,file_server,file,ets,application,
                    application_master,error_logger,kernel,heart,code,
                    file_io_server,application_controller,gen,gen_server,
                    code_server,filename,erl_parse,supervisor,lists,gen_event,
                    proc_lib,erl_eval,erl_lint,
                    erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {applications,
                   [{sasl,"SASL  CXC 138 11","3.1.2"},
                    {os_mon,"CPO  CXC 138 46","2.4.4"},
                    {inets,"INETS  CXC 138 49","6.5.2.4"},
                    {crypto,"CRYPTO","4.2.2.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {stdlib,"ERTS  CXC 138 10","3.4.5.1"},
                    {ssl,"Erlang/OTP SSL application","8.2.6.4"},
                    {kernel,"ERTS  CXC 138 10","5.4.3.2"},
                    {public_key,"Public key infrastructure","1.5.2"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.5.2",
                        "5.0.5.2"},
                    {ns_server,"Couchbase server","6.5.0-4960-enterprise"}]},
               {pre_loaded,
                   [erts_dirty_process_code_checker,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,zlib,prim_file,prim_inet,
                    prim_eval,init,erts_code_purger,otp_ring0]},
               {process_count,129},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [application_controller,erl_prim_loader,httpd_sup,auth,
                    dtls_udp_sup,cb_dist,dtls_connection_sup,
                    ns_server_cluster_sup,tls_connection_sup,sasl_sup,
                    release_handler,lhttpc_sup,httpc_sup,lhttpc_manager,
                    alarm_handler,kernel_safe_sup,httpc_profile_sup,
                    ssl_listen_tracker_supdist,httpc_manager,
                    httpc_handler_sup,ssl_connection_sup_dist,'sink-ns_log',
                    local_tasks,standard_error_sup,ftp_sup,
                    'sink-disk_json_rpc','sink-disk_metakv',inets_sup,
                    'sink-disk_access_int','sink-disk_access',standard_error,
                    'sink-disk_reports',ale_stats_events,'sink-disk_stats',
                    'sink-disk_xdcr',timer_server,'sink-disk_debug',ale_sup,
                    'sink-disk_error',inet_db,'sink-disk_default',
                    ale_dynamic_sup,ssl_pem_cache_dist,rex,global_group,
                    net_sup,ssl_connection_sup,kernel_sup,ssl_admin_sup,
                    tftp_sup,global_name_server,ssl_sup,root_sup,os_mon_sup,
                    erts_code_purger,file_server_2,error_logger,cpu_sup,
                    memsup,erl_epmd,init,disksup,ale,erl_signal_server,
                    net_kernel,dist_manager,ssl_pem_cache,ssl_manager,
                    ssl_dist_admin_sup,ssl_dist_connection_sup,ssl_dist_sup,
                    ssl_tls_dist_proxy,ssl_manager_dist,user,sasl_safe_sup,
                    ssl_listen_tracker_sup,code_server]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,1}]
[ns_server:info,2023-05-27T08:32:47.199Z,ns_1@cb.local:ns_server_cluster_sup<0.185.0>:log_os_info:start_link:27]Manifest:
["<manifest>",
 "  <remote fetch=\"git://github.com/blevesearch/\" name=\"blevesearch\" />",
 "  <remote fetch=\"git://github.com/couchbase/\" name=\"couchbase\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"ssh://git@github.com/couchbase/\" name=\"couchbase-priv\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbasedeps/\" name=\"couchbasedeps\" review=\"review.couchbase.org\" />",
 "  <remote fetch=\"git://github.com/couchbaselabs/\" name=\"couchbaselabs\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project groups=\"kv\" name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"bc8aef24ea57884464027f841c1ad7436a42c615\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"691cec38f47eaab04ad81556cc065d22f1eb8749\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"672a36b64a0632b72aa4b4df59635ceaa0e340de\" />",
 "  <project groups=\"backup,notdefault,enterprise\" name=\"backup\" path=\"goproj/src/github.com/couchbase/backup\" remote=\"couchbase-priv\" revision=\"cfa0f75f28402d2e1aa254b2a374bead19433526\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"benchmark\" remote=\"couchbasedeps\" revision=\"74b24058ad4914b837200d0341050657ba154e4a\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"5cd1345cca3ed72f1e63d41d622fcda73e63fea8\" upstream=\"master\" />",
 "  <project name=\"bleve\" path=\"godeps/src/github.com/blevesearch/bleve\" remote=\"blevesearch\" revision=\"b7a0cb6a1d4fdbaeb7ab5bdec6a9732b995e39a0\" />",
 "  <project name=\"bleve-mapping-ui\" path=\"godeps/src/github.com/blevesearch/bleve-mapping-ui\" remote=\"blevesearch\" revision=\"7987f3c80047347b1e2c3a5fafae8da56daf97d7\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"buffer\" path=\"godeps/src/github.com/tdewolff/buffer\" remote=\"couchbasedeps\" revision=\"43cef5ba7b6ce99cc410632dad46cf1c6c97026e\" />",
 "  <project groups=\"notdefault,build\" name=\"build\" path=\"cbbuild\" revision=\"f2a16b53bb74146f20d18ba2c0443d5f10a9a550\" upstream=\"master\">",
 "    <annotation name=\"RELEASE\" value=\"mad-hatter\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"4960\" />",
 "    <annotation name=\"VERSION\" value=\"6.5.0\" />","  </project>",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"e3ec01671ca2f253a5f32cf9e258d3be7fdbfe9a\" />",
 "  <project groups=\"notdefault,enterprise,analytics\" name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"c86a9fc60d074711470b112753c5695dee79dcf7\" />",
 "  <project groups=\"analytics\" name=\"cbas-ui\" revision=\"8744108f25c4520b09009ff277d35223e208fe30\" />",
 "  <project name=\"cbauth\" path=\"godeps/src/github.com/couchbase/cbauth\" revision=\"82614adbe4d480de5675d8eee9b21a180a779222\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"cbflag\" path=\"godeps/src/github.com/couchbase/cbflag\" revision=\"9892b6db3537c54be7719f47ad25e0d513333b3e\" upstream=\"master\" />",
 "  <project name=\"cbft\" path=\"goproj/src/github.com/couchbase/cbft\" revision=\"ef487dda0baef8a258bac4f7482af3b761e4a8e0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"cbftx\" path=\"goproj/src/github.com/couchbase/cbftx\" remote=\"couchbase-priv\" revision=\"46dbb7c6edac7dfef017ae889d7a5b7536ce904d\" upstream=\"master\" />",
 "  <project name=\"cbgt\" path=\"goproj/src/github.com/couchbase/cbgt\" revision=\"c78e34377d7a8f017328f57a3376642f37458464\" upstream=\"mad-hatter\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"31ba0584a81d5b293cedfb236109ab95036aa395\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"b8e6d5d421bcc34f522e3a9a12fd6e09980995b1\" upstream=\"master\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project groups=\"notdefault,kv_ee,enterprise\" name=\"couch_rocks\" remote=\"couchbase-priv\" revision=\"75f37fa46bfe5e445dee077157303968a3e09126\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchbase-cli\" revision=\"abb0c1036566f4bd579aaadbaaa4e13466a23ef7\" upstream=\"master\" />",
 "  <project name=\"couchdb\" revision=\"fa3c64b1b85ad3145bb7910d3fe7ee90c060247e\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,packaging\" name=\"couchdbx-app\" revision=\"b2a111967ba02772dc600d5c15a6514e2dea7d68\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"couchstore\" revision=\"fff3e20090414206853b2293f17667279dda0337\" />",
 "  <project groups=\"backup\" name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"bd6f299fb381e4c3393d1c4b1f0b94f5e77650c8\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"cznic-b\" path=\"godeps/src/github.com/cznic/b\" remote=\"couchbasedeps\" revision=\"b96e30f1b7bd34b0b9d8760798d67eca83d7f09e\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"13cf07af78594aff20d00db4633af27d81fc921d\" upstream=\"master\" />",
 "  <project name=\"dparval\" path=\"godeps/src/github.com/couchbase/dparval\" revision=\"9def03782da875a2477c05bf64985db3f19f59ae\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"etcd-bbolt\" path=\"godeps/src/github.com/etcd-io/bbolt\" remote=\"couchbasedeps\" revision=\"7ee3ded59d4835e10f3e7d0f7603c42aa5e83820\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"dec7a7d51b71309d43d7aea4803cd45f6ad001da\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"398acea25e003c1739d3f45f53121bdec857e485\" upstream=\"mad-hatter\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project groups=\"backup,kv\" name=\"forestdb\" revision=\"4c3b2f9b1d869b6b71556e461d6ee68f941c1ba5\" upstream=\"cb-master\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"92def13f6b049553da1aa1488ce0bde6b7d0f459\" upstream=\"master\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"d910dd063dd68fb4d2a1ba344440f834ebb4ef62\" upstream=\"master\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"godeps/src/github.com/couchbase/go-couchbase\" revision=\"12d479a70a3ef189d8fb2424f5e2eea3632c0c9a\" upstream=\"mad-hatter\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" upstream=\"20161221-couchbase\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-porterstemmer\" path=\"godeps/src/github.com/blevesearch/go-porterstemmer\" remote=\"blevesearch\" revision=\"23a2c8e5cf1f380f27722c6d2ae8896431dc7d0e\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"1f5f7f282713ccfab3f46b1610cb8da34bcf676f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"go-sqlite3\" path=\"godeps/src/github.com/mattn/go-sqlite3\" remote=\"couchbasedeps\" revision=\"ad30583d8387ce8118f8605eaeb3b4f7b4ae0ee1\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"godeps/src/github.com/couchbase/go_json\" revision=\"d47ffbbc4863b0020bb85c4e181d4044ea184d40\" upstream=\"mad-hatter\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"6cf4e348b127e21f56e53eb8c3faaea56afdc588\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" />",
 "  <project groups=\"backup\" name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"083dcfef49cfdcb42a0f5ecf8c0c29b0cbaa640f\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" />",
 "  <project name=\"godbc\" path=\"godeps/src/github.com/couchbase/godbc\" revision=\"b2aaaa21900ab3e95d37d38fb5a0f320426cbe56\" upstream=\"mad-hatter\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project groups=\"backup\" name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" upstream=\"master\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbase/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" upstream=\"master\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project groups=\"backup\" name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"godeps/src/github.com/couchbase/gomemcached\" revision=\"2b4197fedf38f694a33465050d1396e03e97db19\" upstream=\"mad-hatter\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"563cdf343321e2025b73852bcf454860a4880300\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"googletest\" remote=\"couchbasedeps\" revision=\"f397fa5ec6365329b2e82eb2d8c03a7897bbefb5\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project groups=\"backup\" name=\"goutils\" path=\"godeps/src/github.com/couchbase/goutils\" revision=\"b49639060d85b267c5bdb7d4e3246d4ccca94e79\" upstream=\"mad-hatter\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"03e000156faeecd5e77eb79fc45d7c73f26b2899\" upstream=\"mad-hatter\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" />",
 "  <project groups=\"kv\" name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"57542c7e7ced375346e9ac55dad85b942cfad556\" upstream=\"refs/tags/v0.25.0\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"fc2e1b715bf9c098bf0991af666388dd446edf9b\" upstream=\"mad-hatter\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"bf1c66bbce23153d89b23f8960071a680dbef54b\" />",
 "  <project groups=\"backup\" name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"5b7baa20429a46a5543ee259664cc86502738cad\" upstream=\"master\" />",
 "  <project groups=\"kv\" name=\"kv_engine\" revision=\"2a368c39481ff4d42c6f755bd7d185b9a57554ca\" upstream=\"6.5.0\" />",
 "  <project name=\"levigo\" path=\"godeps/src/github.com/jmhodges/levigo\" remote=\"couchbasedeps\" revision=\"1ddad808d437abb2b8a55a950ec2616caa88969b\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"libcouchbase\" revision=\"152e1a18bbcfd75bbb5a1388ed5ee050cde8a56d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project groups=\"notdefault,enterprise,kv_ee\" name=\"magma\" remote=\"couchbase-priv\" revision=\"c8e91e0af8b46d0a0e026d23ebbfab4048f670b6\" />",
 "  <project name=\"minify\" path=\"godeps/src/github.com/tdewolff/minify\" remote=\"couchbasedeps\" revision=\"ede45cc53f43891267b1fe7c689db9c76d4ce0fb\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"mobile-service\" path=\"goproj/src/github.com/couchbase/mobile-service\" revision=\"4672fde0390f115a25f4f4bfe9d1511836de47a7\" upstream=\"master\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"a0cae174c4987cb28c071e0796e25b58834108d8\" upstream=\"master\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"aa48ddbc0e832bc68dde56c4b69e30c5cb3983eb\" upstream=\"master\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"godeps/src/github.com/couchbase/n1fty\" revision=\"f28de9b4e73d7acdf3b07b7f7318bb23973f7dc6\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"4fc6475fb3352618cdf93fead56271bb29d15571\" upstream=\"mad-hatter\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"3fe2759eb53c12478f75bd1613f8998401b0635c\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"parse\" path=\"godeps/src/github.com/tdewolff/parse\" remote=\"couchbasedeps\" revision=\"0334a869253aca4b3a10c56c3f3139b394aec3a9\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"bf8340a459bd383e5eb7d44a9a1b3af23b6cf8cd\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project groups=\"kv\" name=\"phosphor\" revision=\"53ca1eeae7bd3deea5b7bf48b3d4188b47e530d1\" upstream=\"master\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"4aa86645ce4b4673de08f6829b446b9c00cd3f3d\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"platform\" revision=\"bec44f963f3c4d73d3735380a8107b7292558749\" upstream=\"mad-hatter\" />",
 "  <project groups=\"kv\" name=\"product-texts\" revision=\"7a3aa547b3f5eb3ea28d279a08384609cd2cea7c\" upstream=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"a1708edce7216cdc4f21b4d4dd0eb4001d38e3c0\" upstream=\"mad-hatter\" />",
 "  <project groups=\"notdefault,enterprise\" name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"3ef4ab89910a53b6acfaba4cc7d96091ab33a346\" upstream=\"mad-hatter\" />",
 "  <project name=\"query-ui\" revision=\"d736c5b2b97eeea0bf8170a40cfa7533e168388e\" upstream=\"master\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"e3419088e4d3b4fe3aad3b364fdbe9a154f85f17\" upstream=\"master\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"d0ce1763c3526f65703c395da50da7a7fb2138d5\" />",
 "  <project name=\"segment\" path=\"godeps/src/github.com/blevesearch/segment\" remote=\"blevesearch\" revision=\"762005e7a34fd909a84586299f1dd457371d36ee\" />",
 "  <project groups=\"kv\" name=\"sigar\" revision=\"c33791d6d5de19d6c5575aa33f8e5dba848414d8\" upstream=\"master\" />",
 "  <project name=\"snowballstem\" path=\"godeps/src/github.com/blevesearch/snowballstem\" remote=\"blevesearch\" revision=\"26b06a2c243d4f8ca5db3486f94409dd5b2a7467\" />",
 "  <project groups=\"kv\" name=\"spdlog\" path=\"third_party/spdlog\" remote=\"couchbasedeps\" revision=\"20967a170429d0d37e09a485bc3cf5b153554924\" upstream=\"v1.1.0-couchbase\" />",
 "  <project name=\"strconv\" path=\"godeps/src/github.com/tdewolff/strconv\" remote=\"couchbasedeps\" revision=\"9b189f5be77f33c46776f24dbddb2a7ab32af214\" />",
 "  <project groups=\"kv\" name=\"subjson\" revision=\"ae63ab4b653870e400855f8563da40dda49f0eb3\" upstream=\"master\" />",
 "  <project groups=\"backup\" name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"7fbe1cd0fcc20051e1fcb87fbabec4a1bacaaeba\" />",
 "  <project name=\"testrunner\" revision=\"ee64d41320d14fabe814a241a5cf4f6a6f6e827a\" upstream=\"mad-hatter\" />",
 "  <project groups=\"backup\" name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"88f656faf3f37f690df1a32515b479415e1a6769\" />",
 "  <project groups=\"kv\" name=\"tlm\" revision=\"7279de40e2a171aeed67b2566bd499d7157df965\">",
 "    <copyfile dest=\"GNUmakefile\" src=\"GNUmakefile\" />",
 "    <copyfile dest=\"Makefile\" src=\"Makefile\" />",
 "    <copyfile dest=\"CMakeLists.txt\" src=\"CMakeLists.txt\" />",
 "    <copyfile dest=\".clang-format\" src=\"dot-clang-format\" />",
 "    <copyfile dest=\"third_party/CMakeLists.txt\" src=\"third-party-CMakeLists.txt\" />",
 "  </project>",
 "  <project groups=\"backup\" name=\"ts\" path=\"godeps/src/github.com/olekukonko/ts\" remote=\"couchbasedeps\" revision=\"ecf753e7c962639ab5a1fb46f7da627d4c0a04b8\" />",
 "  <project groups=\"backup\" name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vellum\" path=\"godeps/src/github.com/couchbase/vellum\" revision=\"ef2e028c01fdb60c46da4067d2e83745b8d54120\" upstream=\"master\" />",
 "  <project groups=\"notdefault,packaging\" name=\"voltron\" remote=\"couchbase-priv\" revision=\"45188488712448a326c8efad0d8c7b00e8afbefe\" upstream=\"master\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2023-05-27T08:32:47.201Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.187.0>},
                       {id,timeout_diag_logger},
                       {mfargs,{timeout_diag_logger,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.202Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.188.0>},
                       {id,ns_cookie_manager},
                       {mfargs,{ns_cookie_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.203Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.189.0>},
                       {id,ns_cluster},
                       {mfargs,{ns_cluster,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:47.203Z,ns_1@cb.local:ns_config_sup<0.190.0>:ns_config_sup:init:32]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2023-05-27T08:32:47.204Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.191.0>},
                       {id,ns_config_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_config_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.204Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.192.0>},
                       {id,ns_config_events_local},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_config_events_local}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:47.225Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1106]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2023-05-27T08:32:47.228Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1120]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:debug,2023-05-27T08:32:47.248Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1128]Here's full dynamic config we loaded:
[[{auto_failover_cfg,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
    {enabled,true},
    {timeout,120},
    {count,0},
    {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
    {failover_server_group,false},
    {max_count,1},
    {failed_over_server_groups,[]},
    {can_abort_rebalance,true}]},
  {alert_limits,
   [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
  {audit,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {enabled,[]},
    {disabled_users,[]},
    {auditd_enabled,false},
    {rotate_interval,86400},
    {rotate_size,20971520},
    {disabled,[]},
    {sync,[]},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
  {audit_decriptors,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {8243,
     [{name,<<"mutate document">>},
      {description,<<"Document was mutated via the REST API">>},
      {enabled,true},
      {module,ns_server}]},
    {8255,
     [{name,<<"read document">>},
      {description,<<"Document was read via the REST API">>},
      {enabled,false},
      {module,ns_server}]},
    {8257,
     [{name,<<"alert email sent">>},
      {description,<<"An alert email was successfully sent">>},
      {enabled,true},
      {module,ns_server}]},
    {20480,
     [{name,<<"opened DCP connection">>},
      {description,<<"opened DCP connection">>},
      {enabled,true},
      {module,memcached}]},
    {20482,
     [{name,<<"external memcached bucket flush">>},
      {description,
       <<"External user flushed the content of a memcached bucket">>},
      {enabled,true},
      {module,memcached}]},
    {20483,
     [{name,<<"invalid packet">>},
      {description,<<"Rejected an invalid packet">>},
      {enabled,true},
      {module,memcached}]},
    {20485,
     [{name,<<"authentication succeeded">>},
      {description,<<"Authentication to the cluster succeeded">>},
      {enabled,false},
      {module,memcached}]},
    {20488,
     [{name,<<"document read">>},
      {description,<<"Document was read">>},
      {enabled,false},
      {module,memcached}]},
    {20489,
     [{name,<<"document locked">>},
      {description,<<"Document was locked">>},
      {enabled,false},
      {module,memcached}]},
    {20490,
     [{name,<<"document modify">>},
      {description,<<"Document was modified">>},
      {enabled,false},
      {module,memcached}]},
    {20491,
     [{name,<<"document delete">>},
      {description,<<"Document was deleted">>},
      {enabled,false},
      {module,memcached}]},
    {20492,
     [{name,<<"select bucket">>},
      {description,<<"The specified bucket was selected">>},
      {enabled,true},
      {module,memcached}]},
    {28672,
     [{name,<<"SELECT statement">>},
      {description,<<"A N1QL SELECT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28673,
     [{name,<<"EXPLAIN statement">>},
      {description,<<"A N1QL EXPLAIN statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28674,
     [{name,<<"PREPARE statement">>},
      {description,<<"A N1QL PREPARE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28675,
     [{name,<<"INFER statement">>},
      {description,<<"A N1QL INFER statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28676,
     [{name,<<"INSERT statement">>},
      {description,<<"A N1QL INSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28677,
     [{name,<<"UPSERT statement">>},
      {description,<<"A N1QL UPSERT statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28678,
     [{name,<<"DELETE statement">>},
      {description,<<"A N1QL DELETE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28679,
     [{name,<<"UPDATE statement">>},
      {description,<<"A N1QL UPDATE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28680,
     [{name,<<"MERGE statement">>},
      {description,<<"A N1QL MERGE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28681,
     [{name,<<"CREATE INDEX statement">>},
      {description,<<"A N1QL CREATE INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28682,
     [{name,<<"DROP INDEX statement">>},
      {description,<<"A N1QL DROP INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28683,
     [{name,<<"ALTER INDEX statement">>},
      {description,<<"A N1QL ALTER INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28684,
     [{name,<<"BUILD INDEX statement">>},
      {description,<<"A N1QL BUILD INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28685,
     [{name,<<"GRANT ROLE statement">>},
      {description,<<"A N1QL GRANT ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28686,
     [{name,<<"REVOKE ROLE statement">>},
      {description,<<"A N1QL REVOKE ROLE statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28687,
     [{name,<<"UNRECOGNIZED statement">>},
      {description,
       <<"An unrecognized statement was received by the N1QL query engine">>},
      {enabled,false},
      {module,n1ql}]},
    {28688,
     [{name,<<"CREATE PRIMARY INDEX statement">>},
      {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
      {enabled,false},
      {module,n1ql}]},
    {28689,
     [{name,<<"/admin/stats API request">>},
      {description,<<"An HTTP request was made to the API at /admin/stats.">>},
      {enabled,false},
      {module,n1ql}]},
    {28690,
     [{name,<<"/admin/vitals API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/vitals.">>},
      {enabled,false},
      {module,n1ql}]},
    {28691,
     [{name,<<"/admin/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28692,
     [{name,<<"/admin/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28693,
     [{name,<<"/admin/indexes/prepareds API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
      {enabled,false},
      {module,n1ql}]},
    {28694,
     [{name,<<"/admin/indexes/active_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28695,
     [{name,<<"/admin/indexes/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28697,
     [{name,<<"/admin/ping API request">>},
      {description,<<"An HTTP request was made to the API at /admin/ping.">>},
      {enabled,false},
      {module,n1ql}]},
    {28698,
     [{name,<<"/admin/config API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/config.">>},
      {enabled,false},
      {module,n1ql}]},
    {28699,
     [{name,<<"/admin/ssl_cert API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
      {enabled,false},
      {module,n1ql}]},
    {28700,
     [{name,<<"/admin/settings API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/settings.">>},
      {enabled,false},
      {module,n1ql}]},
    {28701,
     [{name,<<"/admin/clusters API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/clusters.">>},
      {enabled,false},
      {module,n1ql}]},
    {28702,
     [{name,<<"/admin/completed_requests API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/completed_requests.">>},
      {enabled,false},
      {module,n1ql}]},
    {28704,
     [{name,<<"/admin/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {28705,
     [{name,<<"/admin/indexes/functions API request">>},
      {description,
       <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
      {enabled,false},
      {module,n1ql}]},
    {32768,
     [{name,<<"Create Function">>},
      {description,<<"Eventing function definition was created or updated">>},
      {enabled,true},
      {module,eventing}]},
    {32769,
     [{name,<<"Delete Function">>},
      {description,<<"Eventing function definition was deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32770,
     [{name,<<"Fetch Functions">>},
      {description,<<"Eventing function definition was read">>},
      {enabled,false},
      {module,eventing}]},
    {32771,
     [{name,<<"List Deployed">>},
      {description,<<"Eventing deployed functions list was read">>},
      {enabled,false},
      {module,eventing}]},
    {32772,
     [{name,<<"Fetch Drafts">>},
      {description,<<"Eventing function draft definitions were read">>},
      {enabled,false},
      {module,eventing}]},
    {32773,
     [{name,<<"Delete Drafts">>},
      {description,<<"Eventing function draft definitions were deleted">>},
      {enabled,true},
      {module,eventing}]},
    {32774,
     [{name,<<"Save Draft">>},
      {description,<<"Save a draft definition to the store">>},
      {enabled,true},
      {module,eventing}]},
    {32775,
     [{name,<<"Start Debug">>},
      {description,<<"Start eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32776,
     [{name,<<"Stop Debug">>},
      {description,<<"Stop eventing function debugger">>},
      {enabled,true},
      {module,eventing}]},
    {32777,
     [{name,<<"Start Tracing">>},
      {description,<<"Start tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32778,
     [{name,<<"Stop Tracing">>},
      {description,<<"Stop tracing eventing function execution">>},
      {enabled,true},
      {module,eventing}]},
    {32779,
     [{name,<<"Set Settings">>},
      {description,<<"Save settings for a given app">>},
      {enabled,true},
      {module,eventing}]},
    {32780,
     [{name,<<"Fetch Config">>},
      {description,<<"Get config for eventing">>},
      {enabled,false},
      {module,eventing}]},
    {32781,
     [{name,<<"Save Config">>},
      {description,<<"Save config for eventing">>},
      {enabled,true},
      {module,eventing}]},
    {32782,
     [{name,<<"Cleanup Eventing">>},
      {description,<<"Clears up app definitions and settings from metakv">>},
      {enabled,true},
      {module,eventing}]},
    {32783,
     [{name,<<"Get Settings">>},
      {description,<<"Get settings for a given app">>},
      {enabled,false},
      {module,eventing}]},
    {32784,
     [{name,<<"Import Functions">>},
      {description,<<"Import a list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32785,
     [{name,<<"Export Functions">>},
      {description,<<"Export the list of functions">>},
      {enabled,false},
      {module,eventing}]},
    {32786,
     [{name,<<"List Running">>},
      {description,<<"Eventing running function list was read">>},
      {enabled,false},
      {module,eventing}]},
    {36865,
     [{name,<<"Service configuration change">>},
      {description,<<"A successful service configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {36866,
     [{name,<<"Node configuration change">>},
      {description,<<"A successful node configuration change was made.">>},
      {enabled,true},
      {module,analytics}]},
    {40960,
     [{name,<<"Create Design Doc">>},
      {description,<<"Design Doc is Created">>},
      {enabled,true},
      {module,view_engine}]},
    {40961,
     [{name,<<"Delete Design Doc">>},
      {description,<<"Design Doc is Deleted">>},
      {enabled,true},
      {module,view_engine}]},
    {40962,
     [{name,<<"Query DDoc Meta Data">>},
      {description,<<"Design Doc Meta Data Query Request">>},
      {enabled,true},
      {module,view_engine}]},
    {40963,
     [{name,<<"View Query">>},
      {description,<<"View Query Request">>},
      {enabled,false},
      {module,view_engine}]},
    {40964,
     [{name,<<"Update Design Doc">>},
      {description,<<"Design Doc is Updated">>},
      {enabled,true},
      {module,view_engine}]}]},
  {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
  {autocompaction,
   [{database_fragmentation_threshold,{30,undefined}},
    {view_fragmentation_threshold,{30,undefined}}]},
  {buckets,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
    {configs,[]}]},
  {cbas_memory_quota,1129},
  {cert_and_pkey,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U0J7myDdqIj25WTddDPRaYC2zr9QZtGfwU\nZ9Tff/yVZP6c8JlMVpgyPXMdkV26rINFZWXszttCravqmy8uM1wj2If4H0iA9HJB\n+lpIsnPL8tpv44Yh3Ao0SIh+RpDgrIHU7524ecHIRi6hXpCHCejmbmSzJ5lSnZ0y\nVAX8U2L7QKoy/i1rNri8EjTlSaxmAMUxd6yQiVGIECsH/0sDocK8pQ1cYkzThBrv\nZdtNAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQC9vJLL4XYgi575\nJ7Ko8jo++XojWVhz2pKlj8FCjtZmfHNRig8JGusbMfGZb2fMUkvN86jtWR8/tamu\nZLt7peK8D4pkpWHzubNE27dbKI8p0QGEpcYI31YSPA5OPSdzwy4GVRgs1ev8rPMA\nC0lAJvyF9MZK0f4b9+T4FoYJxuv7WnSf0tIrqx4BKI6B7P818DY3KZxgCmMQOcYL\n77XiMIKuiba5Km+VcQkv1lxgTmemHyXeFYaIjWDHYhHp23WahV1FoBt4bf+5GdZn\nnZAWAA03hELa32QzN58NB3Z5xdTguDafVsydS+ZhS38aiON0H2potwZivAlUE4yj\nI47GYXM0\n-----END CERTIFICATE-----\n">>,
     <<"*****">>}]},
  {client_cert_auth,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {state,"disable"},
    {prefixes,[]}]},
  {cluster_compat_version,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},
    6,5]},
  {drop_request_memory_threshold_mib,undefined},
  {email_alerts,
   [{recipients,["root@localhost"]},
    {sender,"couchbase@localhost"},
    {enabled,false},
    {email_server,
     [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
    {alerts,
     [auto_failover_node,auto_failover_maximum_reached,
      auto_failover_other_nodes_down,auto_failover_cluster_too_small,
      auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
      ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
      ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
  {fts_memory_quota,256},
  {index_aware_rebalance_disabled,false},
  {log_redaction_default_cfg,[{redact_level,none}]},
  {max_bucket_count,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    30]},
  {memcached,[]},
  {memory_quota,1492},
  {nodes_wanted,['ns_1@cb.local']},
  {otp,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]},
  {password_policy,[{min_length,6},{must_present,[]}]},
  {quorum_nodes,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    'ns_1@cb.local']},
  {remote_clusters,[]},
  {replication,[{enabled,true}]},
  {rest,[{port,8091}]},
  {rest_creds,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
    {"<ud>admin</ud>",
     {auth,
      [{<<"plain">>,"*****"},
       {<<"sha512">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,
           <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
          {<<"i">>,4000}]}},
       {<<"sha256">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
          {<<"i">>,4000}]}},
       {<<"sha1">>,
        {[{<<"h">>,"*****"},
          {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
          {<<"i">>,4000}]}}]}}]},
  {retry_rebalance,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
    {enabled,false},
    {after_time_period,300},
    {max_attempts,1}]},
  {scramsha_fallback_salt,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    <<211,88,119,65,90,45,162,36,255,132,241,44>>]},
  {secure_headers,[]},
  {server_groups,
   [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
  {set_view_update_daemon,
   [{update_interval,5000},
    {update_min_changes,5000},
    {replica_update_min_changes,5000}]},
  {uuid,
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
    <<"b1cf035dd538cdacd1c975b6e16f1302">>]},
  {{couchdb,max_parallel_indexers},4},
  {{couchdb,max_parallel_replica_indexers},2},
  {{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{12,63852150140}}]}]},
  {{metakv,<<"/eventing/settings/config">>},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
    <<"{\"ram_quota\":256}">>]},
  {{metakv,<<"/indexing/settings/config">>},
   <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
  {{metakv,<<"/query/settings/config">>},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
    <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
  {{request_limit,capi},undefined},
  {{request_limit,rest},undefined},
  {{node,'ns_1@cb.local',address_family},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    inet]},
  {{node,'ns_1@cb.local',audit},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
  {{node,'ns_1@cb.local',capi_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8092]},
  {{node,'ns_1@cb.local',cbas_admin_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9110]},
  {{node,'ns_1@cb.local',cbas_cc_client_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9113]},
  {{node,'ns_1@cb.local',cbas_cc_cluster_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9112]},
  {{node,'ns_1@cb.local',cbas_cc_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9111]},
  {{node,'ns_1@cb.local',cbas_cluster_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9115]},
  {{node,'ns_1@cb.local',cbas_console_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9114]},
  {{node,'ns_1@cb.local',cbas_data_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9116]},
  {{node,'ns_1@cb.local',cbas_debug_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    -1]},
  {{node,'ns_1@cb.local',cbas_dirs},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
    "/opt/couchbase/var/lib/couchbase/data"]},
  {{node,'ns_1@cb.local',cbas_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8095]},
  {{node,'ns_1@cb.local',cbas_messaging_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9118]},
  {{node,'ns_1@cb.local',cbas_metadata_callback_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9119]},
  {{node,'ns_1@cb.local',cbas_metadata_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9121]},
  {{node,'ns_1@cb.local',cbas_parent_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9122]},
  {{node,'ns_1@cb.local',cbas_replication_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9120]},
  {{node,'ns_1@cb.local',cbas_result_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9117]},
  {{node,'ns_1@cb.local',cbas_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18095]},
  {{node,'ns_1@cb.local',compaction_daemon},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {check_interval,30},
    {min_db_file_size,131072},
    {min_view_file_size,20971520}]},
  {{node,'ns_1@cb.local',config_version},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {6,5}]},
  {{node,'ns_1@cb.local',erl_external_listeners},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {inet,false},
    {inet6,false}]},
  {{node,'ns_1@cb.local',eventing_debug_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9140]},
  {{node,'ns_1@cb.local',eventing_dir},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
    47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,
    105,98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
  {{node,'ns_1@cb.local',eventing_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8096]},
  {{node,'ns_1@cb.local',eventing_https_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18096]},
  {{node,'ns_1@cb.local',fts_grpc_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9130]},
  {{node,'ns_1@cb.local',fts_grpc_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    19130]},
  {{node,'ns_1@cb.local',fts_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8094]},
  {{node,'ns_1@cb.local',fts_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18094]},
  {{node,'ns_1@cb.local',indexer_admin_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9100]},
  {{node,'ns_1@cb.local',indexer_http_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9102]},
  {{node,'ns_1@cb.local',indexer_https_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    19102]},
  {{node,'ns_1@cb.local',indexer_scan_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9101]},
  {{node,'ns_1@cb.local',indexer_stcatchup_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9104]},
  {{node,'ns_1@cb.local',indexer_stinit_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9103]},
  {{node,'ns_1@cb.local',indexer_stmaint_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9105]},
  {{node,'ns_1@cb.local',is_enterprise},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    true]},
  {{node,'ns_1@cb.local',isasl},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
  {{node,'ns_1@cb.local',membership},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    active]},
  {{node,'ns_1@cb.local',memcached},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,11210},
    {dedicated_port,11209},
    {dedicated_ssl_port,11206},
    {ssl_port,11207},
    {admin_user,"@ns_server"},
    {other_users,
     ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
      "@cbas"]},
    {admin_pass,"*****"},
    {engines,
     [{membase,
       [{engine,"/opt/couchbase/lib/memcached/ep.so"},
        {static_config_string,"failpartialwarmup=false"}]},
      {memcached,
       [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
        {static_config_string,"vb0=true"}]}]},
    {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
    {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
    {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
    {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
    {log_prefix,"memcached.log"},
    {log_generations,20},
    {log_cyclesize,10485760},
    {log_sleeptime,19},
    {log_rotation_period,39003}]},
  {{node,'ns_1@cb.local',memcached_config},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    {[{interfaces,
       {memcached_config_mgr,omit_missing_mcd_ports,
        [{[{host,<<"*">>},
           {port,port},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_port},
           {system,true},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,ssl_port},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
         {[{host,<<"*">>},
           {port,dedicated_ssl_port},
           {system,true},
           {ssl,
            {[{key,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
              {cert,
               <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
           {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
           {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
      {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
      {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
      {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
      {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
      {connection_idle_time,connection_idle_time},
      {privilege_debug,privilege_debug},
      {breakpad,
       {[{enabled,breakpad_enabled},
         {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
      {opentracing,
       {[{enabled,opentracing_enabled},
         {module,{"~s",[opentracing_module]}},
         {config,{"~s",[opentracing_config]}}]}},
      {admin,{"~s",[admin_user]}},
      {verbosity,verbosity},
      {audit_file,{"~s",[audit_file]}},
      {rbac_file,{"~s",[rbac_file]}},
      {dedupe_nmvb_maps,dedupe_nmvb_maps},
      {tracing_enabled,tracing_enabled},
      {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
      {xattr_enabled,true},
      {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
      {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
      {max_connections,max_connections},
      {system_connections,system_connections},
      {num_reader_threads,num_reader_threads},
      {num_writer_threads,num_writer_threads},
      {logger,
       {[{filename,{"~s/~s",[log_path,log_prefix]}},
         {cyclesize,log_cyclesize},
         {sleeptime,log_sleeptime}]}},
      {external_auth_service,
       {memcached_config_mgr,get_external_auth_service,[]}},
      {active_external_users_push_interval,
       {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
  {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    11206]},
  {{node,'ns_1@cb.local',memcached_defaults},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {max_connections,65000},
    {system_connections,5000},
    {connection_idle_time,0},
    {verbosity,0},
    {privilege_debug,false},
    {opentracing_enabled,false},
    {opentracing_module,[]},
    {opentracing_config,[]},
    {breakpad_enabled,true},
    {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
    {dedupe_nmvb_maps,false},
    {tracing_enabled,true},
    {datatype_snappy,true},
    {num_reader_threads,<<"default">>},
    {num_writer_threads,<<"default">>}]},
  {{node,'ns_1@cb.local',moxi},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,0}]},
  {{node,'ns_1@cb.local',node_encryption},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    false]},
  {{node,'ns_1@cb.local',ns_log},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
  {{node,'ns_1@cb.local',port_servers},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
  {{node,'ns_1@cb.local',projector_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9999]},
  {{node,'ns_1@cb.local',projector_ssl_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9999]},
  {{node,'ns_1@cb.local',query_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    8093]},
  {{node,'ns_1@cb.local',rest},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
    {port,8091},
    {port_meta,global}]},
  {{node,'ns_1@cb.local',saslauthd_enabled},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    true]},
  {{node,'ns_1@cb.local',ssl_capi_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18092]},
  {{node,'ns_1@cb.local',ssl_query_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18093]},
  {{node,'ns_1@cb.local',ssl_rest_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    18091]},
  {{node,'ns_1@cb.local',uuid},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    <<"949a059dcc29e773ec37709a7973341b">>]},
  {{node,'ns_1@cb.local',xdcr_rest_port},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    9998]},
  {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
   [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
    false]}]]
[ns_server:info,2023-05-27T08:32:47.253Z,ns_1@cb.local:ns_config<0.193.0>:ns_config:load_config:1149]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   false]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9998]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   <<"949a059dcc29e773ec37709a7973341b">>]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18091]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18093]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18092]},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,8091},
   {port_meta,global}]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9999]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',node_encryption},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   false]},
 {{node,'ns_1@cb.local',moxi},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,0}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {opentracing_enabled,false},
   {opentracing_module,[]},
   {opentracing_config,[]},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>}]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   11206]},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {[{interfaces,
      {memcached_config_mgr,omit_missing_mcd_ports,
       [{[{host,<<"*">>},
          {port,port},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_port},
          {system,true},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,ssl_port},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
        {[{host,<<"*">>},
          {port,dedicated_ssl_port},
          {system,true},
          {ssl,
           {[{key,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
             {cert,
              <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
          {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
          {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
     {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
     {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {opentracing,
      {[{enabled,opentracing_enabled},
        {module,{"~s",[opentracing_module]}},
        {config,{"~s",[opentracing_config]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize},
        {sleeptime,log_sleeptime}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing",
     "@cbas"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_sleeptime,19},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   active]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   true]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9100]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8094]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9130]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8096]},
 {{node,'ns_1@cb.local',eventing_dir},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9140]},
 {{node,'ns_1@cb.local',erl_external_listeners},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {inet,false},
   {inet6,false}]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {6,5}]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8095]},
 {{node,'ns_1@cb.local',cbas_dirs},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
   "/opt/couchbase/var/lib/couchbase/data"]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   9110]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   8092]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]},
 {{node,'ns_1@cb.local',address_family},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   inet]},
 {{request_limit,rest},undefined},
 {{request_limit,capi},undefined},
 {{metakv,<<"/query/settings/config">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
   <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>]},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{metakv,<<"/eventing/settings/config">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
   <<"{\"ram_quota\":256}">>]},
 {{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>},
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{12,63852150140}}]}]},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {uuid,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
   <<"b1cf035dd538cdacd1c975b6e16f1302">>]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {secure_headers,[]},
 {scramsha_fallback_salt,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
   <<211,88,119,65,90,45,162,36,255,132,241,44>>]},
 {retry_rebalance,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {enabled,false},
   {after_time_period,300},
   {max_attempts,1}]},
 {rest_creds,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
   {"<ud>admin</ud>",
    {auth,
     [{<<"plain">>,"*****"},
      {<<"sha512">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,
          <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
         {<<"i">>,4000}]}},
      {<<"sha256">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
         {<<"i">>,4000}]}},
      {<<"sha1">>,
       {[{<<"h">>,"*****"},
         {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
         {<<"i">>,4000}]}}]}}]},
 {rest,[{port,8091}]},
 {replication,[{enabled,true}]},
 {remote_clusters,[]},
 {quorum_nodes,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   'ns_1@cb.local']},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {otp,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]},
 {nodes_wanted,['ns_1@cb.local']},
 {memory_quota,1492},
 {memcached,[]},
 {max_bucket_count,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|30]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {index_aware_rebalance_disabled,false},
 {fts_memory_quota,256},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     ep_clock_cas_drift_threshold_exceeded,communication_issue]}]},
 {drop_request_memory_threshold_mib,undefined},
 {cluster_compat_version,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},
   6,5]},
 {client_cert_auth,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {state,"disable"},
   {prefixes,[]}]},
 {cert_and_pkey,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
   {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U0J7myDdqIj25WTddDPRaYC2zr9QZtGfwU\nZ9Tff/yVZP6c8JlMVpgyPXMdkV26rINFZWXszttCravqmy8uM1wj2If4H0iA9HJB\n+lpIsnPL8tpv44Yh3Ao0SIh+RpDgrIHU7524ecHIRi6hXpCHCejmbmSzJ5lSnZ0y\nVAX8U2L7QKoy/i1rNri8EjTlSaxmAMUxd6yQiVGIECsH/0sDocK8pQ1cYkzThBrv\nZdtNAgMBAAGjODA2MA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQC9vJLL4XYgi575\nJ7Ko8jo++XojWVhz2pKlj8FCjtZmfHNRig8JGusbMfGZb2fMUkvN86jtWR8/tamu\nZLt7peK8D4pkpWHzubNE27dbKI8p0QGEpcYI31YSPA5OPSdzwy4GVRgs1ev8rPMA\nC0lAJvyF9MZK0f4b9+T4FoYJxuv7WnSf0tIrqx4BKI6B7P818DY3KZxgCmMQOcYL\n77XiMIKuiba5Km+VcQkv1lxgTmemHyXeFYaIjWDHYhHp23WahV1FoBt4bf+5GdZn\nnZAWAA03hELa32QzN58NB3Z5xdTguDafVsydS+ZhS38aiON0H2potwZivAlUE4yj\nI47GYXM0\n-----END CERTIFICATE-----\n">>,
    <<"*****">>}]},
 {cbas_memory_quota,1129},
 {buckets,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
   {configs,[]}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {audit_decriptors,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {8243,
    [{name,<<"mutate document">>},
     {description,<<"Document was mutated via the REST API">>},
     {enabled,true},
     {module,ns_server}]},
   {8255,
    [{name,<<"read document">>},
     {description,<<"Document was read via the REST API">>},
     {enabled,false},
     {module,ns_server}]},
   {8257,
    [{name,<<"alert email sent">>},
     {description,<<"An alert email was successfully sent">>},
     {enabled,true},
     {module,ns_server}]},
   {20480,
    [{name,<<"opened DCP connection">>},
     {description,<<"opened DCP connection">>},
     {enabled,true},
     {module,memcached}]},
   {20482,
    [{name,<<"external memcached bucket flush">>},
     {description,
      <<"External user flushed the content of a memcached bucket">>},
     {enabled,true},
     {module,memcached}]},
   {20483,
    [{name,<<"invalid packet">>},
     {description,<<"Rejected an invalid packet">>},
     {enabled,true},
     {module,memcached}]},
   {20485,
    [{name,<<"authentication succeeded">>},
     {description,<<"Authentication to the cluster succeeded">>},
     {enabled,false},
     {module,memcached}]},
   {20488,
    [{name,<<"document read">>},
     {description,<<"Document was read">>},
     {enabled,false},
     {module,memcached}]},
   {20489,
    [{name,<<"document locked">>},
     {description,<<"Document was locked">>},
     {enabled,false},
     {module,memcached}]},
   {20490,
    [{name,<<"document modify">>},
     {description,<<"Document was modified">>},
     {enabled,false},
     {module,memcached}]},
   {20491,
    [{name,<<"document delete">>},
     {description,<<"Document was deleted">>},
     {enabled,false},
     {module,memcached}]},
   {20492,
    [{name,<<"select bucket">>},
     {description,<<"The specified bucket was selected">>},
     {enabled,true},
     {module,memcached}]},
   {28672,
    [{name,<<"SELECT statement">>},
     {description,<<"A N1QL SELECT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28673,
    [{name,<<"EXPLAIN statement">>},
     {description,<<"A N1QL EXPLAIN statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28674,
    [{name,<<"PREPARE statement">>},
     {description,<<"A N1QL PREPARE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28675,
    [{name,<<"INFER statement">>},
     {description,<<"A N1QL INFER statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28676,
    [{name,<<"INSERT statement">>},
     {description,<<"A N1QL INSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28677,
    [{name,<<"UPSERT statement">>},
     {description,<<"A N1QL UPSERT statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28678,
    [{name,<<"DELETE statement">>},
     {description,<<"A N1QL DELETE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28679,
    [{name,<<"UPDATE statement">>},
     {description,<<"A N1QL UPDATE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28680,
    [{name,<<"MERGE statement">>},
     {description,<<"A N1QL MERGE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28681,
    [{name,<<"CREATE INDEX statement">>},
     {description,<<"A N1QL CREATE INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28682,
    [{name,<<"DROP INDEX statement">>},
     {description,<<"A N1QL DROP INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28683,
    [{name,<<"ALTER INDEX statement">>},
     {description,<<"A N1QL ALTER INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28684,
    [{name,<<"BUILD INDEX statement">>},
     {description,<<"A N1QL BUILD INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28685,
    [{name,<<"GRANT ROLE statement">>},
     {description,<<"A N1QL GRANT ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28686,
    [{name,<<"REVOKE ROLE statement">>},
     {description,<<"A N1QL REVOKE ROLE statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28687,
    [{name,<<"UNRECOGNIZED statement">>},
     {description,
      <<"An unrecognized statement was received by the N1QL query engine">>},
     {enabled,false},
     {module,n1ql}]},
   {28688,
    [{name,<<"CREATE PRIMARY INDEX statement">>},
     {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
     {enabled,false},
     {module,n1ql}]},
   {28689,
    [{name,<<"/admin/stats API request">>},
     {description,<<"An HTTP request was made to the API at /admin/stats.">>},
     {enabled,false},
     {module,n1ql}]},
   {28690,
    [{name,<<"/admin/vitals API request">>},
     {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
     {enabled,false},
     {module,n1ql}]},
   {28691,
    [{name,<<"/admin/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28692,
    [{name,<<"/admin/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28693,
    [{name,<<"/admin/indexes/prepareds API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
     {enabled,false},
     {module,n1ql}]},
   {28694,
    [{name,<<"/admin/indexes/active_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28695,
    [{name,<<"/admin/indexes/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28697,
    [{name,<<"/admin/ping API request">>},
     {description,<<"An HTTP request was made to the API at /admin/ping.">>},
     {enabled,false},
     {module,n1ql}]},
   {28698,
    [{name,<<"/admin/config API request">>},
     {description,<<"An HTTP request was made to the API at /admin/config.">>},
     {enabled,false},
     {module,n1ql}]},
   {28699,
    [{name,<<"/admin/ssl_cert API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/ssl_cert.">>},
     {enabled,false},
     {module,n1ql}]},
   {28700,
    [{name,<<"/admin/settings API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/settings.">>},
     {enabled,false},
     {module,n1ql}]},
   {28701,
    [{name,<<"/admin/clusters API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/clusters.">>},
     {enabled,false},
     {module,n1ql}]},
   {28702,
    [{name,<<"/admin/completed_requests API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/completed_requests.">>},
     {enabled,false},
     {module,n1ql}]},
   {28704,
    [{name,<<"/admin/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {28705,
    [{name,<<"/admin/indexes/functions API request">>},
     {description,
      <<"An HTTP request was made to the API at /admin/indexes/functions.">>},
     {enabled,false},
     {module,n1ql}]},
   {32768,
    [{name,<<"Create Function">>},
     {description,<<"Eventing function definition was created or updated">>},
     {enabled,true},
     {module,eventing}]},
   {32769,
    [{name,<<"Delete Function">>},
     {description,<<"Eventing function definition was deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32770,
    [{name,<<"Fetch Functions">>},
     {description,<<"Eventing function definition was read">>},
     {enabled,false},
     {module,eventing}]},
   {32771,
    [{name,<<"List Deployed">>},
     {description,<<"Eventing deployed functions list was read">>},
     {enabled,false},
     {module,eventing}]},
   {32772,
    [{name,<<"Fetch Drafts">>},
     {description,<<"Eventing function draft definitions were read">>},
     {enabled,false},
     {module,eventing}]},
   {32773,
    [{name,<<"Delete Drafts">>},
     {description,<<"Eventing function draft definitions were deleted">>},
     {enabled,true},
     {module,eventing}]},
   {32774,
    [{name,<<"Save Draft">>},
     {description,<<"Save a draft definition to the store">>},
     {enabled,true},
     {module,eventing}]},
   {32775,
    [{name,<<"Start Debug">>},
     {description,<<"Start eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32776,
    [{name,<<"Stop Debug">>},
     {description,<<"Stop eventing function debugger">>},
     {enabled,true},
     {module,eventing}]},
   {32777,
    [{name,<<"Start Tracing">>},
     {description,<<"Start tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32778,
    [{name,<<"Stop Tracing">>},
     {description,<<"Stop tracing eventing function execution">>},
     {enabled,true},
     {module,eventing}]},
   {32779,
    [{name,<<"Set Settings">>},
     {description,<<"Save settings for a given app">>},
     {enabled,true},
     {module,eventing}]},
   {32780,
    [{name,<<"Fetch Config">>},
     {description,<<"Get config for eventing">>},
     {enabled,false},
     {module,eventing}]},
   {32781,
    [{name,<<"Save Config">>},
     {description,<<"Save config for eventing">>},
     {enabled,true},
     {module,eventing}]},
   {32782,
    [{name,<<"Cleanup Eventing">>},
     {description,<<"Clears up app definitions and settings from metakv">>},
     {enabled,true},
     {module,eventing}]},
   {32783,
    [{name,<<"Get Settings">>},
     {description,<<"Get settings for a given app">>},
     {enabled,false},
     {module,eventing}]},
   {32784,
    [{name,<<"Import Functions">>},
     {description,<<"Import a list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32785,
    [{name,<<"Export Functions">>},
     {description,<<"Export the list of functions">>},
     {enabled,false},
     {module,eventing}]},
   {32786,
    [{name,<<"List Running">>},
     {description,<<"Eventing running function list was read">>},
     {enabled,false},
     {module,eventing}]},
   {36865,
    [{name,<<"Service configuration change">>},
     {description,<<"A successful service configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {36866,
    [{name,<<"Node configuration change">>},
     {description,<<"A successful node configuration change was made.">>},
     {enabled,true},
     {module,analytics}]},
   {40960,
    [{name,<<"Create Design Doc">>},
     {description,<<"Design Doc is Created">>},
     {enabled,true},
     {module,view_engine}]},
   {40961,
    [{name,<<"Delete Design Doc">>},
     {description,<<"Design Doc is Deleted">>},
     {enabled,true},
     {module,view_engine}]},
   {40962,
    [{name,<<"Query DDoc Meta Data">>},
     {description,<<"Design Doc Meta Data Query Request">>},
     {enabled,true},
     {module,view_engine}]},
   {40963,
    [{name,<<"View Query">>},
     {description,<<"View Query Request">>},
     {enabled,false},
     {module,view_engine}]},
   {40964,
    [{name,<<"Update Design Doc">>},
     {description,<<"Design Doc is Updated">>},
     {enabled,true},
     {module,view_engine}]}]},
 {audit,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
   {enabled,[]},
   {disabled_users,[]},
   {auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {auto_failover_cfg,
  [{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
   {enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]}]
[error_logger:info,2023-05-27T08:32:47.256Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.193.0>},
                       {id,ns_config},
                       {mfargs,
                           {ns_config,start_link,
                               ["/opt/couchbase/etc/couchbase/config",
                                ns_config_default]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.257Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.199.0>},
                       {id,ns_config_remote},
                       {mfargs,{ns_config_replica,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.258Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_config_sup}
             started: [{pid,<0.200.0>},
                       {id,ns_config_log},
                       {mfargs,{ns_config_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.258Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.190.0>},
                       {id,ns_config_sup},
                       {mfargs,{ns_config_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:47.259Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{13,63852395567}}]}]
[error_logger:info,2023-05-27T08:32:47.259Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.202.0>},
                       {id,netconfig_updater},
                       {mfargs,{netconfig_updater,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.260Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.205.0>},
                       {id,json_rpc_connection_sup},
                       {mfargs,{json_rpc_connection_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.265Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.208.0>},
                       {name,remote_monitors},
                       {mfargs,{remote_monitors,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:47.266Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:58]Barrier menelaus_barrier has started
[error_logger:info,2023-05-27T08:32:47.266Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.209.0>},
                       {name,menelaus_barrier},
                       {mfargs,{menelaus_sup,barrier_start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.267Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.210.0>},
                       {name,rest_lhttpc_pool},
                       {mfargs,
                           {lhttpc_manager,start_link,
                               [[{name,rest_lhttpc_pool},
                                 {connection_timeout,120000},
                                 {pool_size,20}]]}},
                       {restart_type,{permanent,1}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.269Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.211.0>},
                       {name,memcached_refresh},
                       {mfargs,{memcached_refresh,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.270Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.213.0>},
                       {id,ssl_service_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ssl_service_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:47.328Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:debug,2023-05-27T08:32:47.328Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet6_tls_dist because listener is not started
[ns_server:debug,2023-05-27T08:32:47.328Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: ignoring closing of inet_tls_dist because listener is not started
[ns_server:info,2023-05-27T08:32:47.348Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:init:462]Used ssl options:
[{keyfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {certfile,"/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
 {versions,['tlsv1.1','tlsv1.2']},
 {cacerts,[<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,95,103,117,59,166,49,
             58,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,48,32,
             6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,101,114,
             118,101,114,32,55,48,49,56,51,48,98,50,48,30,23,13,49,51,48,49,
             48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,51,53,57,
             53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,117,99,104,
             98,97,115,101,32,83,101,114,118,101,114,32,55,48,49,56,51,48,98,
             50,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,1,5,0,3,130,1,
             15,0,48,130,1,10,2,130,1,1,0,192,97,0,160,196,124,86,76,76,241,
             92,35,220,170,212,78,154,52,104,79,25,232,189,131,139,135,15,
             155,214,199,43,184,2,114,204,250,241,110,76,96,156,151,94,187,0,
             216,138,82,17,125,188,1,163,168,204,18,49,110,109,211,194,87,65,
             18,22,218,61,235,124,91,95,73,186,220,237,129,145,9,14,31,207,
             206,211,101,52,39,185,178,13,218,136,143,110,86,77,215,67,61,22,
             152,11,108,235,245,6,109,25,252,20,103,212,223,127,252,149,100,
             254,156,240,153,76,86,152,50,61,115,29,145,93,186,172,131,69,
             101,101,236,206,219,66,173,171,234,155,47,46,51,92,35,216,135,
             248,31,72,128,244,114,65,250,90,72,178,115,203,242,218,111,227,
             134,33,220,10,52,72,136,126,70,144,224,172,129,212,239,157,184,
             121,193,200,70,46,161,94,144,135,9,232,230,110,100,179,39,153,
             82,157,157,50,84,5,252,83,98,251,64,170,50,254,45,107,54,184,
             188,18,52,229,73,172,102,0,197,49,119,172,144,137,81,136,16,43,
             7,255,75,3,161,194,188,165,13,92,98,76,211,132,26,239,101,219,
             77,2,3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,255,4,4,3,2,2,
             164,48,19,6,3,85,29,37,4,12,48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,
             3,85,29,19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,134,72,134,247,
             13,1,1,11,5,0,3,130,1,1,0,189,188,146,203,225,118,32,139,158,
             249,39,178,168,242,58,62,249,122,35,89,88,115,218,146,165,143,
             193,66,142,214,102,124,115,81,138,15,9,26,235,27,49,241,153,111,
             103,204,82,75,205,243,168,237,89,31,63,181,169,174,100,187,123,
             165,226,188,15,138,100,165,97,243,185,179,68,219,183,91,40,143,
             41,209,1,132,165,198,8,223,86,18,60,14,78,61,39,115,195,46,6,85,
             24,44,213,235,252,172,243,0,11,73,64,38,252,133,244,198,74,209,
             254,27,247,228,248,22,134,9,198,235,251,90,116,159,210,210,43,
             171,30,1,40,142,129,236,255,53,240,54,55,41,156,96,10,99,16,57,
             198,11,239,181,226,48,130,174,137,182,185,42,111,149,113,9,47,
             214,92,96,78,103,166,31,37,222,21,134,136,141,96,199,98,17,233,
             219,117,154,133,93,69,160,27,120,109,255,185,25,214,103,157,144,
             22,0,13,55,132,66,218,223,100,51,55,159,13,7,118,121,197,212,
             224,184,54,159,86,204,157,75,230,97,75,127,26,136,227,116,31,
             106,104,183,6,98,188,9,84,19,140,163,35,142,198,97,115,52>>]},
 {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,10,
       118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,158,
       232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,66,
       211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,250,
       145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,104,
       159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,246,
       169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,110,
       167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,118,190,
       67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,74,8,205,
       174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,221,95,184,
       110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,76,187,66,
       211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,69,254,147,
       103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,202,133,173,
       72,6,69,167,89,112,174,40,229,171,2,1,2>>},
 {ciphers,[{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
           {ecdhe_rsa,aes_256_gcm,aead,sha384},
           {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdhe_rsa,aes_256_cbc,sha384,sha384},
           {ecdh_ecdsa,aes_256_gcm,aead,sha384},
           {ecdh_rsa,aes_256_gcm,aead,sha384},
           {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
           {ecdh_rsa,aes_256_cbc,sha384,sha384},
           {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
           {ecdhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,chacha20_poly1305,aead,sha256},
           {dhe_rsa,aes_256_gcm,aead,sha384},
           {dhe_dss,aes_256_gcm,aead,sha384},
           {dhe_rsa,aes_256_cbc,sha256},
           {dhe_dss,aes_256_cbc,sha256},
           {rsa,aes_256_gcm,aead,sha384},
           {rsa,aes_256_cbc,sha256},
           {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
           {ecdhe_rsa,aes_128_gcm,aead,sha256},
           {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdhe_rsa,aes_128_cbc,sha256,sha256},
           {ecdh_ecdsa,aes_128_gcm,aead,sha256},
           {ecdh_rsa,aes_128_gcm,aead,sha256},
           {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
           {ecdh_rsa,aes_128_cbc,sha256,sha256},
           {dhe_rsa,aes_128_gcm,aead,sha256},
           {dhe_dss,aes_128_gcm,aead,sha256},
           {dhe_rsa,aes_128_cbc,sha256},
           {dhe_dss,aes_128_cbc,sha256},
           {rsa,aes_128_gcm,aead,sha256},
           {rsa,aes_128_cbc,sha256},
           {ecdhe_ecdsa,aes_256_cbc,sha},
           {ecdhe_rsa,aes_256_cbc,sha},
           {dhe_rsa,aes_256_cbc,sha},
           {dhe_dss,aes_256_cbc,sha},
           {ecdh_ecdsa,aes_256_cbc,sha},
           {ecdh_rsa,aes_256_cbc,sha},
           {rsa,aes_256_cbc,sha},
           {ecdhe_ecdsa,aes_128_cbc,sha},
           {ecdhe_rsa,aes_128_cbc,sha},
           {dhe_rsa,aes_128_cbc,sha},
           {dhe_dss,aes_128_cbc,sha},
           {ecdh_ecdsa,aes_128_cbc,sha},
           {ecdh_rsa,aes_128_cbc,sha},
           {rsa,aes_128_cbc,sha},
           {ecdhe_ecdsa,'3des_ede_cbc',sha},
           {ecdhe_rsa,'3des_ede_cbc',sha},
           {dhe_rsa,'3des_ede_cbc',sha},
           {dhe_dss,'3des_ede_cbc',sha},
           {ecdh_ecdsa,'3des_ede_cbc',sha},
           {ecdh_rsa,'3des_ede_cbc',sha},
           {rsa,'3des_ede_cbc',sha}]},
 {honor_cipher_order,true},
 {secure_renegotiate,true},
 {client_renegotiation,false}]
[error_logger:info,2023-05-27T08:32:47.349Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.214.0>},
                       {id,ns_ssl_services_setup},
                       {mfargs,{ns_ssl_services_setup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:47.359Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-27T08:32:47.360Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-27T08:32:47.360Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-27T08:32:47.360Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:info,2023-05-27T08:32:47.376Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-27T08:32:47.376Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[error_logger:info,2023-05-27T08:32:47.375Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.218.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"0.0.0.0"},
                           {name,menelaus_web_ssl_ipv4},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:47.376Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-27T08:32:47.377Z,ns_1@cb.local:<0.217.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-27T08:32:47.377Z,ns_1@cb.local:<0.216.0>:restartable:start_child:98]Started child process <0.217.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2023-05-27T08:32:47.377Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.217.0>,menelaus_web}
             started: [{pid,<0.236.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                        {menelaus_web,http_server,
                         [[{ip,"::"},
                           {name,menelaus_web_ssl_ipv6},
                           {ssl,true},
                           {ssl_opts,
                            [{keyfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {certfile,
                              "/opt/couchbase/var/lib/couchbase/config/ssl-cert-key.pem"},
                             {versions,['tlsv1.1','tlsv1.2']},
                             {cacerts,
                              [<<48,130,3,2,48,130,1,234,160,3,2,1,2,2,8,23,
                                 95,103,117,59,166,49,58,48,13,6,9,42,134,72,
                                 134,247,13,1,1,11,5,0,48,36,49,34,48,32,6,3,
                                 85,4,3,19,25,67,111,117,99,104,98,97,115,
                                 101,32,83,101,114,118,101,114,32,55,48,49,
                                 56,51,48,98,50,48,30,23,13,49,51,48,49,48,
                                 49,48,48,48,48,48,48,90,23,13,52,57,49,50,
                                 51,49,50,51,53,57,53,57,90,48,36,49,34,48,
                                 32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,
                                 115,101,32,83,101,114,118,101,114,32,55,48,
                                 49,56,51,48,98,50,48,130,1,34,48,13,6,9,42,
                                 134,72,134,247,13,1,1,1,5,0,3,130,1,15,0,48,
                                 130,1,10,2,130,1,1,0,192,97,0,160,196,124,
                                 86,76,76,241,92,35,220,170,212,78,154,52,
                                 104,79,25,232,189,131,139,135,15,155,214,
                                 199,43,184,2,114,204,250,241,110,76,96,156,
                                 151,94,187,0,216,138,82,17,125,188,1,163,
                                 168,204,18,49,110,109,211,194,87,65,18,22,
                                 218,61,235,124,91,95,73,186,220,237,129,145,
                                 9,14,31,207,206,211,101,52,39,185,178,13,
                                 218,136,143,110,86,77,215,67,61,22,152,11,
                                 108,235,245,6,109,25,252,20,103,212,223,127,
                                 252,149,100,254,156,240,153,76,86,152,50,61,
                                 115,29,145,93,186,172,131,69,101,101,236,
                                 206,219,66,173,171,234,155,47,46,51,92,35,
                                 216,135,248,31,72,128,244,114,65,250,90,72,
                                 178,115,203,242,218,111,227,134,33,220,10,
                                 52,72,136,126,70,144,224,172,129,212,239,
                                 157,184,121,193,200,70,46,161,94,144,135,9,
                                 232,230,110,100,179,39,153,82,157,157,50,84,
                                 5,252,83,98,251,64,170,50,254,45,107,54,184,
                                 188,18,52,229,73,172,102,0,197,49,119,172,
                                 144,137,81,136,16,43,7,255,75,3,161,194,188,
                                 165,13,92,98,76,211,132,26,239,101,219,77,2,
                                 3,1,0,1,163,56,48,54,48,14,6,3,85,29,15,1,1,
                                 255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
                                 48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,
                                 19,1,1,255,4,5,48,3,1,1,255,48,13,6,9,42,
                                 134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,
                                 189,188,146,203,225,118,32,139,158,249,39,
                                 178,168,242,58,62,249,122,35,89,88,115,218,
                                 146,165,143,193,66,142,214,102,124,115,81,
                                 138,15,9,26,235,27,49,241,153,111,103,204,
                                 82,75,205,243,168,237,89,31,63,181,169,174,
                                 100,187,123,165,226,188,15,138,100,165,97,
                                 243,185,179,68,219,183,91,40,143,41,209,1,
                                 132,165,198,8,223,86,18,60,14,78,61,39,115,
                                 195,46,6,85,24,44,213,235,252,172,243,0,11,
                                 73,64,38,252,133,244,198,74,209,254,27,247,
                                 228,248,22,134,9,198,235,251,90,116,159,210,
                                 210,43,171,30,1,40,142,129,236,255,53,240,
                                 54,55,41,156,96,10,99,16,57,198,11,239,181,
                                 226,48,130,174,137,182,185,42,111,149,113,9,
                                 47,214,92,96,78,103,166,31,37,222,21,134,
                                 136,141,96,199,98,17,233,219,117,154,133,93,
                                 69,160,27,120,109,255,185,25,214,103,157,
                                 144,22,0,13,55,132,66,218,223,100,51,55,159,
                                 13,7,118,121,197,212,224,184,54,159,86,204,
                                 157,75,230,97,75,127,26,136,227,116,31,106,
                                 104,183,6,98,188,9,84,19,140,163,35,142,198,
                                 97,115,52>>]},
                             {dh,
                              <<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,
                                35,238,246,5,77,93,120,10,118,129,36,52,111,
                                193,167,220,49,229,106,105,152,133,121,157,73,
                                158,232,153,197,197,21,171,140,30,207,52,165,
                                45,8,221,162,21,199,183,66,211,247,51,224,102,
                                214,190,130,96,253,218,193,35,43,139,145,89,
                                200,250,145,92,50,80,134,135,188,205,254,148,
                                122,136,237,220,186,147,187,104,159,36,147,
                                217,117,74,35,163,145,249,175,242,18,221,124,
                                54,140,16,246,169,84,252,45,47,99,136,30,60,
                                189,203,61,86,225,117,255,4,91,46,110,167,173,
                                106,51,65,10,248,94,225,223,73,40,232,140,26,
                                11,67,170,118,190,67,31,127,233,39,68,88,132,
                                171,224,62,187,207,160,189,209,101,74,8,205,
                                174,146,173,80,105,144,246,25,153,86,36,24,
                                178,163,64,202,221,95,184,110,244,32,226,217,
                                34,55,188,230,55,16,216,247,173,246,139,76,
                                187,66,211,159,17,46,20,18,48,80,27,250,96,
                                189,29,214,234,241,34,69,254,147,103,220,133,
                                40,164,84,8,44,241,61,164,151,9,135,41,60,75,
                                4,202,133,173,72,6,69,167,89,112,174,40,229,
                                171,2,1,2>>},
                             {ciphers,
                              [{ecdhe_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdhe_rsa,aes_256_gcm,aead,sha384},
                               {ecdhe_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_rsa,aes_256_cbc,sha384,sha384},
                               {ecdh_ecdsa,aes_256_gcm,aead,sha384},
                               {ecdh_rsa,aes_256_gcm,aead,sha384},
                               {ecdh_ecdsa,aes_256_cbc,sha384,sha384},
                               {ecdh_rsa,aes_256_cbc,sha384,sha384},
                               {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
                               {ecdhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,chacha20_poly1305,aead,sha256},
                               {dhe_rsa,aes_256_gcm,aead,sha384},
                               {dhe_dss,aes_256_gcm,aead,sha384},
                               {dhe_rsa,aes_256_cbc,sha256},
                               {dhe_dss,aes_256_cbc,sha256},
                               {rsa,aes_256_gcm,aead,sha384},
                               {rsa,aes_256_cbc,sha256},
                               {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdhe_rsa,aes_128_gcm,aead,sha256},
                               {ecdhe_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdhe_rsa,aes_128_cbc,sha256,sha256},
                               {ecdh_ecdsa,aes_128_gcm,aead,sha256},
                               {ecdh_rsa,aes_128_gcm,aead,sha256},
                               {ecdh_ecdsa,aes_128_cbc,sha256,sha256},
                               {ecdh_rsa,aes_128_cbc,sha256,sha256},
                               {dhe_rsa,aes_128_gcm,aead,sha256},
                               {dhe_dss,aes_128_gcm,aead,sha256},
                               {dhe_rsa,aes_128_cbc,sha256},
                               {dhe_dss,aes_128_cbc,sha256},
                               {rsa,aes_128_gcm,aead,sha256},
                               {rsa,aes_128_cbc,sha256},
                               {ecdhe_ecdsa,aes_256_cbc,sha},
                               {ecdhe_rsa,aes_256_cbc,sha},
                               {dhe_rsa,aes_256_cbc,sha},
                               {dhe_dss,aes_256_cbc,sha},
                               {ecdh_ecdsa,aes_256_cbc,sha},
                               {ecdh_rsa,aes_256_cbc,sha},
                               {rsa,aes_256_cbc,sha},
                               {ecdhe_ecdsa,aes_128_cbc,sha},
                               {ecdhe_rsa,aes_128_cbc,sha},
                               {dhe_rsa,aes_128_cbc,sha},
                               {dhe_dss,aes_128_cbc,sha},
                               {ecdh_ecdsa,aes_128_cbc,sha},
                               {ecdh_rsa,aes_128_cbc,sha},
                               {rsa,aes_128_cbc,sha},
                               {ecdhe_ecdsa,'3des_ede_cbc',sha},
                               {ecdhe_rsa,'3des_ede_cbc',sha},
                               {dhe_rsa,'3des_ede_cbc',sha},
                               {dhe_dss,'3des_ede_cbc',sha},
                               {ecdh_ecdsa,'3des_ede_cbc',sha},
                               {ecdh_rsa,'3des_ede_cbc',sha},
                               {rsa,'3des_ede_cbc',sha}]},
                             {honor_cipher_order,true},
                             {secure_renegotiate,true},
                             {client_renegotiation,false}]},
                           {port,18091}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.378Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_ssl_services_sup}
             started: [{pid,<0.216.0>},
                       {id,ns_rest_ssl_service},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_ssl_services_setup,
                                    start_link_rest_service,[]},
                                1000]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.378Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.212.0>},
                       {name,ns_ssl_services_sup},
                       {mfargs,{ns_ssl_services_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.382Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.254.0>},
                       {name,ldap_auth_cache},
                       {mfargs,{ldap_auth_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.383Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.257.0>},
                       {id,user_storage_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,user_storage_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.386Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.259.0>},
                       {id,users_replicator},
                       {mfargs,{menelaus_users,start_replicator,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:47.387Z,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:54]Start waiting for startup
[ns_server:debug,2023-05-27T08:32:47.388Z,ns_1@cb.local:users_storage<0.260.0>:replicated_storage:anounce_startup:68]Announce my startup to <0.259.0>
[ns_server:debug,2023-05-27T08:32:47.388Z,ns_1@cb.local:users_replicator<0.259.0>:replicated_storage:wait_for_startup:57]Received replicated storage registration from <0.260.0>
[ns_server:debug,2023-05-27T08:32:47.389Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[error_logger:info,2023-05-27T08:32:47.389Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_storage_sup}
             started: [{pid,<0.260.0>},
                       {id,users_storage},
                       {mfargs,{menelaus_users,start_storage,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.389Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.258.0>},
                       {id,users_storage_sup},
                       {mfargs,{users_storage_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:47.390Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:init:47]Starting versioned cache compiled_roles_cache
[error_logger:info,2023-05-27T08:32:47.390Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.262.0>},
                       {id,compiled_roles_cache},
                       {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.394Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,users_sup}
             started: [{pid,<0.265.0>},
                       {id,roles_cache},
                       {mfargs,{roles_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.394Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.256.0>},
                       {name,users_sup},
                       {mfargs,{users_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.396Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.269.0>},
                       {id,dets_sup},
                       {mfargs,{dets_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:47.396Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.270.0>},
                       {id,dets},
                       {mfargs,{dets_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:47.402Z,ns_1@cb.local:wait_link_to_couchdb_node<0.273.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:152]Waiting for ns_couchdb node to start
[error_logger:info,2023-05-27T08:32:47.402Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.268.0>},
                       {name,start_couchdb_node},
                       {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
                       {restart_type,{permanent,5}},
                       {shutdown,86400000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:47.402Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-27T08:32:47.402Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-27T08:32:47.402Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.151144609.1471676420.256888>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-27T08:32:47.402Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.151144609.1471676420.256888>,
                                  inet_tcp_dist,<0.276.0>,
                                  #Ref<0.151144609.1471676420.256889>}
[ns_server:debug,2023-05-27T08:32:47.403Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.151144609.1471676420.256888>,
                               inet_tcp_dist,<0.276.0>,
                               #Ref<0.151144609.1471676420.256889>}
[error_logger:info,2023-05-27T08:32:47.403Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.276.0>,shutdown}}
[ns_server:debug,2023-05-27T08:32:47.403Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-05-27T08:32:47.403Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2023-05-27T08:32:47.413Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:convert_docs_to_55_in_dets:209]Checking for pre 5.5 records in dets: users_storage
[ns_server:debug,2023-05-27T08:32:47.414Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:init_after_ack:170]Loading 0 items, 300 words took 24ms
[ns_server:debug,2023-05-27T08:32:47.415Z,ns_1@cb.local:users_replicator<0.259.0>:doc_replicator:loop:60]doing replicate_newnodes_docs
[error_logger:info,2023-05-27T08:32:47.603Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-27T08:32:47.603Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-05-27T08:32:47.604Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.151144609.1471676420.256899>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-27T08:32:47.604Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.151144609.1471676420.256899>,
                                  inet_tcp_dist,<0.279.0>,
                                  #Ref<0.151144609.1471676423.256956>}
[ns_server:debug,2023-05-27T08:32:47.604Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Connection down: {con,#Ref<0.151144609.1471676420.256899>,
                               inet_tcp_dist,<0.279.0>,
                               #Ref<0.151144609.1471676423.256956>}
[ns_server:debug,2023-05-27T08:32:47.604Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-05-27T08:32:47.604Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{'EXIT',<0.279.0>,shutdown}}
[error_logger:info,2023-05-27T08:32:47.604Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{net_kernel,913,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-27T08:32:47.804Z,ns_1@cb.local:net_kernel<0.179.0>:cb_dist:info_msg:754]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[error_logger:info,2023-05-27T08:32:47.804Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================INFO REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-05-27T08:32:47.804Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Added connection {con,#Ref<0.151144609.1471676420.256913>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-05-27T08:32:47.805Z,ns_1@cb.local:cb_dist<0.176.0>:cb_dist:info_msg:754]cb_dist: Updated connection: {con,#Ref<0.151144609.1471676420.256913>,
                                  inet_tcp_dist,<0.282.0>,
                                  #Ref<0.151144609.1471676417.258138>}
[ns_server:debug,2023-05-27T08:32:47.838Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[ns_server:debug,2023-05-27T08:32:48.039Z,ns_1@cb.local:<0.274.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:166]ns_couchdb is not ready: false
[error_logger:info,2023-05-27T08:32:48.292Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.286.0>},
                       {id,timer2_server},
                       {mfargs,{timer2,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.459Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.273.0>},
                       {name,wait_for_couchdb_node},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<ns_server_nodes_sup.0.58023840>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.463Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:ns_storage_conf:setup_db_and_ix_paths:64]Initialize db_and_ix_paths variable with [{db_path,
                                           "/opt/couchbase/var/lib/couchbase/data"},
                                          {index_path,
                                           "/opt/couchbase/var/lib/couchbase/data"}]
[error_logger:info,2023-05-27T08:32:48.466Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.289.0>},
                       {name,ns_disksup},
                       {mfargs,{ns_disksup,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.467Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.290.0>},
                       {name,diag_handler_worker},
                       {mfargs,{work_queue,start_link,[diag_handler_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.468Z,ns_1@cb.local:ns_server_sup<0.288.0>:dir_size:start_link:39]Starting quick version of dir_size with program name: godu
[error_logger:info,2023-05-27T08:32:48.468Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.291.0>},
                       {name,dir_size},
                       {mfargs,{dir_size,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.470Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.292.0>},
                       {name,request_throttler},
                       {mfargs,{request_throttler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.477Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.293.0>},
                       {name,ns_log},
                       {mfargs,{ns_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.477Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.294.0>},
                       {name,ns_crash_log_consumer},
                       {mfargs,{ns_log,start_link_crash_consumer,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.480Z,ns_1@cb.local:memcached_passwords<0.295.0>:memcached_cfg:init:62]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-05-27T08:32:48.480Z,ns_1@cb.local:memcached_passwords<0.295.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2023-05-27T08:32:48.492Z,ns_1@cb.local:ns_couchdb_port<0.268.0>:ns_port_server:log:224]ns_couchdb<0.268.0>: Apache CouchDB  (LogLevel=info) is starting.
ns_couchdb<0.268.0>: Apache CouchDB has started. Time to relax.
ns_couchdb<0.268.0>: working as port
ns_couchdb<0.268.0>: 283: Booted. Waiting for shutdown request

[ns_server:debug,2023-05-27T08:32:48.501Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.295.0>
[ns_server:debug,2023-05-27T08:32:48.501Z,ns_1@cb.local:memcached_passwords<0.295.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-27T08:32:48.501Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.295.0>
[ns_server:debug,2023-05-27T08:32:48.504Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[error_logger:info,2023-05-27T08:32:48.504Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.295.0>},
                       {name,memcached_passwords},
                       {mfargs,{memcached_passwords,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.507Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,<0.299.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2023-05-27T08:32:48.507Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,kernel_safe_sup}
             started: [{pid,<0.298.0>},
                       {id,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.508Z,ns_1@cb.local:memcached_permissions<0.300.0>:memcached_cfg:init:62]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-27T08:32:48.511Z,ns_1@cb.local:memcached_permissions<0.300.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-27T08:32:48.512Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.300.0>
[ns_server:debug,2023-05-27T08:32:48.512Z,ns_1@cb.local:memcached_permissions<0.300.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-27T08:32:48.512Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.300.0>
[ns_server:warn,2023-05-27T08:32:48.514Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-27T08:32:48.514Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-05-27T08:32:48.515Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2023-05-27T08:32:48.515Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.300.0>},
                       {name,memcached_permissions},
                       {mfargs,{memcached_permissions,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-27T08:32:48.515Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-27T08:32:48.515Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-27T08:32:48.516Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.303.0>},
                       {name,ns_email_alert},
                       {mfargs,{ns_email_alert,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.517Z,ns_1@cb.local:ns_node_disco<0.306.0>:ns_node_disco:init:128]Initting ns_node_disco with []
[error_logger:info,2023-05-27T08:32:48.517Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.305.0>},
                       {id,ns_node_disco_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,ns_node_disco_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.517Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[user:info,2023-05-27T08:32:48.517Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:128]Node 'ns_1@cb.local' synchronized otp cookie {sanitized,
                                              <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>} from cluster
[ns_server:debug,2023-05-27T08:32:48.517Z,ns_1@cb.local:<0.307.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-27T08:32:48.518Z,ns_1@cb.local:<0.307.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[error_logger:info,2023-05-27T08:32:48.519Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.306.0>},
                       {id,ns_node_disco},
                       {mfargs,{ns_node_disco,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.519Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.308.0>},
                       {id,ns_node_disco_log},
                       {mfargs,{ns_node_disco_log,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.520Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.309.0>},
                       {id,ns_node_disco_conf_events},
                       {mfargs,{ns_node_disco_conf_events,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.521Z,ns_1@cb.local:ns_config_rep<0.311.0>:ns_config_rep:init:71]init pulling
[error_logger:info,2023-05-27T08:32:48.521Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.310.0>},
                       {id,ns_config_rep_merger},
                       {mfargs,{ns_config_rep,start_link_merger,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.521Z,ns_1@cb.local:ns_config_rep<0.311.0>:ns_config_rep:init:73]init pushing
[ns_server:debug,2023-05-27T08:32:48.524Z,ns_1@cb.local:ns_config_rep<0.311.0>:ns_config_rep:init:77]init reannouncing
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:versioned_cache:handle_info:92]Flushing cache compiled_roles_cache due to version change from undefined to {[6,
                                                                              5],
                                                                             {0,
                                                                              795521514},
                                                                             {0,
                                                                              795521514},
                                                                             true,
                                                                             []}
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:44]ns_node_disco_conf_events config on nodes_wanted
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:ns_config_events<0.191.0>:ns_node_disco_conf_events:handle_event:50]ns_node_disco_conf_events config on otp
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:ns_cookie_manager<0.188.0>:ns_cookie_manager:do_cookie_sync:107]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:<0.318.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:<0.317.0>:ns_node_disco:do_nodes_wanted_updated_fun:214]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:<0.317.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:<0.318.0>:ns_node_disco:do_nodes_wanted_updated_fun:220]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
alert_limits ->
[{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]
[ns_server:debug,2023-05-27T08:32:48.525Z,ns_1@cb.local:memcached_permissions<0.300.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-05-27T08:32:48.526Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,[]},
 {disabled_users,[]},
 {auditd_enabled,false},
 {rotate_interval,86400},
 {rotate_size,20971520},
 {disabled,[]},
 {sync,[]},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]
[ns_server:debug,2023-05-27T08:32:48.526Z,ns_1@cb.local:memcached_passwords<0.295.0>:memcached_cfg:write_cfg:118]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[error_logger:info,2023-05-27T08:32:48.526Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_node_disco_sup}
             started: [{pid,<0.311.0>},
                       {id,ns_config_rep},
                       {mfargs,{ns_config_rep,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.526Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.304.0>},
                       {name,ns_node_disco_sup},
                       {mfargs,{ns_node_disco_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.526Z,ns_1@cb.local:ns_config_rep<0.311.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([alert_limits,audit,audit_decriptors,
                               auto_failover_cfg,auto_reprovision_cfg,
                               autocompaction,buckets,cbas_memory_quota,
                               cert_and_pkey,client_cert_auth,
                               cluster_compat_version,
                               drop_request_memory_threshold_mib,email_alerts,
                               fts_memory_quota,
                               index_aware_rebalance_disabled,
                               log_redaction_default_cfg,max_bucket_count,
                               memcached,memory_quota,nodes_wanted,otp,
                               password_policy,quorum_nodes,remote_clusters,
                               replication,rest,rest_creds,retry_rebalance,
                               scramsha_fallback_salt,secure_headers,
                               server_groups,set_view_update_daemon,uuid,
                               {couchdb,max_parallel_indexers},
                               {couchdb,max_parallel_replica_indexers},
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>},
                               {metakv,<<"/eventing/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>},
                               {request_limit,capi},
                               {request_limit,rest},
                               {node,'ns_1@cb.local',address_family},
                               {node,'ns_1@cb.local',audit},
                               {node,'ns_1@cb.local',capi_port},
                               {node,'ns_1@cb.local',cbas_admin_port},
                               {node,'ns_1@cb.local',cbas_cc_client_port},
                               {node,'ns_1@cb.local',cbas_cc_cluster_port},
                               {node,'ns_1@cb.local',cbas_cc_http_port},
                               {node,'ns_1@cb.local',cbas_cluster_port},
                               {node,'ns_1@cb.local',cbas_console_port},
                               {node,'ns_1@cb.local',cbas_data_port},
                               {node,'ns_1@cb.local',cbas_debug_port},
                               {node,'ns_1@cb.local',cbas_dirs},
                               {node,'ns_1@cb.local',cbas_http_port},
                               {node,'ns_1@cb.local',cbas_messaging_port},
                               {node,'ns_1@cb.local',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@cb.local',cbas_metadata_port},
                               {node,'ns_1@cb.local',cbas_parent_port},
                               {node,'ns_1@cb.local',cbas_replication_port},
                               {node,'ns_1@cb.local',cbas_result_port},
                               {node,'ns_1@cb.local',cbas_ssl_port},
                               {node,'ns_1@cb.local',compaction_daemon},
                               {node,'ns_1@cb.local',config_version},
                               {node,'ns_1@cb.local',erl_external_listeners}]..)
[ns_server:debug,2023-05-27T08:32:48.528Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
audit_decriptors ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Eventing function definition was created or updated">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Eventing function definition was deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Eventing function definition was read">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Eventing deployed functions list was read">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Eventing function draft definitions were read">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Eventing function draft definitions were deleted">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Save a draft definition to the store">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Start tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Stop tracing eventing function execution">>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Save settings for a given app">>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Get config for eventing">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Save config for eventing">>},
   {enabled,true},
   {module,eventing}]},
 {32782,
  [{name,<<"Cleanup Eventing">>},
   {description,<<"Clears up app definitions and settings from metakv">>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Get settings for a given app">>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Import a list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Export the list of functions">>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Eventing running function list was read">>},
   {enabled,false},
   {module,eventing}]},
 {36865,
  [{name,<<"Service configuration change">>},
   {description,<<"A successful service configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {36866,
  [{name,<<"Node configuration change">>},
   {description,<<"A successful node configuration change was made.">>},
   {enabled,true},
   {module,analytics}]},
 {40960,
  [{name,<<"Create Design Doc">>},
   {description,<<"Design Doc is Created">>},
   {enabled,true},
   {module,view_engine}]},
 {40961,
  [{name,<<"Delete Design Doc">>},
   {description,<<"Design Doc is Deleted">>},
   {enabled,true},
   {module,view_engine}]},
 {40962,
  [{name,<<"Query DDoc Meta Data">>},
   {description,<<"Design Doc Meta Data Query Request">>},
   {enabled,true},
   {module,view_engine}]},
 {40963,
  [{name,<<"View Query">>},
   {description,<<"View Query Request">>},
   {enabled,false},
   {module,view_engine}]},
 {40964,
  [{name,<<"Update Design Doc">>},
   {description,<<"Design Doc is Updated">>},
   {enabled,true},
   {module,view_engine}]}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_reprovision_cfg ->
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
autocompaction ->
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
buckets ->
[[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}],{configs,[]}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cbas_memory_quota ->
1129
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cert_and_pkey ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDAjCCAeqgAwIBAgIIF19ndTumMTowDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA3MDE4MzBiMjAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgNzAxODMw\nYjIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAYQCgxHxWTEzxXCPc\nqtROmjRoTxnovYOLhw+b1scruAJyzPrxbkxgnJdeuwDYilIRfbwBo6jMEjFubdPC\nV0ESFto963xbX0m63O2BkQkOH8/O02U"...>>,
  <<"*****">>}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.300.0>
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
client_cert_auth ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {state,"disable"},
 {prefixes,[]}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
cluster_compat_version ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{5,63851397742}}]},6,5]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:memcached_permissions<0.300.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{user,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
drop_request_memory_threshold_mib ->
undefined
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.300.0>
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
email_alerts ->
[{recipients,["root@localhost"]},
 {sender,"couchbase@localhost"},
 {enabled,false},
 {email_server,[{user,[]},
                {pass,"*****"},
                {host,"localhost"},
                {port,25},
                {encrypt,false}]},
 {alerts,[auto_failover_node,auto_failover_maximum_reached,
          auto_failover_other_nodes_down,auto_failover_cluster_too_small,
          auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
          ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
          ep_clock_cas_drift_threshold_exceeded,communication_issue]}]
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
fts_memory_quota ->
256
[ns_server:debug,2023-05-27T08:32:48.529Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
index_aware_rebalance_disabled ->
false
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
log_redaction_default_cfg ->
[{redact_level,none}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
max_bucket_count ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|30]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memcached ->
[]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
memory_quota ->
1492
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
nodes_wanted ->
['ns_1@cb.local']
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
otp ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {cookie,{sanitized,<<"T/wNzhcfksK5X/1nLqiwD3W7bEjjQyDhy/kyQr3RtcY=">>}}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
password_policy ->
[{min_length,6},{must_present,[]}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
quorum_nodes ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 'ns_1@cb.local']
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
remote_clusters ->
[]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
replication ->
[{enabled,true}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest ->
[{port,8091}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
rest_creds ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 {"<ud>admin</ud>",
  {auth,
   [{<<"plain">>,"*****"},
    {<<"sha512">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,
        <<"jKq3uagPhi5t1AL2DKFzCwJ9XijAXGPzN4CMDJ49jhBj2wpebDsACKR53mckqwOl8y8iHeOi2rcPlkuZXdPYKw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"Qs1ovmU8v9eE2glwXV9FjbSpjNtuOaxf4wBIRRxWELw=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,"*****"},
       {<<"s">>,<<"L8tRj3e1Xvl8os6DTL/uROUmC7Y=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2023-05-27T08:32:48.530Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
retry_rebalance ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]},
 {enabled,false},
 {after_time_period,300},
 {max_attempts,1}]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
scramsha_fallback_salt ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<211,88,119,65,90,45,162,36,255,132,241,44>>]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
secure_headers ->
[]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
server_groups ->
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
set_view_update_daemon ->
[{update_interval,5000},
 {update_min_changes,5000},
 {replica_update_min_changes,5000}]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
uuid ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397749}}]}|
 <<"b1cf035dd538cdacd1c975b6e16f1302">>]
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_indexers} ->
4
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{couchdb,max_parallel_replica_indexers} ->
2
[ns_server:debug,2023-05-27T08:32:48.531Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{13,63852395567}}]}]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/eventing/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397742}}]}|
 <<"{\"ram_quota\":256}">>]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/indexing/settings/config">>} ->
<<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"in"...>>
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]}|
 <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_si"...>>]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,capi} ->
undefined
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{request_limit,rest} ->
undefined
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|inet]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',audit} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8092]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9110]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_client_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9113]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9112]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cc_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9111]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_cluster_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9115]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_console_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9114]
[ns_server:debug,2023-05-27T08:32:48.532Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_data_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9116]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|-1]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8095]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_messaging_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9118]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9119]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_metadata_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9121]
[ns_server:warn,2023-05-27T08:32:48.533Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_parent_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9122]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_replication_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9120]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_result_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9117]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',cbas_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18095]
[error_logger:info,2023-05-27T08:32:48.533Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.323.0>},
                       {name,vbucket_map_mirror},
                       {mfargs,{vbucket_map_mirror,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',compaction_daemon} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2023-05-27T08:32:48.533Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',config_version} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|{6,5}]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {inet,false},
 {inet6,false}]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_debug_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9140]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397741}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8096]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',eventing_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18096]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9130]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:trigger_ssl_reload:594]Notify services [capi_ssl_service] about secure_headers_changed change
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:740]Going to notify following services: [capi_ssl_service]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19130]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8094]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',fts_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18094]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_admin_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9100]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_http_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9102]
[ns_server:debug,2023-05-27T08:32:48.534Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_https_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|19102]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_scan_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9101]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stcatchup_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9104]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stinit_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9103]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',indexer_stmaint_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9105]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',is_enterprise} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',isasl} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 active]
[ns_server:debug,2023-05-27T08:32:48.535Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_sleeptime,19},
 {log_rotation_period,39003}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_config} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 {[{interfaces,
    {memcached_config_mgr,omit_missing_mcd_ports,
     [{[{host,<<"*">>},
        {port,port},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_port},
        {system,true},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,ssl_port},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]},
      {[{host,<<"*">>},
        {port,dedicated_ssl_port},
        {system,true},
        {ssl,
         {[{key,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-key.pem">>},
           {cert,
            <<"/opt/couchbase/var/lib/couchbase/config/memcached-cert.pem">>}]}},
        {ipv4,{memcached_config_mgr,get_afamily_type,[inet]}},
        {ipv6,{memcached_config_mgr,get_afamily_type,[inet6]}}]}]}},
   {ssl_cipher_list,{memcached_config_mgr,get_ssl_cipher_list,[]}},
   {ssl_cipher_order,{memcached_config_mgr,get_ssl_cipher_order,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {ssl_minimum_protocol,{memcached_config_mgr,ssl_minimum_protocol,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {opentracing,
    {[{enabled,opentracing_enabled},
      {module,{"~s",[opentracing_module]}},
      {config,{"~s",[opentracing_config]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},
      {cyclesize,log_cyclesize},
      {sleeptime,log_sleeptime}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}}]}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|11206]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',memcached_defaults} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {opentracing_enabled,false},
 {opentracing_module,[]},
 {opentracing_config,[]},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',moxi} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,0}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ns_log} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',port_servers} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',projector_ssl_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9999]
[ns_server:debug,2023-05-27T08:32:48.536Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|8093]
[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',rest} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]},
 {port,8091},
 {port_meta,global}]
[error_logger:info,2023-05-27T08:32:48.537Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.329.0>},
                       {name,bucket_info_cache},
                       {mfargs,{bucket_info_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',saslauthd_enabled} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|true]
[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_capi_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18092]
[error_logger:info,2023-05-27T08:32:48.537Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.332.0>},
                       {name,ns_tick_event},
                       {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_query_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18093]
[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',ssl_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|18091]
[error_logger:info,2023-05-27T08:32:48.537Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.333.0>},
                       {name,buckets_events},
                       {mfargs,
                           {gen_event,start_link,[{local,buckets_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',uuid} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|
 <<"949a059dcc29e773ec37709a7973341b">>]
[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',xdcr_rest_port} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|9998]
[error_logger:info,2023-05-27T08:32:48.537Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.334.0>},
                       {name,ns_stats_event},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_stats_event}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.537Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{node,'ns_1@cb.local',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{1,63851397738}}]}|false]
[error_logger:info,2023-05-27T08:32:48.539Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.335.0>},
                       {name,samples_loader_tasks},
                       {mfargs,{samples_loader_tasks,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.540Z,ns_1@cb.local:<0.328.0>:ns_ssl_services_setup:notify_service:772]Successfully notified service capi_ssl_service
[ns_server:info,2023-05-27T08:32:48.540Z,ns_1@cb.local:ns_ssl_services_setup<0.214.0>:ns_ssl_services_setup:notify_services:756]Succesfully notified services [capi_ssl_service]
[error_logger:info,2023-05-27T08:32:48.543Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.337.0>},
                       {id,ns_heart},
                       {mfargs,{ns_heart,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.543Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_heart_sup}
             started: [{pid,<0.340.0>},
                       {id,ns_heart_slow_updater},
                       {mfargs,{ns_heart,start_link_slow_updater,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.543Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.336.0>},
                       {name,ns_heart_sup},
                       {mfargs,{ns_heart_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.545Z,ns_1@cb.local:ns_heart<0.337.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]},
                 {ns_heart,handle_info,2,
                           [{file,"src/ns_heart.erl"},{line,118}]}]}}

[ns_server:debug,2023-05-27T08:32:48.545Z,ns_1@cb.local:ns_heart<0.337.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,update_current_status,1,
                           [{file,"src/ns_heart.erl"},{line,187}]}]}}

[error_logger:info,2023-05-27T08:32:48.546Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.345.0>},
                       {id,ns_doctor_events},
                       {mfargs,
                           {gen_event,start_link,[{local,ns_doctor_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_doctor_sup}
             started: [{pid,<0.346.0>},
                       {id,ns_doctor},
                       {mfargs,{ns_doctor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.550Z,ns_1@cb.local:<0.342.0>:restartable:start_child:98]Started child process <0.344.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2023-05-27T08:32:48.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.342.0>},
                       {name,ns_doctor_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_doctor_sup,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.550Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.349.0>},
                       {name,master_activity_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.553Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.350.0>},
                       {name,xdcr_ckpt_store},
                       {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.553Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.351.0>},
                       {name,metakv_worker},
                       {mfargs,{work_queue,start_link,[metakv_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.553Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.352.0>},
                       {name,index_events},
                       {mfargs,{gen_event,start_link,[{local,index_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.554Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.353.0>},
                       {name,index_settings_manager},
                       {mfargs,{index_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.554Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:302]Suspended by process <0.295.0>
[ns_server:debug,2023-05-27T08:32:48.554Z,ns_1@cb.local:memcached_passwords<0.295.0>:replicated_dets:select_from_dets_locked:350]Starting select with {users_storage,[{{docv2,{auth,{'_',local}},'_','_'},
                                      [],
                                      ['$_']}],
                                    100}
[ns_server:debug,2023-05-27T08:32:48.554Z,ns_1@cb.local:users_storage<0.260.0>:replicated_dets:handle_call:309]Released by process <0.295.0>
[error_logger:info,2023-05-27T08:32:48.556Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.355.0>},
                       {name,query_settings_manager},
                       {mfargs,{query_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.557Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2023-05-27T08:32:48.558Z,ns_1@cb.local:memcached_refresh<0.211.0>:ns_memcached:connect:1101]Unable to connect: {error,{badmatch,{error,econnrefused}}}.
[ns_server:debug,2023-05-27T08:32:48.558Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-05-27T08:32:48.560Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.358.0>},
                       {name,eventing_settings_manager},
                       {mfargs,{eventing_settings_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.560Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.360.0>},
                       {name,audit_events},
                       {mfargs,{gen_event,start_link,[{local,audit_events}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.565Z,ns_1@cb.local:ns_heart<0.337.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-27T08:32:48.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.362.0>},
                       {id,menelaus_ui_auth},
                       {mfargs,{menelaus_ui_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.566Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.364.0>},
                       {id,scram_sha},
                       {mfargs,{scram_sha,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.566Z,ns_1@cb.local:ns_heart<0.337.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-27T08:32:48.567Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.365.0>},
                       {id,menelaus_local_auth},
                       {mfargs,{menelaus_local_auth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.570Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.366.0>},
                       {id,menelaus_web_cache},
                       {mfargs,{menelaus_web_cache,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.367.0>},
                       {id,menelaus_stats_gatherer},
                       {mfargs,{menelaus_stats_gatherer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.571Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.368.0>},
                       {id,json_rpc_events},
                       {mfargs,
                           {gen_event,start_link,[{local,json_rpc_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.572Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-27T08:32:48.573Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-27T08:32:48.573Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-27T08:32:48.573Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[error_logger:info,2023-05-27T08:32:48.574Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.370.0>,menelaus_web}
             started: [{pid,<0.376.0>},
                       {id,menelaus_web_ipv4},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"0.0.0.0"},{name,menelaus_web_ipv4}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.577Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for cbas
[ns_server:info,2023-05-27T08:32:48.578Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for eventing
[ns_server:info,2023-05-27T08:32:48.579Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for fts
[ns_server:info,2023-05-27T08:32:48.579Z,ns_1@cb.local:<0.370.0>:menelaus_pluggable_ui:validate_plugin_spec:135]Loaded pluggable UI specification for n1ql
[ns_server:debug,2023-05-27T08:32:48.579Z,ns_1@cb.local:<0.369.0>:restartable:start_child:98]Started child process <0.370.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2023-05-27T08:32:48.579Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {<0.370.0>,menelaus_web}
             started: [{pid,<0.395.0>},
                       {id,menelaus_web_ipv6},
                       {mfargs,
                           {menelaus_web,http_server,
                               [[{ip,"::"},{name,menelaus_web_ipv6}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.579Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.369.0>},
                       {id,menelaus_web},
                       {mfargs,
                           {restartable,start_link,
                               [{menelaus_web,start_link,[]},infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.580Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.414.0>},
                       {id,menelaus_event},
                       {mfargs,{menelaus_event,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.581Z,ns_1@cb.local:ns_heart_slow_status_updater<0.340.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]},
                 {proc_lib,init_p_do_apply,3,
                           [{file,"proc_lib.erl"},{line,247}]}]}}

[ns_server:debug,2023-05-27T08:32:48.581Z,ns_1@cb.local:ns_heart_slow_status_updater<0.340.0>:ns_heart:grab_latest_stats:263]Ignoring failure to grab "@system-processes" stats:
{'EXIT',{badarg,[{ets,last,['stats_archiver-@system-processes-minute'],[]},
                 {stats_archiver,latest_sample,2,
                                 [{file,"src/stats_archiver.erl"},{line,120}]},
                 {ns_heart,grab_latest_stats,1,
                           [{file,"src/ns_heart.erl"},{line,259}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,'-current_status_slow_inner/0-lc$^0/1-0-',1,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow_inner,0,
                           [{file,"src/ns_heart.erl"},{line,282}]},
                 {ns_heart,current_status_slow,1,
                           [{file,"src/ns_heart.erl"},{line,250}]},
                 {ns_heart,slow_updater_loop,0,
                           [{file,"src/ns_heart.erl"},{line,244}]}]}}

[ns_server:debug,2023-05-27T08:32:48.582Z,ns_1@cb.local:ns_heart_slow_status_updater<0.340.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-27T08:32:48.582Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.423.0>},
                       {id,hot_keys_keeper},
                       {mfargs,{hot_keys_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.582Z,ns_1@cb.local:ns_heart_slow_status_updater<0.340.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:46]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-05-27T08:32:48.583Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.424.0>},
                       {id,menelaus_web_alerts_srv},
                       {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.585Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,menelaus_sup}
             started: [{pid,<0.425.0>},
                       {id,menelaus_cbauth},
                       {mfargs,{menelaus_cbauth,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[user:info,2023-05-27T08:32:48.585Z,ns_1@cb.local:ns_server_sup<0.288.0>:menelaus_sup:start_link:48]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "6.5.0-4960-enterprise".
[error_logger:info,2023-05-27T08:32:48.585Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.361.0>},
                       {name,menelaus},
                       {mfargs,{menelaus_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.585Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.431.0>},
                       {name,ns_ports_setup},
                       {mfargs,{ns_ports_setup,start,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.586Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.436.0>},
                       {id,service_agent_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_agent_children_sup},
                                service_agent_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.586Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_agent_sup}
             started: [{pid,<0.437.0>},
                       {id,service_agent_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<service_agent_sup.0.107373856>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.587Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.434.0>},
                       {name,service_agent_sup},
                       {mfargs,{service_agent_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.589Z,ns_1@cb.local:ns_ports_setup<0.431.0>:ns_ports_manager:set_dynamic_children:54]Setting children [memcached,saslauthd_port,projector,goxdcr]
[error_logger:info,2023-05-27T08:32:48.590Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.447.0>},
                       {name,ns_memcached_sockets_pool},
                       {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.598Z,ns_1@cb.local:memcached_auth_server<0.448.0>:memcached_auth_server:reconnect:233]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2023-05-27T08:32:48.598Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.448.0>},
                       {name,memcached_auth_server},
                       {mfargs,{memcached_auth_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.598Z,ns_1@cb.local:ns_audit_cfg<0.450.0>:ns_audit_cfg:write_audit_json:259]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "48537283"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2023-05-27T08:32:48.609Z,ns_1@cb.local:ns_audit_cfg<0.450.0>:ns_audit_cfg:notify_memcached:170]Instruct memcached to reload audit config
[error_logger:info,2023-05-27T08:32:48.609Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.450.0>},
                       {name,ns_audit_cfg},
                       {mfargs,{ns_audit_cfg,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:warn,2023-05-27T08:32:48.610Z,ns_1@cb.local:<0.454.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[ns_server:debug,2023-05-27T08:32:48.611Z,ns_1@cb.local:memcached_config_mgr<0.456.0>:memcached_config_mgr:init:49]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-05-27T08:32:48.611Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.455.0>},
                       {name,ns_audit},
                       {mfargs,{ns_audit,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.612Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.456.0>},
                       {name,memcached_config_mgr},
                       {mfargs,{memcached_config_mgr,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.613Z,ns_1@cb.local:<0.457.0>:ns_memcached_log_rotator:init:42]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2023-05-27T08:32:48.613Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.457.0>},
                       {name,ns_memcached_log_rotator},
                       {mfargs,{ns_memcached_log_rotator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.614Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.458.0>},
                       {name,testconditions_store},
                       {mfargs,{simple_store,start_link,[testconditions]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.615Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.459.0>},
                       {name,terse_cluster_info_uploader},
                       {mfargs,{terse_cluster_info_uploader,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.618Z,ns_1@cb.local:terse_cluster_info_uploader<0.459.0>:terse_cluster_info_uploader:handle_info:48]Refreshing terse cluster info with <<"{\"rev\":13,\"nodesExt\":[{\"services\":{\"mgmt\":8091,\"mgmtSSL\":18091,\"kv\":11210,\"kvSSL\":11207,\"capi\":8092,\"capiSSL\":18092,\"projector\":9999,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[error_logger:info,2023-05-27T08:32:48.619Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.464.0>},
                       {id,ns_bucket_sup},
                       {mfargs,{ns_bucket_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:warn,2023-05-27T08:32:48.619Z,ns_1@cb.local:<0.463.0>:ns_memcached:connect:1104]Unable to connect: {error,{badmatch,{error,econnrefused}}}, retrying.
[error_logger:info,2023-05-27T08:32:48.620Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_bucket_worker_sup}
             started: [{pid,<0.465.0>},
                       {id,ns_bucket_worker},
                       {mfargs,{ns_bucket_worker,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.620Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.461.0>},
                       {name,ns_bucket_worker_sup},
                       {mfargs,{ns_bucket_worker_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.621Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.467.0>},
                       {name,system_stats_collector},
                       {mfargs,{system_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.622Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.471.0>},
                       {name,{stats_archiver,"@system"}},
                       {mfargs,{stats_archiver,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.623Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.473.0>},
                       {name,{stats_reader,"@system"}},
                       {mfargs,{stats_reader,start_link,["@system"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.624Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.474.0>},
                       {name,{stats_archiver,"@system-processes"}},
                       {mfargs,
                           {stats_archiver,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.624Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.476.0>},
                       {name,{stats_reader,"@system-processes"}},
                       {mfargs,
                           {stats_reader,start_link,["@system-processes"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.477.0>},
                       {name,{stats_archiver,"@query"}},
                       {mfargs,{stats_archiver,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.625Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.479.0>},
                       {name,{stats_reader,"@query"}},
                       {mfargs,{stats_reader,start_link,["@query"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.626Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.480.0>},
                       {name,query_stats_collector},
                       {mfargs,{query_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.627Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.482.0>},
                       {name,{stats_archiver,"@global"}},
                       {mfargs,{stats_archiver,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.627Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.484.0>},
                       {name,{stats_reader,"@global"}},
                       {mfargs,{stats_reader,start_link,["@global"]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.629Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.485.0>},
                       {name,global_stats_collector},
                       {mfargs,{global_stats_collector,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.632Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.487.0>},
                       {name,goxdcr_status_keeper},
                       {mfargs,{goxdcr_status_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.632Z,ns_1@cb.local:goxdcr_status_keeper<0.487.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-05-27T08:32:48.633Z,ns_1@cb.local:goxdcr_status_keeper<0.487.0>:goxdcr_rest:get_from_goxdcr:140]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-05-27T08:32:48.635Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.491.0>},
                       {id,service_stats_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_stats_children_sup},
                                services_stats_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.636Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.493.0>},
                       {id,service_status_keeper_worker},
                       {mfargs,
                           {work_queue,start_link,
                               [service_status_keeper_worker]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.642Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.494.0>},
                       {id,service_status_keeper_index},
                       {mfargs,{service_index,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.647Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.497.0>},
                       {id,service_status_keeper_fts},
                       {mfargs,{service_fts,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_status_keeper_sup}
             started: [{pid,<0.500.0>},
                       {id,service_status_keeper_eventing},
                       {mfargs,{service_eventing,start_keeper,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.492.0>},
                       {id,service_status_keeper_sup},
                       {mfargs,{service_status_keeper_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.651Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,services_stats_sup}
             started: [{pid,<0.503.0>},
                       {id,service_stats_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<services_stats_sup.0.108537742>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.652Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.490.0>},
                       {name,services_stats_sup},
                       {mfargs,{services_stats_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.672Z,ns_1@cb.local:<0.508.0>:new_concurrency_throttle:init:115]init concurrent throttle process, pid: <0.508.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[error_logger:info,2023-05-27T08:32:48.675Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.506.0>},
                       {name,compaction_daemon},
                       {mfargs,{compaction_daemon,start_link,[]}},
                       {restart_type,{permanent,4}},
                       {shutdown,86400000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:32:48.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2023-05-27T08:32:48.681Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,cluster_logs_sup}
             started: [{pid,<0.510.0>},
                       {id,ets_holder},
                       {mfargs,
                           {cluster_logs_collection_task,
                               start_link_ets_holder,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.682Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.509.0>},
                       {name,cluster_logs_sup},
                       {mfargs,{cluster_logs_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.682Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.511.0>},
                       {name,leader_events},
                       {mfargs,{gen_event,start_link,[{local,leader_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.691Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.515.0>},
                       {id,leader_activities},
                       {mfargs,{leader_activities,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.697Z,ns_1@cb.local:ns_ports_setup<0.431.0>:ns_ports_setup:set_children:85]Monitor ns_child_ports_sup <12939.109.0>
[ns_server:debug,2023-05-27T08:32:48.697Z,ns_1@cb.local:memcached_config_mgr<0.456.0>:memcached_config_mgr:init:51]ns_ports_setup seems to be ready
[ns_server:debug,2023-05-27T08:32:48.700Z,ns_1@cb.local:memcached_config_mgr<0.456.0>:memcached_config_mgr:find_port_pid_loop:137]Found memcached port <12939.116.0>
[ns_server:warn,2023-05-27T08:32:48.701Z,ns_1@cb.local:leader_lease_agent<0.516.0>:leader_lease_agent:maybe_recover_persisted_lease:399]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"02fa71c1eed17d8523c799a2efd87a61">>},
                       {time_left,15000},
                       {status,active}]
[error_logger:info,2023-05-27T08:32:48.701Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_leases_sup}
             started: [{pid,<0.516.0>},
                       {id,leader_lease_agent},
                       {mfargs,{leader_lease_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.701Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.514.0>},
                       {id,leader_leases_sup},
                       {mfargs,{leader_leases_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.704Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.521.0>},
                       {id,leader_registry_server},
                       {mfargs,{leader_registry_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.706Z,ns_1@cb.local:leader_registry_sup<0.519.0>:mb_master:check_master_takeover_needed:283]Sending master node question to the following nodes: []
[ns_server:debug,2023-05-27T08:32:48.706Z,ns_1@cb.local:leader_registry_sup<0.519.0>:mb_master:check_master_takeover_needed:285]Got replies: []
[ns_server:debug,2023-05-27T08:32:48.706Z,ns_1@cb.local:leader_registry_sup<0.519.0>:mb_master:check_master_takeover_needed:291]Was unable to discover master, not going to force mastership takeover
[user:info,2023-05-27T08:32:48.711Z,ns_1@cb.local:mb_master<0.525.0>:mb_master:init:103]I'm the only node, so I'm the master.
[ns_server:debug,2023-05-27T08:32:48.711Z,ns_1@cb.local:leader_registry<0.521.0>:leader_registry_server:handle_new_leader:241]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2023-05-27T08:32:48.717Z,ns_1@cb.local:memcached_config_mgr<0.456.0>:memcached_config_mgr:init:82]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2023-05-27T08:32:48.717Z,ns_1@cb.local:memcached_config_mgr<0.456.0>:memcached_config_mgr:init:86]activated memcached port server
[ns_server:debug,2023-05-27T08:32:48.719Z,ns_1@cb.local:mb_master<0.525.0>:master_activity_events:submit_cast:82]Failed to send master activity event: {error,badarg}
[error_logger:info,2023-05-27T08:32:48.720Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.528.0>},
                       {id,leader_lease_acquirer},
                       {mfargs,{leader_lease_acquirer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.721Z,ns_1@cb.local:leader_quorum_nodes_manager<0.530.0>:leader_quorum_nodes_manager:pull_config:114]Attempting to pull config from nodes:
[]
[error_logger:info,2023-05-27T08:32:48.721Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.530.0>},
                       {id,leader_quorum_nodes_manager},
                       {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.721Z,ns_1@cb.local:leader_quorum_nodes_manager<0.530.0>:leader_quorum_nodes_manager:pull_config:119]Pulled config successfully.
[ns_server:warn,2023-05-27T08:32:48.724Z,ns_1@cb.local:<0.534.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@cb.local' because its already taken by {'ns_1@cb.local',
                                                                           <<"02fa71c1eed17d8523c799a2efd87a61">>} (valid for 14976ms)
[ns_server:info,2023-05-27T08:32:48.725Z,ns_1@cb.local:mb_master_sup<0.527.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.537.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-27T08:32:48.725Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.537.0>},
                       {id,ns_tick},
                       {mfargs,{ns_tick,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,10},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.729Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.539.0>},
                       {id,compat_mode_events},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,compat_mode_events}]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.730Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.540.0>},
                       {id,compat_mode_manager},
                       {mfargs,{compat_mode_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.732Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.542.0>},
                       {id,ns_janitor_server},
                       {mfargs,{ns_janitor_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.733Z,ns_1@cb.local:ns_orchestrator_child_sup<0.541.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.543.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-27T08:32:48.733Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.543.0>},
                       {id,auto_reprovision},
                       {mfargs,{auto_reprovision,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:info,2023-05-27T08:32:48.736Z,ns_1@cb.local:ns_orchestrator_child_sup<0.541.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.544.0> on 'ns_1@cb.local'

[ns_server:info,2023-05-27T08:32:48.736Z,ns_1@cb.local:ns_orchestrator_child_sup<0.541.0>:misc:start_singleton:857]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.545.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-27T08:32:48.736Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.544.0>},
                       {id,auto_rebalance},
                       {mfargs,{auto_rebalance,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.736Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_child_sup}
             started: [{pid,<0.545.0>},
                       {id,ns_orchestrator},
                       {mfargs,{ns_orchestrator,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.736Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.541.0>},
                       {id,ns_orchestrator_child_sup},
                       {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.740Z,ns_1@cb.local:<0.547.0>:auto_failover:init:185]init auto_failover.
[user:info,2023-05-27T08:32:48.740Z,ns_1@cb.local:<0.547.0>:auto_failover:handle_call:216]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2023-05-27T08:32:48.743Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
{local_changes_count,<<"949a059dcc29e773ec37709a7973341b">>} ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{14,63852395568}}]}]
[ns_server:info,2023-05-27T08:32:48.743Z,ns_1@cb.local:ns_orchestrator_sup<0.538.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.547.0> on 'ns_1@cb.local'

[ns_server:debug,2023-05-27T08:32:48.743Z,ns_1@cb.local:ns_config_rep<0.311.0>:ns_config_rep:do_push_keys:321]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"949a059dcc29e773ec37709a7973341b">>}]..)
[ns_server:info,2023-05-27T08:32:48.743Z,ns_1@cb.local:mb_master_sup<0.527.0>:misc:start_singleton:857]start_singleton(work_queue, start_link, [{via,leader_registry,collections}]): started as <0.552.0> on 'ns_1@cb.local'

[error_logger:info,2023-05-27T08:32:48.743Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_orchestrator_sup}
             started: [{pid,<0.547.0>},
                       {id,auto_failover},
                       {mfargs,{auto_failover,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.743Z,ns_1@cb.local:ns_config_log<0.200.0>:ns_config_log:log_common:231]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"949a059dcc29e773ec37709a7973341b">>,{2,63851397742}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true}]
[error_logger:info,2023-05-27T08:32:48.744Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.538.0>},
                       {id,ns_orchestrator_sup},
                       {mfargs,{ns_orchestrator_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.744Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.552.0>},
                       {id,collections},
                       {mfargs,{collections,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.745Z,ns_1@cb.local:<0.557.0>:license_reporting:init:66]Starting license_reporting server
[ns_server:info,2023-05-27T08:32:48.746Z,ns_1@cb.local:mb_master_sup<0.527.0>:misc:start_singleton:857]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.557.0> on 'ns_1@cb.local'

[ns_server:debug,2023-05-27T08:32:48.746Z,ns_1@cb.local:<0.512.0>:restartable:start_child:98]Started child process <0.513.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2023-05-27T08:32:48.746Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,mb_master_sup}
             started: [{pid,<0.557.0>},
                       {id,license_reporting},
                       {mfargs,{license_reporting,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.746Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_registry_sup}
             started: [{pid,<0.525.0>},
                       {id,mb_master},
                       {mfargs,{mb_master,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.746Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,leader_services_sup}
             started: [{pid,<0.519.0>},
                       {id,leader_registry_sup},
                       {mfargs,{leader_registry_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.746Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.512.0>},
                       {name,leader_services_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{leader_services_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.747Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.559.0>},
                       {name,ns_tick_agent},
                       {mfargs,{ns_tick_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.747Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.561.0>},
                       {name,master_activity_events_ingress},
                       {mfargs,
                           {gen_event,start_link,
                               [{local,master_activity_events_ingress}]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.747Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.562.0>},
                       {name,master_activity_events_timestamper},
                       {mfargs,
                           {master_activity_events,start_link_timestamper,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.748Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.563.0>},
                       {name,master_activity_events_pids_watcher},
                       {mfargs,
                           {master_activity_events_pids_watcher,start_link,
                               []}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.749Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.564.0>},
                       {name,master_activity_events_keeper},
                       {mfargs,{master_activity_events_keeper,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,brutal_kill},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.754Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.567.0>},
                       {id,ns_server_monitor},
                       {mfargs,{ns_server_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.754Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.569.0>},
                       {id,service_monitor_children_sup},
                       {mfargs,
                           {supervisor,start_link,
                               [{local,service_monitor_children_sup},
                                health_monitor_sup,child]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.756Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.576.0>},
                       {id,{kv,dcp_traffic_monitor}},
                       {mfargs,{dcp_traffic_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.759Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.578.0>},
                       {id,{kv,kv_stats_monitor}},
                       {mfargs,{kv_stats_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.760Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,service_monitor_children_sup}
             started: [{pid,<0.580.0>},
                       {id,{kv,kv_monitor}},
                       {mfargs,{kv_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.760Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.570.0>},
                       {id,service_monitor_worker},
                       {mfargs,
                           {erlang,apply,
                               [#Fun<health_monitor_sup.0.112499759>,[]]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.761Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.582.0>},
                       {id,node_monitor},
                       {mfargs,{node_monitor,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.762Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,health_monitor_sup}
             started: [{pid,<0.588.0>},
                       {id,node_status_analyzer},
                       {mfargs,{node_status_analyzer,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.762Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.566.0>},
                       {name,health_monitor_sup},
                       {mfargs,{health_monitor_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.763Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.590.0>},
                       {name,rebalance_agent},
                       {mfargs,{rebalance_agent,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.768Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:27]Notifying on barrier menelaus_barrier
[error_logger:info,2023-05-27T08:32:48.768Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_sup}
             started: [{pid,<0.591.0>},
                       {name,ns_rebalance_report_manager},
                       {mfargs,{ns_rebalance_report_manager,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[ns_server:debug,2023-05-27T08:32:48.768Z,ns_1@cb.local:menelaus_barrier<0.209.0>:one_shot_barrier:barrier_body:62]Barrier menelaus_barrier got notification from <0.207.0>
[error_logger:info,2023-05-27T08:32:48.768Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_nodes_sup}
             started: [{pid,<0.288.0>},
                       {name,ns_server_sup},
                       {mfargs,{ns_server_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.768Z,ns_1@cb.local:ns_server_nodes_sup<0.207.0>:one_shot_barrier:notify:32]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2023-05-27T08:32:48.768Z,ns_1@cb.local:<0.206.0>:restartable:start_child:98]Started child process <0.207.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2023-05-27T08:32:48.768Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.206.0>},
                       {id,ns_server_nodes_sup},
                       {mfargs,
                           {restartable,start_link,
                               [{ns_server_nodes_sup,start_link,[]},
                                infinity]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[error_logger:info,2023-05-27T08:32:48.770Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,ns_server_cluster_sup}
             started: [{pid,<0.593.0>},
                       {id,remote_api},
                       {mfargs,{remote_api,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,1000},
                       {child_type,worker}]

[error_logger:info,2023-05-27T08:32:48.770Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
          supervisor: {local,root_sup}
             started: [{pid,<0.185.0>},
                       {id,ns_server_cluster_sup},
                       {mfargs,{ns_server_cluster_sup,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,infinity},
                       {child_type,supervisor}]

[ns_server:debug,2023-05-27T08:32:48.770Z,ns_1@cb.local:<0.5.0>:child_erlang:child_loop:130]219: Entered child_loop
[error_logger:info,2023-05-27T08:32:48.770Z,ns_1@cb.local:error_logger<0.32.0>:ale_error_logger_handler:do_log:203]
=========================PROGRESS REPORT=========================
         application: ns_server
          started_at: 'ns_1@cb.local'

[ns_server:debug,2023-05-27T08:32:48.771Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@",admin}
[ns_server:debug,2023-05-27T08:32:48.778Z,ns_1@cb.local:json_rpc_connection-projector-cbauth<0.594.0>:json_rpc_connection:init:73]Observed revrpc connection: label "projector-cbauth", handling process <0.594.0>
[ns_server:debug,2023-05-27T08:32:48.778Z,ns_1@cb.local:menelaus_cbauth<0.425.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"projector-cbauth",<0.594.0>} started
[ns_server:debug,2023-05-27T08:32:48.778Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.595.0>:json_rpc_connection:init:73]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.595.0>
[ns_server:debug,2023-05-27T08:32:48.778Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.596.0>:json_rpc_connection:init:73]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.596.0>
[ns_server:debug,2023-05-27T08:32:48.784Z,ns_1@cb.local:menelaus_cbauth<0.425.0>:menelaus_cbauth:handle_cast:107]Observed json rpc process {"goxdcr-cbauth",<0.596.0>} started
[ns_server:debug,2023-05-27T08:32:48.785Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@projector-cbauth",admin}
[ns_server:debug,2023-05-27T08:32:48.786Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {"@goxdcr-cbauth",admin}
[ns_server:debug,2023-05-27T08:32:49.524Z,ns_1@cb.local:memcached_refresh<0.211.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2023-05-27T08:32:49.742Z,ns_1@cb.local:<0.547.0>:auto_failover_logic:log_master_activity:177]Transitioned node {'ns_1@cb.local',<<"949a059dcc29e773ec37709a7973341b">>} state new -> up
[ns_server:info,2023-05-27T08:32:53.736Z,ns_1@cb.local:<0.545.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:info,2023-05-27T08:32:58.736Z,ns_1@cb.local:<0.545.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:debug,2023-05-27T08:32:59.111Z,ns_1@cb.local:compiled_roles_cache<0.262.0>:menelaus_roles:build_compiled_roles:753]Compile roles for user {[],anonymous}
[menelaus:info,2023-05-27T08:32:59.126Z,ns_1@cb.local:<0.389.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6025>,'POST',"/settigs/stats",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"57"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[menelaus:info,2023-05-27T08:32:59.133Z,ns_1@cb.local:<0.386.0>:menelaus_web:get_action:795]Invalid post received: {mochiweb_request,
                           [#Port<0.6027>,'POST',"/settings/compaction",
                            {1,1},
                            {5,
                             {"host",
                              {'Host',"127.0.0.1:8091"},
                              {"accept",
                               {'Accept',"*/*"},
                               nil,
                               {"content-length",
                                {'Content-Length',"54"},
                                nil,
                                {"content-type",
                                 {'Content-Type',
                                     "application/x-www-form-urlencoded"},
                                 nil,nil}}},
                              {"user-agent",
                               {'User-Agent',"curl/7.66.0-DEV"},
                               nil,nil}}}]}
[ns_server:debug,2023-05-27T08:33:03.701Z,ns_1@cb.local:leader_lease_agent<0.516.0>:leader_lease_agent:handle_lease_expired:286]Lease held by {lease_holder,<<"02fa71c1eed17d8523c799a2efd87a61">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:warn,2023-05-27T08:33:03.702Z,ns_1@cb.local:<0.534.0>:leader_lease_acquire_worker:handle_lease_already_acquired:232]Failed to acquire lease from 'ns_1@cb.local' because its already taken by {'ns_1@cb.local',
                                                                           <<"02fa71c1eed17d8523c799a2efd87a61">>} (valid for 0ms)
[ns_server:debug,2023-05-27T08:33:03.703Z,ns_1@cb.local:leader_lease_agent<0.516.0>:leader_lease_agent:do_handle_acquire_lease:149]Granting lease to {lease_holder,<<"0a190bb96611dfa38042e84d695ba542">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:info,2023-05-27T08:33:03.736Z,ns_1@cb.local:<0.545.0>:ns_orchestrator:handle_info:523]Skipping janitor in state janitor_running
[ns_server:info,2023-05-27T08:33:03.738Z,ns_1@cb.local:<0.534.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:302]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"0a190bb96611dfa38042e84d695ba542">>)
[ns_server:debug,2023-05-27T08:33:18.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:33:18.675Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:33:18.676Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:33:18.676Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:33:48.676Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:33:48.676Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:33:48.677Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:33:48.677Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:34:02.383Z,ns_1@cb.local:ldap_auth_cache<0.254.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-27T08:34:18.677Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:34:18.677Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:34:18.677Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:34:18.678Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:34:48.678Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:34:48.678Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:34:48.678Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:34:48.678Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:35:17.384Z,ns_1@cb.local:ldap_auth_cache<0.254.0>:active_cache:cleanup:231]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-05-27T08:35:18.679Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:35:18.679Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:35:18.679Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:35:18.679Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:35:48.680Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:35:48.680Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-05-27T08:35:48.680Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_daemon:process_scheduler_message:1306]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-05-27T08:35:48.681Z,ns_1@cb.local:compaction_daemon<0.506.0>:compaction_scheduler:schedule_next:60]Finished compaction for compact_kv too soon. Next run will be in 30s
